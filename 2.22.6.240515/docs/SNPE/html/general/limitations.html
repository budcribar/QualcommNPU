

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Limitations &mdash; Snapdragon Neural Processing Engine SDK</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/custom_css.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="References" href="appx_ref.html" />
    <link rel="prev" title="API" href="api.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> Qualcomm® Neural Processing SDK
          

          
          </a>

          
            
            
              <div class="version">
                v2.22.6
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="revision_history.html">Revision History</a></li>
<li class="toctree-l1"><a class="reference internal" href="revision_history_windows.html">Revision History - Windows</a></li>
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="setup.html">Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="usergroup1.html">Network Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="usergroup5.html">Input Data and Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="usergroup6.html">Tutorials and Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="usergroup10.html">Benchmarking and Accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="tools.html">Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="usergroup11.html">Debug Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="api.html">API</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Limitations</a></li>
<li class="toctree-l1"><a class="reference internal" href="appx_ref.html">References</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Qualcomm® Neural Processing SDK</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Limitations</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="limitations">
<h1>Limitations<a class="headerlink" href="#limitations" title="Permalink to this heading">¶</a></h1>
<div class="ui-resizable side-nav-resizable docutils container" id="side-nav">
<div class="docutils container" id="nav-tree">
<div class="docutils container" id="nav-tree-contents">
</div>
</div>
</div>
<div class="docutils container" id="doc-content">
<div class="header docutils container">
</div>
<div class="contents docutils container">
<div class="textblock docutils container">
<p>This chapter describes limitations discovered in this release
during testing. Future releases will provide fixes for
discovered issues.</p>
<p class="rubric" id="general-snpe-limitations">General Qualcomm® Neural Processing SDK Limitations</p>
<ul class="simple">
<li><p>Qualcomm® Neural Processing SDK currently supports 4D input data, where the first
dimension is batch.</p></li>
<li><p>Only batch of 1 is supported for RCNN networks like
Faster-RCNN. See <a class="reference external" href="limitations.html#limitations_layers">Layer
Limitations</a> below.</p></li>
</ul>
<p class="rubric" id="general-java-api-limitations">General Java API Limitations</p>
<ul class="simple">
<li><p>Confine INeuralNetwork instance usage to a single thread
The current SDK INeuralNetwork class instances are meant to
be accessed from a single thread. Developers must make sure
that is enforced within the application or unexpected errors
may occur.</p></li>
</ul>
<p class="rubric" id="general-cpu-runtime-limitations">General CPU Runtime Limitations</p>
<ul class="simple">
<li><p>Not all layers have been optimized for the CPU runtime. For
example, deconvolution is dramatically slower than
convolution.</p></li>
</ul>
<p class="rubric" id="general-gpu-runtime-limitations">General GPU Runtime Limitations</p>
<ul class="simple">
<li><p>In GPU_FLOAT32_16_HYBRID mode, the GPU kernels use
HALF_FLOAT precision for all intermediate data handling and
FULL_FLOAT precision for all of its computations. While this
does not typically affect mAP for networks that are being
used for classification this can overflow/underflow which
can impact use of the engine for uses other than
classification. If an impact is observed, try running with
the CPU runtime which is always FULL_FLOAT to validate any
overflow/underflow issues.</p></li>
<li><p>In GPU_FLOAT16 mode, the GPU kernels use HALF_FLOAT
precision for all intermediate data handling and all of its
computations. In this mode, due to lower computation
precision comparing to GPU_FLOAT32_16_HYBRID, chances of
negative impact on network’s accuracy (e.g. mAP score) are
higher. Users are encouraged to test accuracy performance of
their network using this mode to ensure it meets
requirements of their use case.</p></li>
<li><p>For absolute size restrictions, the concept of “packed”
channels refers to the number of channels divided by 4, and
rounded up to the nearest integer:
<strong>packed_channels = ceil(channels / 4.0)</strong></p></li>
<li><p>Whenever a layer has a 4-dimensional (i.e. batch x width x
height x channels) component, such as input, output, or
weight tensor, that component will have the following size
restrictions:</p>
<ul>
<li><p>Number of packed channels * width &lt; MaxPerGPUSize</p></li>
</ul>
</li>
<li><p>For all layers that have weights/biases, restrictions are:</p>
<ul>
<li><p>Filter size * filter size * 4 &lt;= MaxPerGPUSize</p></li>
<li><p>Number of output channels / 4 &lt;= MaxPerGPUSize</p></li>
</ul>
</li>
<li><p>The MaxPerGPUSize is dependent on Qualcomm Adreno™ GPU type
and the values are given below</p>
<ul>
<li><p>A330: 8192</p></li>
<li><p>A430, A530: 16384</p></li>
</ul>
</li>
<li><p>While loading any network, GPU runtime may choose to merge
(squash) few layers with the previous layers in the network,
depending on the compatibility of the layers. This results
in missing performance information for the squashed layers.</p></li>
</ul>
<p class="rubric" id="general-dsp-runtime-limitations">General DSP Runtime Limitations</p>
<ul class="simple">
<li><p>When using non-quantized models, the first network execution
after network initialization may be significantly slower
than subsequent executions. To avoid this, use a DLC file
that has been quantized by
<a class="reference external" href="tools.html#tools_snpe-dlc-quantize">snpe-dlc-quantize</a>.</p></li>
</ul>
<p class="rubric" id="general-aip-runtime-limitations">General AIP Runtime Limitations</p>
<ul class="simple">
<li><p>If the input layer of a network needs to be processed by HTA
the input must be a 4D tensor with shape format as NHWC
where the batch dimension N must be 1 and the number of
channels C cannot exceed 16.</p></li>
<li><p>However, one could take advantage of manually partitioning a
network to bypass this limitation by having the input layer
be processed on the HVX instead.</p></li>
<li><p>AIP runtime supports batched input for models which are
completely using HTA or the models which have all the layers
running on HTA except Softmax which is partitioned to HVX.</p></li>
</ul>
<p class="rubric" id="layer-limitations">Layer Limitations</p>
<ul>
<li><p><strong>ArgMax</strong></p>
<ul class="simple">
<li><p>For DSP runtime, ArgMax only outputs float; its output
cannot be a quantized data type due to accuracy.</p></li>
</ul>
</li>
<li><p><strong>Color space conversion</strong></p>
<ul class="simple">
<li><p>For NV21 input image encoding type, width or height must
be multiple of 2. The reason is 4 Y (2wx2h) is sharing
one UV pair.</p></li>
</ul>
</li>
<li><p><strong>Concatenation</strong></p>
<ul class="simple">
<li><p>For GPU runtime, the number of input channels in each of
the inputs can assume arbitrary values. However, if one
or more of these are not a multiple of 4, performance of
the layer will be diminished.</p></li>
</ul>
</li>
<li><p><strong>Convolution</strong></p>
<ul class="simple">
<li><p>For GPU runtime, when the number of groups is greater
than 1, the number of output channels must be a multiple
of 4 * the number of groups. For example, with 2 groups,
the number of output channels must be a multiple of 8
(4*2=8).</p></li>
</ul>
</li>
<li><p><strong>Crop</strong></p>
<ul class="simple">
<li><p>For GPU runtime, the number of input channels in each of
the inputs must be a multiple of 4.</p></li>
<li><p>Crop on the DSP is not optimized in all cases. Spatial
cropping is optimized (cropping height and/or width,
leaving other dimensions unchanged)</p></li>
</ul>
</li>
<li><p><strong>Deconvolution</strong></p>
<ul class="simple">
<li><p>For GPU and CPU runtime, the number of output channels
(i.e. number of filters) can be any value (not
necessarily a multiple of 4).</p></li>
<li><p>For GPU runtime the following limitations apply:</p>
<ul>
<li><p>number of packed input channels * number output
channels &lt;= MaxPerGPUSize</p></li>
<li><p>Filter size-X * Filter size-Y &lt;= MaxPerGPUSize</p></li>
<li><p>Stride &lt;= filter size</p></li>
</ul>
</li>
<li><p>For DSP runtime, deconvolutions with stride &gt; 4 are not
fully optimized.</p></li>
</ul>
</li>
<li><p><strong>Depthwise Convolution</strong></p>
<ul class="simple">
<li><p>Depthwise Convolution on the DSP is not optimized for all
cases. The following case is optimized:</p>
<ul>
<li><p>Horizontal stride is &lt;= 2.</p></li>
<li><p>Filter is 3x3.</p></li>
<li><p>Depth is a multiple of 32.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Detection Output</strong></p>
<ul class="simple">
<li><p>keepTopK must be provided.</p></li>
<li><p>Output buffer must be of sufficient size and in Float
format.</p></li>
<li><p>For DSP runtime, batch &gt; 1 and dlc caching is not
supported.</p></li>
</ul>
</li>
<li><p><strong>Fully connected</strong></p>
<ul class="simple">
<li><p>For GPU runtime, the following limitations apply:</p>
<ul>
<li><p>Input width * input height * number of input
channels &lt;= MaxPerGPUSize</p></li>
<li><p>Number of output channels &lt;= MaxPerGPUSize</p></li>
</ul>
</li>
<li><p>For DSP Runtime, batch &gt; 1 is optimized only when input
height * width * channel is a multiple of 16.</p></li>
</ul>
</li>
<li><p><strong>Input Image Scaling</strong></p>
<ul class="simple">
<li><p>The DSP runtime image scaling performs well under the
conditions listed below. Other configurations are not
optimized.</p>
<ul>
<li><p>Scale factor is an upscale by 2x AND</p></li>
<li><p>Depth is a power of 2 AND either</p>
<ul>
<li><p>Depth is less than 128 with width equal to a power
of 2 OR</p></li>
<li><p>Depth is greater than 128.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Instance Normalization</strong></p>
<ul>
<li><div class="line-block">
<div class="line">For certain models containing InstanceNorm layers, the
default value for the “epsilon” parameter could
overwhelm the standard deviation of the input tensor.
In such cases a numerical discrepancy between the
source framework and Qualcomm® Neural Processing SDK can happen. For such cases it
helps to override the value of epsilon in the source
model to a much smaller value.</div>
</div>
</li>
</ul>
</li>
<li><p><strong>Pad</strong></p>
<ul class="simple">
<li><p>For the DSP runtime,</p>
<ul>
<li><p>does not support non 4D padding inputs.</p></li>
<li><p>does not support padding along batch.</p></li>
<li><p>does not support padding along depth for reflect
padding.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Power</strong></p>
<ul class="simple">
<li><p>Power layer is only supported on DSP.</p></li>
</ul>
</li>
<li><p><strong>Proposal</strong></p>
<ul class="simple">
<li><p>Proposal layer is not supported on the GPU.</p></li>
<li><p>Only batch of 1 is supported.</p></li>
</ul>
</li>
<li><p><strong>ROI Pooling</strong></p>
<ul class="simple">
<li><p>ROI Pooling is not supported on the GPU.</p></li>
<li><p>For DSP runtime, the input to the ROI Pooling layer must
be a Proposal layer or an OPAQUE Input layer.</p></li>
<li><p>Only batch of 1 is supported.</p></li>
</ul>
</li>
<li><p><strong>Scale</strong></p>
<ul class="simple">
<li><p>Scale is only supported on the DSP.</p></li>
<li><p>For DSP runtime, only channel scaling is supported.</p></li>
</ul>
</li>
<li><p><strong>Slice</strong></p>
<ul class="simple">
<li><p>Currently does not support creation of a slice layer
without slice points defined.</p></li>
</ul>
</li>
<li><p><strong>Tile</strong></p>
<ul class="simple">
<li><p>The Tile layer will currently be displayed as a
“Concatenation” layer when the topology of a network
containing it is viewed using snpe-dlc-info.</p></li>
</ul>
</li>
<li><p><strong>UDO</strong></p>
<ul class="simple">
<li><p><strong>DSP runtime</strong></p>
<ul>
<li><p>Qualcomm® Neural Processing SDK DSP requires a quantized model if the UDO has at
least one quantized output.</p></li>
<li><p>The data types supported in DSP UDO layers are
FLOAT_32 and UINT_8 (quantized with TF schema).</p></li>
</ul>
</li>
<li><p><strong>GPU runtime</strong></p>
<ul>
<li><p>Only 16-bit floating point (OpenCL half) activations
are supported in the network.</p></li>
<li><p>The only data type supported for activation tensors in
GPU UDO layers is FLOAT_16.</p></li>
</ul>
</li>
<li><p><strong>CPU runtime</strong></p>
<ul>
<li><p>CPU runtime always operations with full precision
(FP32) tensors.</p></li>
<li><p>The only data type supported for activation tensors in
CPU UDO layers is FLOAT_32.</p></li>
</ul>
</li>
<li><p><strong>Package Generation</strong></p>
<ul>
<li><p>Multiple UDOs cannot be defined in a single config
file if they are intended to be used with core type =
DSP.
In this case users are required to create one config
file per UDO and generate separate packages with each
op. This restriction does not apply to core types CPU
or GPU.</p></li>
<li><p>A tensor parameter in a UDO definition can be
expressed with only one data type (e.g: either
FLOAT_32 or FLOAT_16 but not both).
Users wanting to use their UDOs on multiple runtimes
with different data types may be required to create
separated config files per data type and generate
multiple corresponding packages.</p></li>
</ul>
</li>
<li><p><strong>Application</strong></p>
<ul>
<li><p>UDO integration is supported only with native C APIs.
Java extensions are not available in this release.
Users who want to integrate UDOs into Android
applications will have to interface with Qualcomm® Neural Processing SDK APIs at
the JNI level in order to take advantage of this
functionality.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="rubric" id="tool-limitations">Tool Limitations</p>
<ul class="simple">
<li><p>Default input raw datatype is float32.</p></li>
<li><p><strong>snpe-net-run</strong></p>
<ul>
<li><p>Default profiling level is detailed.</p></li>
</ul>
</li>
<li><p><strong>snpe_bench.py</strong></p>
<ul>
<li><p>Default profiling level is basic.</p></li>
</ul>
</li>
<li><p><strong>snpe-dlc-info</strong></p>
<ul>
<li><p>For deconvolution layers, the num filters value shown is
actually num filters / group.</p>
<ul>
<li><p>Example: snpe-dlc-info shows num filters as 1 for a
deconvolution layer with num_output of 11 and group of
11.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>snpe-tensorflow-to-dlc</strong></p>
<ul>
<li><p>The TensorFlow converter does not support conversion of
TensorFlow graphs that have been quantized using
TensorFlow tools. In order to quantize a TensorFlow
model, run the TensorFlow converter
(<a class="reference external" href="tools.html#tools_snpe-tensorflow-to-dlc">snpe-tensorflow-to-dlc</a>)
first, then run
<a class="reference external" href="tools.html#tools_snpe-dlc-quantize">snpe-dlc-quantize</a>
on the DLC file generated by the TensorFlow converter.</p></li>
<li><p>Convolution</p>
<ul>
<li><p>BiasAdd node is optional and when missing a bias of
zeros will be added.</p></li>
</ul>
</li>
<li><p>Concat</p>
<ul>
<li><p>Concat node must have at least 2 non Const inputs.</p></li>
</ul>
</li>
<li><p>ElementWise Sum/Mul/Max</p>
<ul>
<li><p>Must be the only operation within it’s scope.</p></li>
<li><p>Does not support scalar operands.</p></li>
</ul>
</li>
<li><p>Fully Connected</p>
<ul>
<li><p>Inputs to MatMul operation must be 1D.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="appx_ref.html" class="btn btn-neutral float-right" title="References" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="api.html" class="btn btn-neutral float-left" title="API" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020-2023, Qualcomm Technologies, Inc..

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>