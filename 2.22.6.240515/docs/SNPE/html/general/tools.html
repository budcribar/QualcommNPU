

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Tools &mdash; Snapdragon Neural Processing Engine SDK</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/custom_css.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Debug Tools" href="usergroup11.html" />
    <link rel="prev" title="Inference Accuracy" href="accuracy.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> Qualcomm® Neural Processing SDK
          

          
          </a>

          
            
            
              <div class="version">
                v2.22.6
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="revision_history.html">Revision History</a></li>
<li class="toctree-l1"><a class="reference internal" href="revision_history_windows.html">Revision History - Windows</a></li>
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="setup.html">Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="usergroup1.html">Network Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="usergroup5.html">Input Data and Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="usergroup6.html">Tutorials and Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="usergroup10.html">Benchmarking and Accuracy</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Tools</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#model-conversion">Model Conversion</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#snpe-onnx-to-dlc">snpe-onnx-to-dlc</a></li>
<li class="toctree-l3"><a class="reference internal" href="#snpe-pytorch-to-dlc">snpe-pytorch-to-dlc</a></li>
<li class="toctree-l3"><a class="reference internal" href="#snpe-tensorflow-to-dlc">snpe-tensorflow-to-dlc</a></li>
<li class="toctree-l3"><a class="reference internal" href="#snpe-tflite-to-dlc">snpe-tflite-to-dlc</a></li>
<li class="toctree-l3"><a class="reference internal" href="#qairt-converter">qairt-converter</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#model-preparation">Model Preparation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#snpe-dlc-graph-prepare">snpe-dlc-graph-prepare</a></li>
<li class="toctree-l3"><a class="reference internal" href="#snpe-dlc-quant">snpe-dlc-quant</a></li>
<li class="toctree-l3"><a class="reference internal" href="#snpe-dlc-quantize">snpe-dlc-quantize</a></li>
<li class="toctree-l3"><a class="reference internal" href="#snpe-udo-package-generator">snpe-udo-package-generator</a></li>
<li class="toctree-l3"><a class="reference internal" href="#qairt-quantizer">qairt-quantizer</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#execution">Execution</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#snpe-net-run">snpe-net-run</a></li>
<li class="toctree-l3"><a class="reference internal" href="#snpe-parallel-run">snpe-parallel-run</a></li>
<li class="toctree-l3"><a class="reference internal" href="#snpe-throughput-net-run">snpe-throughput-net-run</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#analysis">Analysis</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#snpe-diagview">snpe-diagview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#snpe-dlc-diff">snpe-dlc-diff</a></li>
<li class="toctree-l3"><a class="reference internal" href="#snpe-dlc-info">snpe-dlc-info</a></li>
<li class="toctree-l3"><a class="reference internal" href="#snpe-dlc-viewer">snpe-dlc-viewer</a></li>
<li class="toctree-l3"><a class="reference internal" href="#snpe-platform-validator">snpe-platform-validator</a></li>
<li class="toctree-l3"><a class="reference internal" href="#snpe-platform-validator-py">snpe-platform-validator-py</a></li>
<li class="toctree-l3"><a class="reference internal" href="#snpe-bench-py">snpe_bench.py</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="usergroup11.html">Debug Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="api.html">API</a></li>
<li class="toctree-l1"><a class="reference internal" href="limitations.html">Limitations</a></li>
<li class="toctree-l1"><a class="reference internal" href="appx_ref.html">References</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Qualcomm® Neural Processing SDK</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Tools</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="tools">
<h1>Tools<a class="headerlink" href="#tools" title="Permalink to this heading">¶</a></h1>
<div class="line-block">
<div class="line">This page describes the SDK tools and features for Linux/Android and Windows developers.</div>
</div>
<table class="docutils align-default">
<colgroup>
<col style="width: 9%" />
<col style="width: 42%" />
<col style="width: 4%" />
<col style="width: 8%" />
<col style="width: 4%" />
<col style="width: 8%" />
<col style="width: 12%" />
<col style="width: 12%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head" rowspan="3"><p>Category</p></th>
<th class="head" rowspan="3"><p>Tool</p></th>
<th class="head" colspan="6"><p>Developer</p></th>
</tr>
<tr class="row-even"><th class="head" colspan="3"><p>Linux/Android</p></th>
<th class="head" colspan="3"><p>Windows</p></th>
</tr>
<tr class="row-odd"><th class="head"><p>Ubuntu</p></th>
<th class="head"><p>WSL x86_64 (*)</p></th>
<th class="head"><p>Device</p></th>
<th class="head"><p>WSL x86_64 (*)</p></th>
<th class="head"><p>Windows x86_64</p></th>
<th class="head"><p>Windows on Snapdragon</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td rowspan="5"><p>Model Conversion</p></td>
<td><p><a class="reference external" href="tools.html#snpe-onnx-to-dlc">snpe-onnx-to-dlc</a></p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>Yes**</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="tools.html#snpe-pytorch-to-dlc">snpe-pytorch-to-dlc</a></p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td></td>
<td><p>Yes</p></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="tools.html#snpe-tensorflow-to-dlc">snpe-tensorflow-to-dlc</a></p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>Yes**</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="tools.html#snpe-tflite-to-dlc">snpe-tflite-to-dlc</a></p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td></td>
<td><p>Yes</p></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="tools.html#qairt-converter">qairt-converter</a></p></td>
<td><p>Yes</p></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td rowspan="5"><p>Model Preparation</p></td>
<td><p><a class="reference external" href="tools.html#snpe-dlc-graph-prepare">snpe-dlc-graph-prepare</a></p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>Yes**</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="tools.html#snpe-dlc-quant">snpe-dlc-quant</a></p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="tools.html#snpe-dlc-quantize">snpe-dlc-quantize</a></p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td></td>
<td><p>Yes</p></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="tools.html#snpe-udo-package-generator">snpe-udo-package-generator</a></p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td></td>
<td><p>Yes</p></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="tools.html#qairt-quantizer">qairt-quantizer</a></p></td>
<td><p>Yes</p></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td rowspan="3"><p>Execution</p></td>
<td><p><a class="reference external" href="tools.html#snpe-net-run">snpe-net-run</a></p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="tools.html#snpe-parallel-run">snpe-parallel-run</a></p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td></td>
<td><p>Yes</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="tools.html#snpe-throughput-net-run">snpe-throughput-net-run</a></p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
</tr>
<tr class="row-odd"><td rowspan="10"><p>Analysis</p></td>
<td><p><a class="reference external" href="tools.html#snpe-diagview">snpe-diagview</a></p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td></td>
<td></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="tools.html#snpe-dlc-diff">snpe-dlc-diff</a></p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>Yes**</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="tools.html#snpe-dlc-info">snpe-dlc-info</a></p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>Yes**</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="tools.html#snpe-dlc-viewer">snpe-dlc-viewer</a></p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>Yes**</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="tools.html#snpe-platform-validator">snpe-platform-validator</a></p></td>
<td></td>
<td></td>
<td><p>Yes</p></td>
<td></td>
<td></td>
<td><p>Yes</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="tools.html#snpe-platform-validator-py">snpe-platform-validator-py</a></p></td>
<td><p>Yes</p></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="tools.html#snpe-bench-py">snpe_bench.py</a></p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="debug_tools_snpe-architecture-checker.html">Architecture Checker (Experimental)</a></p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>Yes**</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="debug_tools_snpe-quantization-checker.html">Quantization Checker (Experimental)</a></p></td>
<td><p>Yes</p></td>
<td></td>
<td></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="debug_tools_snpe-accuracy-debugger.html">Accuracy Debugger (Experimental)</a></p></td>
<td><p>Yes</p></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<div class="notes docutils container">
<div class="admonition note">
<p class="admonition-title">Note</p>
<div class="line-block">
<div class="line">* Binary used in Windows WSL x86_64 is from Ubuntu, related installation documentation
please reference to <a class="reference external" href="setup.html">Qualcomm (R) Neural Processing SDK Setup</a></div>
<div class="line">* For ARM64X in Windows, only snpe-net-run and snpe-throughput-net-run are supported.</div>
<div class="line">** Requires the python scripts and the executables from the Windows x86_64 binary folder.</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<div class="line-block">
<div class="line">* When using converter tools in Windows PowerShell, make sure a virtual environment
with the required packages is actived and converters are executed via <strong>python</strong>,
as shown in the following example.</div>
<div class="line-block">
<div class="line">(venv-3.10) &gt; python snpe-onnx-to-dlc &lt;options&gt;</div>
</div>
</div>
</div>
</div>
<div class="section" id="model-conversion">
<h2>Model Conversion<a class="headerlink" href="#model-conversion" title="Permalink to this heading">¶</a></h2>
<div class="section" id="snpe-onnx-to-dlc">
<h3>snpe-onnx-to-dlc<a class="headerlink" href="#snpe-onnx-to-dlc" title="Permalink to this heading">¶</a></h3>
<p>snpe-onnx-to-dlc converts a serialized ONNX model into a
DLC file.</p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>usage: snpe-onnx-to-dlc [--out_node OUT_NAMES] [--input_type INPUT_NAME INPUT_TYPE]
                        [--input_dtype INPUT_NAME INPUT_DTYPE] [--input_encoding INPUT_ENCODING [INPUT_ENCODING ...]]
                        [--input_layout INPUT_NAME INPUT_LAYOUT] [--custom_io CUSTOM_IO]
                        [--dry_run [DRY_RUN]] [-d INPUT_NAME INPUT_DIM] [-n] [-b BATCH]
                        [-s SYMBOL_NAME VALUE]
                        [--dump_custom_io_config_template DUMP_CUSTOM_IO_CONFIG_TEMPLATE]
                        [--quantization_overrides QUANTIZATION_OVERRIDES] [--keep_quant_nodes]
                        [--disable_batchnorm_folding] [--keep_disconnected_nodes] --input_network
                        INPUT_NETWORK [-h] [--debug [DEBUG]] [-o OUTPUT_PATH]
                        [--copyright_file COPYRIGHT_FILE] [--float_bw FLOAT_BW]
                        [--model_version MODEL_VERSION]
                        [--validation_target RUNTIME_TARGET PROCESSOR_TARGET] [--strict]
                        [--udo_config_paths CUSTOM_OP_CONFIG_PATHS [CUSTOM_OP_CONFIG_PATHS ...]]
                        [--op_package_lib OP_PACKAGE_LIB]
                        [--converter_op_package_lib CONVERTER_OP_PACKAGE_LIB]
                        [-p PACKAGE_NAME | --op_package_config CUSTOM_OP_CONFIG_PATHS [CUSTOM_OP_CONFIG_PATHS ...]]

Script to convert ONNX model into a DLC file.

required arguments:
    --input_network INPUT_NETWORK, -i INPUT_NETWORK
                        Path to the source framework model.

optional arguments:
    --out_node OUT_NAMES, --out_name OUT_NAMES
                        Name of the graph&#39;s output Tensor Names. Multiple output names should be
                        provided separately like:
                            --out_name out_1 --out_name out_2
    --input_type INPUT_NAME INPUT_TYPE, -t INPUT_NAME INPUT_TYPE
                        Type of data expected by each input op/layer. Type for each input is
                        |default| if not specified. For example: &quot;data&quot; image.Note that the quotes
                        should always be included in order to handle special characters, spaces,etc.
                        For multiple inputs specify multiple --input_type on the command line.
                        Eg:
                            --input_type &quot;data1&quot; image --input_type &quot;data2&quot; opaque
                        These options get used by DSP runtime and following descriptions state how
                        input will be handled for each option.
                        Image:
                        Input is float between 0-255 and the input&#39;s mean is 0.0f and the input&#39;s
                        max is 255.0f. We will cast the float to uint8ts and pass the uint8ts to the
                        DSP.
                        Default:
                        Pass the input as floats to the dsp directly and the DSP will quantize it.
                        Opaque:
                        Assumes input is float because the consumer layer(i.e next layer) requires
                        it as float, therefore it won&#39;t be quantized.
                        Choices supported:
                            image
                            default
                            opaque
    --input_dtype INPUT_NAME INPUT_DTYPE
                        The names and datatype of the network input layers specified in the format
                        [input_name datatype], for example:
                            &#39;data&#39; &#39;float32&#39;
                        Default is float32 if not specified
                        Note that the quotes should always be included in order to handlespecial
                        characters, spaces, etc.
                        For multiple inputs specify multiple --input_dtype on the command line like:
                            --input_dtype &#39;data1&#39; &#39;float32&#39; --input_dtype &#39;data2&#39; &#39;float32&#39;
    --input_encoding INPUT_ENCODING [INPUT_ENCODING ...], -e INPUT_ENCODING [INPUT_ENCODING ...]
                        Usage:     --input_encoding &quot;INPUT_NAME&quot; INPUT_ENCODING_IN
                        [INPUT_ENCODING_OUT]
                        Input encoding of the network inputs. Default is bgr.
                        e.g.
                            --input_encoding &quot;data&quot; rgba
                        Quotes must wrap the input node name to handle special characters,
                        spaces, etc. To specify encodings for multiple inputs, invoke
                        --input_encoding for each one.
                        e.g.
                            --input_encoding &quot;data1&quot; rgba --input_encoding &quot;data2&quot; other
                        Optionally, an output encoding may be specified for an input node by
                        providing a second encoding. The default output encoding is bgr.
                        e.g.
                            --input_encoding &quot;data3&quot; rgba rgb
                        Input encoding types:
                            image color encodings: bgr,rgb, nv21, nv12, ...
                            time_series: for inputs of rnn models;
                            other: not available above or is unknown.
                        Supported encodings:
                            bgr
                            rgb
                            rgba
                            argb32
                            nv21
                            nv12
                            time_series
                            other
    --input_layout INPUT_NAME INPUT_LAYOUT, -l INPUT_NAME INPUT_LAYOUT
                        Layout of each input tensor. If not specified, it will use the default
                        based on the Source Framework, shape of input and input encoding.
                        Accepted values are-
                            NCDHW, NDHWC, NCHW, NHWC, NFC, NCF, NTF, TNF, NF, NC, F, NONTRIVIAL
                        N = Batch, C = Channels, D = Depth, H = Height, W = Width, F = Feature, T =
                        Time
                        NDHWC/NCDHW used for 5d inputs
                        NHWC/NCHW used for 4d image-like inputs
                        NFC/NCF used for inputs to Conv1D or other 1D ops
                        NTF/TNF used for inputs with time steps like the ones used for LSTM op
                        NF used for 2D inputs, like the inputs to Dense/FullyConnected layers
                        NC used for 2D inputs with 1 for batch and other for Channels (rarely used)
                        F used for 1D inputs, e.g. Bias tensor
                        NONTRIVIAL for everything elseFor multiple inputs specify multiple
                        --input_layout on the command line.
                        Eg:
                            --input_layout &quot;data1&quot; NCHW --input_layout &quot;data2&quot; NCHW
                        Note: This flag does not set the layout of the input tensor in the converted DLC.
                        Please use --custom_io for that.
    --custom_io CUSTOM_IO
                        Use this option to specify a yaml file for custom IO.
    --dry_run [DRY_RUN] Evaluates the model without actually converting any ops, and returns
                        unsupported ops/attributes as well as unused inputs and/or outputs if any.
                        Leave empty or specify &quot;info&quot; to see dry run as a table, or specify &quot;debug&quot;
                        to show more detailed messages only&quot;
    -d INPUT_NAME INPUT_DIM, --input_dim INPUT_NAME INPUT_DIM
                        The name and dimension of all the input buffers to the network specified in
                        the format [input_name comma-separated-dimensions],
                        for example: &#39;data&#39; 1,224,224,3.
                        Note that the quotes should always be included in order to handle special
                        characters, spaces, etc.
                        NOTE: This feature works only with Onnx 1.6.0 and above
    -n, --no_simplification
                        Do not attempt to simplify the model automatically. This may prevent some
                        models from properly converting
                        when sequences of unsupported static operations are present.
    -b BATCH, --batch BATCH
                        The batch dimension override. This will take the first dimension of all
                        inputs and treat it as a batch dim, overriding it with the value provided
                        here. For example:
                        --batch 6
                        will result in a shape change from [1,3,224,224] to [6,3,224,224].
                        If there are inputs without batch dim this should not be used and each input
                        should be overridden independently using -d option for input dimension
                        overrides.
    -s SYMBOL_NAME VALUE, --define_symbol SYMBOL_NAME VALUE
                        This option allows overriding specific input dimension symbols. For instance
                        you might see input shapes specified with variables such as :
                        data: [1,3,height,width]
                        To override these simply pass the option as:
                        --define_symbol height 224 --define_symbol width 448
                        which results in dimensions that look like:
                        data: [1,3,224,448]
    --dump_custom_io_config_template DUMP_CUSTOM_IO_CONFIG_TEMPLATE
                        Dumps the yaml template for Custom I/O configuration. This file canbe edited
                        as per the custom requirements and passed using the option --custom_ioUse
                        this option to specify a yaml file to which the custom IO config template is
                        dumped.
    --disable_batchnorm_folding
    --keep_disconnected_nodes
                        Disable Optimization that removes Ops not connected to the main graph.
                        This optimization uses output names provided over commandline OR
                        inputs/outputs extracted from the Source model to determine the main graph
    -h, --help          show this help message and exit
    --debug [DEBUG]     Run the converter in debug mode.
    -o OUTPUT_PATH, --output_path OUTPUT_PATH
                        Path where the converted Output model should be saved.If not specified, the
                        converter model will be written to a file with same name as the input model
    --copyright_file COPYRIGHT_FILE
                        Path to copyright file. If provided, the content of the file will be added
                        to the output model.
    --float_bw FLOAT_BW Use the --float_bw option to select the bitwidth to use when using float for
                        parameters(weights/bias) and activations for all ops  or specific Op (via
                        encodings) selected through encoding, either 32 (default) or 16.
    --model_version MODEL_VERSION
                        User-defined ASCII string to identify the model, only first 64 bytes will be
                        stored
    --validation_target RUNTIME_TARGET PROCESSOR_TARGET
                        A combination of processor and runtime target against which model will be
                        validated.
                        Choices for RUNTIME_TARGET:
                           {cpu, gpu, dsp}.
                        Choices for PROCESSOR_TARGET:
                           {snapdragon_801, snapdragon_820, snapdragon_835}.
                        If not specified, will validate model against {snapdragon_820,
                        snapdragon_835} across all runtime targets.
    --strict            If specified, will validate in strict mode whereby model will not be
                        produced if it violates constraints of the specified validation target. If
                        not specified, will validate model in permissive mode against the specified
                        validation target.
    --udo_config_paths CUSTOM_OP_CONFIG_PATHS [CUSTOM_OP_CONFIG_PATHS ...], -udo CUSTOM_OP_CONFIG_PATHS [CUSTOM_OP_CONFIG_PATHS ...]
                        Path to the UDO configs (space separated, if multiple)

Quantizer Options:
    --quantization_overrides QUANTIZATION_OVERRIDES
                        Use this option to specify a json file with parameters to use for
                        quantization. These will override any quantization data carried from
                        conversion (eg TF fake quantization) or calculated during the normal
                        quantization process. Format defined as per AIMET specification.
    --keep_quant_nodes  Use this option to keep activation quantization nodes in the graph rather
                        than stripping them.

Custom Op Package Options:
    --op_package_lib OP_PACKAGE_LIB, -opl OP_PACKAGE_LIB
                        Use this argument to pass an op package library for quantization. Must be in
                        the form &lt;op_package_lib_path:interfaceProviderName&gt; and be separated by a
                        comma for multiple package libs
    --converter_op_package_lib CONVERTER_OP_PACKAGE_LIB, -cpl CONVERTER_OP_PACKAGE_LIB
                        Path to converter op package library compiled by the OpPackage generator.
    -p PACKAGE_NAME, --package_name PACKAGE_NAME
                        A global package name to be used for each node in the Model.cpp file.
                        Defaults to Qnn header defined package name
    --op_package_config CUSTOM_OP_CONFIG_PATHS [CUSTOM_OP_CONFIG_PATHS ...], -opc CUSTOM_OP_CONFIG_PATHS [CUSTOM_OP_CONFIG_PATHS ...]
                        Path to a Qnn Op Package XML configuration file that contains user defined
                        custom operations.

Note: Only one of: {&#39;op_package_config&#39;, &#39;package_name&#39;} can be specified
</pre></div>
</div>
<p>For more information, see <a class="reference external" href="model_conv_onnx.html">ONNX Model
Conversion</a></p>
<div class="line-block">
<div class="line">Additional details:</div>
</div>
<blockquote>
<div><ul>
<li><p><em>input_layout argument:</em></p>
<blockquote>
<div><ul class="simple">
<li><p>Used with TF2Onnx or Keras2Onnx models when the input layout is NHWC.
Onnx converter assumes that 4D inputs to the model are used by CNNs and are in NCHW format.
For Keras2Onnx or TF2Onnx models, where the input is NHWC followed most likely by a transpose
to NCHW, the converter will fail to successfully convert and optimize the model without the
use of this argument.</p></li>
</ul>
</div></blockquote>
</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="snpe-pytorch-to-dlc">
<h3>snpe-pytorch-to-dlc<a class="headerlink" href="#snpe-pytorch-to-dlc" title="Permalink to this heading">¶</a></h3>
<p>snpe-pytorch-to-dlc converts a serialized PyTorch model into a
DLC file.</p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>usage: snpe-pytorch-to-dlc -d INPUT_NAME INPUT_DIM [--out_node OUT_NAMES]
                           [--input_type INPUT_NAME INPUT_TYPE]
                           [--input_dtype INPUT_NAME INPUT_DTYPE] [--input_encoding INPUT_ENCODING [INPUT_ENCODING ...]]
                           [--input_layout INPUT_NAME INPUT_LAYOUT] [--custom_io CUSTOM_IO]
                           [--dump_relay DUMP_RELAY]
                           [--quantization_overrides QUANTIZATION_OVERRIDES] [--keep_quant_nodes]
                           [--disable_batchnorm_folding] [--keep_disconnected_nodes] --input_network
                           INPUT_NETWORK [-h] [--debug [DEBUG]] [-o OUTPUT_PATH]
                           [--copyright_file COPYRIGHT_FILE] [--float_bw FLOAT_BW]
                           [--model_version MODEL_VERSION]
                           [--validation_target RUNTIME_TARGET PROCESSOR_TARGET] [--strict]
                           [--udo_config_paths CUSTOM_OP_CONFIG_PATHS [CUSTOM_OP_CONFIG_PATHS ...]]
                           [--op_package_lib OP_PACKAGE_LIB]
                           [--converter_op_package_lib CONVERTER_OP_PACKAGE_LIB]
                           [-p PACKAGE_NAME | --op_package_config CUSTOM_OP_CONFIG_PATHS [CUSTOM_OP_CONFIG_PATHS ...]]


Script to convert PyTorch model into DLC

required arguments:
    -d INPUT_NAME INPUT_DIM, --input_dim INPUT_NAME INPUT_DIM
                        The names and dimensions of the network input layers specified in the format
                        [input_name comma-separated-dimensions], for example:
                            &#39;data&#39; 1,3,224,224
                        Note that the quotes should always be included in order to handle special
                        characters, spaces, etc. For multiple inputs specify multiple --input_dim on the command line like:
                            --input_dim &#39;data1&#39; 1,3,224,224 --input_dim &#39;data2&#39; 1,50,100,3
    --input_network INPUT_NETWORK, -i INPUT_NETWORK
                        Path to the source framework model.

optional arguments:
    --out_node OUT_NAMES, --out_name OUT_NAMES
                        Name of the graph&#39;s output Tensor Names. Multiple output names should be
                        provided separately like:
                            --out_name out_1 --out_name out_2
    --input_type INPUT_NAME INPUT_TYPE, -t INPUT_NAME INPUT_TYPE
                        Type of data expected by each input op/layer. Type for each input is
                        |default| if not specified. For example: &quot;data&quot; image.Note that the quotes
                        should always be included in order to handle special characters, spaces,etc.
                        For multiple inputs specify multiple --input_type on the command line.
                        Eg:
                           --input_type &quot;data1&quot; image --input_type &quot;data2&quot; opaque
                        These options get used by DSP runtime and following descriptions state how
                        input will be handled for each option.
                        Image:
                        Input is float between 0-255 and the input&#39;s mean is 0.0f and the input&#39;s
                        max is 255.0f. We will cast the float to uint8ts and pass the uint8ts to the
                        DSP.
                        Default:
                        Pass the input as floats to the dsp directly and the DSP will quantize it.
                        Opaque:
                        Assumes input is float because the consumer layer(i.e next layer) requires
                        it as float, therefore it won&#39;t be quantized.
                        Choices supported:
                           image
                           default
                           opaque
    --input_dtype INPUT_NAME INPUT_DTYPE
                        The names and datatype of the network input layers specified in the format
                        [input_name datatype], for example:
                            &#39;data&#39; &#39;float32&#39;
                        Default is float32 if not specified.
                        Note that the quotes should always be included in order to handle special
                        characters, spaces, etc.
                        For multiple inputs specify multiple --input_dtype on the command line like:
                            --input_dtype &#39;data1&#39; &#39;float32&#39; --input_dtype &#39;data2&#39; &#39;float32&#39;
  --input_encoding INPUT_ENCODING [INPUT_ENCODING ...], -e INPUT_ENCODING [INPUT_ENCODING ...]
                        Usage:     --input_encoding &quot;INPUT_NAME&quot; INPUT_ENCODING_IN
                        [INPUT_ENCODING_OUT]
                        Input encoding of the network inputs. Default is bgr.
                        e.g.
                           --input_encoding &quot;data&quot; rgba
                        Quotes must wrap the input node name to handle special characters,
                        spaces, etc. To specify encodings for multiple inputs, invoke
                        --input_encoding for each one.
                        e.g.
                            --input_encoding &quot;data1&quot; rgba --input_encoding &quot;data2&quot; other
                        Optionally, an output encoding may be specified for an input node by
                        providing a second encoding. The default output encoding is bgr.
                        e.g.
                            --input_encoding &quot;data3&quot; rgba rgb
                        Input encoding types:
                             image color encodings: bgr,rgb, nv21, nv12, ...
                            time_series: for inputs of rnn models;
                            other: not available above or is unknown.
                        Supported encodings:
                            bgr
                            rgb
                            rgba
                            argb32
                            nv21
                            nv12
                            time_series
                            other
    --input_layout INPUT_NAME INPUT_LAYOUT, -l INPUT_NAME INPUT_LAYOUT
                        Layout of each input tensor. If not specified, it will use the default
                        based on the Source Framework, shape of input and input encoding.
                        Accepted values are-
                            NCDHW, NDHWC, NCHW, NHWC, NFC, NCF, NTF, TNF, NF, NC, F, NONTRIVIAL
                        N = Batch, C = Channels, D = Depth, H = Height, W = Width, F = Feature, T = Time
                        NDHWC/NCDHW used for 5d inputs
                        NHWC/NCHW used for 4d image-like inputs
                        NFC/NCF used for inputs to Conv1D or other 1D ops
                        NTF/TNF used for inputs with time steps like the ones used for LSTM op
                        NF used for 2D inputs, like the inputs to Dense/FullyConnected layers
                        NC used for 2D inputs with 1 for batch and other for Channels (rarely used)
                        F used for 1D inputs, e.g. Bias tensor
                        NONTRIVIAL for everything elseFor multiple inputs specify multiple
                        --input_layout on the command line.
                        Eg:
                            --input_layout &quot;data1&quot; NCHW --input_layout &quot;data2&quot; NCHW
                        Note: This flag does not set the layout of the input tensor in the converted DLC.
    --custom_io CUSTOM_IO
                        Use this option to specify a yaml file for custom IO.
    --dump_relay DUMP_RELAY
                        Dump Relay ASM and Params at the path provided with the argument
                        Usage: --dump_relay &lt;path_to_dump&gt;
    --disable_batchnorm_folding
    --keep_disconnected_nodes
                        Disable Optimization that removes Ops not connected to the main graph.
                        This optimization uses output names provided over commandline OR
                        inputs/outputs extracted from the Source model to determine the main graph
    -h, --help            show this help message and exit
    -o OUTPUT_PATH, --output_path OUTPUT_PATH
                        Path where the converted Output model should be saved.If not specified, the
                        converter model will be written to a file with same name as the input model
    --copyright_file COPYRIGHT_FILE
                        Path to copyright file. If provided, the content of the file will be added
                        to the output model.
    --model_version MODEL_VERSION
                        User-defined ASCII string to identify the model, only first 64 bytes will be
                        stored
    --validation_target RUNTIME_TARGET PROCESSOR_TARGET
                        A combination of processor and runtime target against which model will be
                        validated.
                        Choices for RUNTIME_TARGET:
                           {cpu, gpu, dsp}.
                        Choices for PROCESSOR_TARGET:
                           {snapdragon_801, snapdragon_820, snapdragon_835}.
                        If not specified, will validate model against {snapdragon_820,
                        snapdragon_835} across all runtime targets.
    --strict            If specified, will validate in strict mode whereby model will not be
                        produced if it violates constraints of the specified validation target. If
                        not specified, will validate model in permissive mode against the specified
                        validation target.
    --udo_config_paths  UDO_CONFIG_PATHS [UDO_CONFIG_PATHS ...], -udo UDO_CONFIG_PATHS
                        [UDO_CONFIG_PATHS ...]
                            Path to the UDO configs (space separated, if multiple)

Quantizer Options:
    --quantization_overrides QUANTIZATION_OVERRIDES
                        Use this option to specify a json file with parameters to use for
                        quantization. These will override any quantization data carried from
                        conversion (eg TF fake quantization) or calculated during the normal
                        quantization process. Format defined as per AIMET specification.
    --keep_quant_nodes  Use this option to keep activation quantization nodes in the graph rather
                        than stripping them.

Custom Op Package Options:
    --op_package_lib OP_PACKAGE_LIB, -opl OP_PACKAGE_LIB
                        Use this argument to pass an op package library for quantization. Must be in
                        the form &lt;op_package_lib_path:interfaceProviderName&gt; and be separated by a
                        comma for multiple package libs
    --converter_op_package_lib CONVERTER_OP_PACKAGE_LIB, -cpl CONVERTER_OP_PACKAGE_LIB
                        Path to converter op package library compiled by the OpPackage generator.
    -p PACKAGE_NAME, --package_name PACKAGE_NAME
                        A global package name to be used for each node in the Model.cpp file.
                        Defaults to Qnn header defined package name
    --op_package_config CUSTOM_OP_CONFIG_PATHS [CUSTOM_OP_CONFIG_PATHS ...], -opc CUSTOM_OP_CONFIG_PATHS [CUSTOM_OP_CONFIG_PATHS ...]
                        Path to a Qnn Op Package XML configuration file that contains user defined
                        custom operations.

Note: Only one of: {&#39;package_name&#39;, &#39;op_package_config&#39;} can be specified
</pre></div>
</div>
<p>For more information, see <a class="reference external" href="model_conv_pytorch.html">PyTorch Model
Conversion</a></p>
<div class="line-block">
<div class="line"><br /></div>
</div>
</div>
<div class="section" id="snpe-tensorflow-to-dlc">
<h3>snpe-tensorflow-to-dlc<a class="headerlink" href="#snpe-tensorflow-to-dlc" title="Permalink to this heading">¶</a></h3>
<p>snpe-tensorflow-to-dlc converts a TensorFlow model into a
DLC file.</p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>usage: snpe-tensorflow-to-dlc -d INPUT_NAME INPUT_DIM --out_node OUT_NAMES
                              [--input_type INPUT_NAME INPUT_TYPE]
                              [--input_dtype INPUT_NAME INPUT_DTYPE] [--input_encoding  [ ...]]
                              [--input_layout INPUT_NAME INPUT_LAYOUT] [--custom_io CUSTOM_IO]
                              [--show_unconsumed_nodes] [--saved_model_tag SAVED_MODEL_TAG]
                              [--saved_model_signature_key SAVED_MODEL_SIGNATURE_KEY]
                              [--quantization_overrides QUANTIZATION_OVERRIDES] [--keep_quant_nodes]
                              [--disable_batchnorm_folding] [--keep_disconnected_nodes]
                              --input_network INPUT_NETWORK [-h] [--debug [DEBUG]] [-o OUTPUT_PATH]
                              [--copyright_file COPYRIGHT_FILE] [--float_bw FLOAT_BW]
                              [--model_version MODEL_VERSION]
                              [--validation_target RUNTIME_TARGET PROCESSOR_TARGET] [--strict]
                              [--udo_config_paths CUSTOM_OP_CONFIG_PATHS [CUSTOM_OP_CONFIG_PATHS ...]]
                              [--op_package_lib OP_PACKAGE_LIB]
                              [--converter_op_package_lib CONVERTER_OP_PACKAGE_LIB]
                              [-p PACKAGE_NAME | --op_package_config CUSTOM_OP_CONFIG_PATHS [CUSTOM_OP_CONFIG_PATHS ...]]

Script to convert TF model into DLC.

required arguments:
    -d INPUT_NAME INPUT_DIM, --input_dim INPUT_NAME INPUT_DIM
                        The names and dimensions of the network input layers specified in the format
                        [input_name comma-separated-dimensions], for example:
                            &#39;data&#39; 1,224,224,3
                        Note that the quotes should always be included in order to handle special
                        characters, spaces, etc.
                        For multiple inputs specify multiple --input_dim on the command line like:
                            --input_dim &#39;data1&#39; 1,224,224,3 --input_dim &#39;data2&#39; 1,50,100,3
    --out_node OUT_NAMES, --out_name OUT_NAMES
                        Name of the graph&#39;s output Tensor Names. Multiple output names should be
                        provided separately like:
                            --out_name out_1 --out_name out_2
    --input_network INPUT_NETWORK, -i INPUT_NETWORK
                        Path to the source framework model.

optional arguments:
    --input_type INPUT_NAME INPUT_TYPE, -t INPUT_NAME INPUT_TYPE
                        Type of data expected by each input op/layer. Type for each input is
                        |default| if not specified. For example: &quot;data&quot; image.Note that the quotes
                        should always be included in order to handle special characters, spaces,etc.
                        For multiple inputs specify multiple --input_type on the command line.
                        Eg:
                           --input_type &quot;data1&quot; image --input_type &quot;data2&quot; opaque
                        These options get used by DSP runtime and following descriptions state how
                        input will be handled for each option.
                        Image:
                        Input is float between 0-255 and the input&#39;s mean is 0.0f and the input&#39;s
                        max is 255.0f. We will cast the float to uint8ts and pass the uint8ts to the
                        DSP.
                        Default:
                        Pass the input as floats to the dsp directly and the DSP will quantize it.
                        Opaque:
                        Assumes input is float because the consumer layer(i.e next layer) requires
                        it as float, therefore it won&#39;t be quantized.
                        Choices supported:
                           image
                           default
                           opaque
    --input_dtype INPUT_NAME INPUT_DTYPE
                        The names and datatype of the network input layers specified in the format
                        [input_name datatype], for example:
                            &#39;data&#39; &#39;float32&#39;
                        Default is float32 if not specified
                        Note that the quotes should always be included in order to handlespecial
                        characters, spaces, etc.
                        For multiple inputs specify multiple --input_dtype on the command line like:
                            --input_dtype &#39;data1&#39; &#39;float32&#39; --input_dtype &#39;data2&#39; &#39;float32&#39;
    --input_encoding INPUT_ENCODING [INPUT_ENCODING ...], -e INPUT_ENCODING [INPUT_ENCODING ...]
                        Usage:     --input_encoding &quot;INPUT_NAME&quot; INPUT_ENCODING_IN
                        [INPUT_ENCODING_OUT]
                        Input encoding of the network inputs. Default is bgr.
                        e.g.
                           --input_encoding &quot;data&quot; rgba
                        Quotes must wrap the input node name to handle special characters,
                        spaces, etc. To specify encodings for multiple inputs, invoke
                        --input_encoding for each one.
                        e.g.
                            --input_encoding &quot;data1&quot; rgba --input_encoding &quot;data2&quot; other
                        Optionally, an output encoding may be specified for an input node by
                        providing a second encoding. The default output encoding is bgr.
                        e.g.
                            --input_encoding &quot;data3&quot; rgba rgb
                        Input encoding types:
                            image color encodings: bgr,rgb, nv21, nv12, ...
                            time_series: for inputs of rnn models;
                            other: not available above or is unknown.
                        Supported encodings:
                            bgr
                            rgb
                            rgba
                            argb32
                            nv21
                            nv12
                            time_series
                            other
    --input_layout INPUT_NAME INPUT_LAYOUT, -l INPUT_NAME INPUT_LAYOUT
                        Layout of each input tensor. If not specified, it will use the default
                        based on the Source Framework, shape of input and input encoding.
                        Accepted values are-
                            NCDHW, NDHWC, NCHW, NHWC, NFC, NCF, NTF, TNF, NF, NC, F, NONTRIVIAL
                        N = Batch, C = Channels, D = Depth, H = Height, W = Width, F = Feature, T = Time
                        NDHWC/NCDHW used for 5d inputs
                        NHWC/NCHW used for 4d image-like inputs
                        NFC/NCF used for inputs to Conv1D or other 1D ops
                        NTF/TNF used for inputs with time steps like the ones used for LSTM op
                        NF used for 2D inputs, like the inputs to Dense/FullyConnected layers
                        NC used for 2D inputs with 1 for batch and other for Channels (rarely used)
                        F used for 1D inputs, e.g. Bias tensor
                        NONTRIVIAL for everything elseFor multiple inputs specify multiple
                        --input_layout on the command line.
                        Eg:
                            --input_layout &quot;data1&quot; NCHW --input_layout &quot;data2&quot; NCHW
                        Note: This flag does not set the layout of the input tensor in the converted DLC.
    --custom_io CUSTOM_IO
                        Use this option to specify a yaml file for custom IO.
    --show_unconsumed_nodes
                        Displays a list of unconsumed nodes, if there any are found. Nodes which are
                        unconsumed do not violate the structural fidelity of the generated graph.
    --saved_model_tag SAVED_MODEL_TAG
                        Specify the tag to seletet a MetaGraph from savedmodel. ex:
                        --saved_model_tag serve. Default value will be &#39;serve&#39; when it is not
                        assigned.
    --saved_model_signature_key SAVED_MODEL_SIGNATURE_KEY
                        Specify signature key to select input and output of the model. ex:
                        --saved_model_signature_key serving_default. Default value will be
                        &#39;serving_default&#39; when it is not assigned
    --disable_batchnorm_folding
    --keep_disconnected_nodes
                        Disable Optimization that removes Ops not connected to the main graph.
                        This optimization uses output names provided over commandline OR
                        inputs/outputs extracted from the Source model to determine the main graph
    -h, --help          show this help message and exit
    --debug [DEBUG]     Run the converter in debug mode.
    -o OUTPUT_PATH, --output_path OUTPUT_PATH
                        Path where the converted Output model should be saved.If not specified, the
                        converter model will be written to a file with same name as the input model
    --copyright_file COPYRIGHT_FILE
                        Path to copyright file. If provided, the content of the file will be added
                        to the output model.
    --float_bw FLOAT_BW   Use the --float_bw option to select the bitwidth to use when using float for
                        parameters(weights/bias) and activations for all ops  or specific Op (via
                        encodings) selected through encoding, either 32 (default) or 16.
    --model_version MODEL_VERSION
                        User-defined ASCII string to identify the model, only first 64 bytes will be
                        stored
    --validation_target RUNTIME_TARGET PROCESSOR_TARGET
                        A combination of processor and runtime target against which model will be
                        validated.
                        Choices for RUNTIME_TARGET:
                           {cpu, gpu, dsp}.
                        Choices for PROCESSOR_TARGET:
                           {snapdragon_801, snapdragon_820, snapdragon_835}.
                        If not specified, will validate model against {snapdragon_820,
                        snapdragon_835} across all runtime targets.
    --strict              If specified, will validate in strict mode whereby model will not be
                        produced if it violates constraints of the specified validation target. If
                        not specified, will validate model in permissive mode against the specified
                        validation target.
    --udo_config_paths CUSTOM_OP_CONFIG_PATHS [CUSTOM_OP_CONFIG_PATHS ...], -udo CUSTOM_OP_CONFIG_PATHS
                        [CUSTOM_OP_CONFIG_PATHS ...]
                        Path to the UDO configs (space separated, if multiple)

Quantizer Options:
    --quantization_overrides QUANTIZATION_OVERRIDES
                        Use this option to specify a json file with parameters to use for
                        quantization. These will override any quantization data carried from
                        conversion (eg TF fake quantization) or calculated during the normal
                        quantization process. Format defined as per AIMET specification.
    --keep_quant_nodes    Use this option to keep activation quantization nodes in the graph rather
                        than stripping them.

Custom Op Package Options:
    --op_package_lib OP_PACKAGE_LIB, -opl OP_PACKAGE_LIB
                        Use this argument to pass an op package library for quantization. Must be in
                        the form &lt;op_package_lib_path:interfaceProviderName&gt; and be separated by a
                        comma for multiple package libs
    --converter_op_package_lib CONVERTER_OP_PACKAGE_LIB, -cpl CONVERTER_OP_PACKAGE_LIB
                        Path to converter op package library compiled by the OpPackage generator.
    -p PACKAGE_NAME, --package_name PACKAGE_NAME
                        A global package name to be used for each node in the Model.cpp file.
                        Defaults to Qnn header defined package name
    --op_package_config CUSTOM_OP_CONFIG_PATHS [CUSTOM_OP_CONFIG_PATHS ...], -opc CUSTOM_OP_CONFIG_PATHS [CUSTOM_OP_CONFIG_PATHS ...]
                        Path to a Qnn Op Package XML configuration file that contains user defined
                        custom operations.

Note: Only one of: {&#39;package_name&#39;, &#39;op_package_config&#39;} can be specified
</pre></div>
</div>
<div class="line-block">
<div class="line">For more information, see <a class="reference external" href="model_conv_tensorflow.html">TensorFlow Model Conversion</a></div>
<div class="line">Additional details:</div>
</div>
<ul>
<li><p><em>input_network argument:</em></p>
<blockquote>
<div><ul>
<li><p>The converter supports a single frozen graph .pb file, a
path to a pair of graph meta and checkpoint files, or the
path to a SavedModel directory (TF 2.x).</p></li>
<li><p>If you are using the TensorFlow
Saver to save your graph during training, 3 files will be
generated as described below:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>&lt;model-name&gt;.meta</p></li>
<li><p>&lt;model-name&gt;</p></li>
<li><p>checkpoint</p></li>
</ol>
</div></blockquote>
</li>
<li><p>The converter –input_network option specifies the path
to the graph meta file. The converter will also use the
checkpoint file to read the graph nodes parameters during
conversion. The checkpoint file must have the same name
without the .meta suffix.</p></li>
<li><p>This argument is required.</p></li>
</ul>
</div></blockquote>
</li>
<li><p><em>input_dim argument:</em></p>
<blockquote>
<div><ul>
<li><p>Specifies the input dimensions of the graph’s input
node(s)</p></li>
<li><p>The converter requires a node name along with dimensions
as input from which it will create an input layer by
using the node output tensor dimensions. When defining a
graph, there is typically a placeholder name used as
input during training in the graph. The placeholder
tensor name is the name you must use as the argument. It
is also possible to use other types of nodes as input,
however the node used as input will not be used as part
of a layer other than the input layer.</p></li>
<li><p><strong>Multiple Inputs</strong></p>
<blockquote>
<div><ul class="simple">
<li><p>Networks with multiple inputs must provide
–<strong>input_dim</strong> INPUT_NAME INPUT_DIM, one for each
input node.</p></li>
<li><p>This argument is required.</p></li>
</ul>
</div></blockquote>
</li>
</ul>
</div></blockquote>
</li>
<li><p><em>out_node argument:</em></p>
<blockquote>
<div><ul>
<li><p>The name of the last node in your TensorFlow graph which
will represent the output layer of your network.</p></li>
<li><p><strong>Multiple Outputs</strong></p>
<blockquote>
<div><ul class="simple">
<li><p>Networks with multiple outputs must provide several
–<strong>out_node</strong> arguments, one for each output node.</p></li>
</ul>
</div></blockquote>
</li>
</ul>
</div></blockquote>
</li>
<li><p><em>output_path argument:</em></p>
<blockquote>
<div><ul class="simple">
<li><p>Specifies the output DLC file name.</p></li>
<li><p>This argument is optional. If not provided the converter
will create a DLC file with the same name as the graph
file name, with a .dlc file extension.</p></li>
</ul>
</div></blockquote>
</li>
<li><p>SavedModel is the default model format in TensorFlow 2 and
can been supported in Qualcomm® Neural Processing SDK TensorFlow Converter now.</p></li>
</ul>
<div class="line-block">
<div class="line"><br /></div>
</div>
</div>
<div class="section" id="snpe-tflite-to-dlc">
<h3>snpe-tflite-to-dlc<a class="headerlink" href="#snpe-tflite-to-dlc" title="Permalink to this heading">¶</a></h3>
<p>snpe-tflite-to-dlc converts a TFLite model into a DLC
file.</p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>usage: snpe-tflite-to-dlc -d INPUT_NAME INPUT_DIM [--out_node OUT_NAMES]
                          [--input_type INPUT_NAME INPUT_TYPE]
                          [--input_dtype INPUT_NAME INPUT_DTYPE] [--input_encoding  [ ...]]
                          [--input_layout INPUT_NAME INPUT_LAYOUT] [--custom_io CUSTOM_IO]
                          [--dump_relay DUMP_RELAY]
                          [--quantization_overrides QUANTIZATION_OVERRIDES] [--keep_quant_nodes]
                          [--disable_batchnorm_folding] [--keep_disconnected_nodes] --input_network
                          INPUT_NETWORK [-h] [--debug [DEBUG]] [-o OUTPUT_PATH]
                          [--copyright_file COPYRIGHT_FILE] [--float_bw FLOAT_BW]
                          [--model_version MODEL_VERSION]
                          [--validation_target RUNTIME_TARGET PROCESSOR_TARGET] [--strict]
                          [--udo_config_paths CUSTOM_OP_CONFIG_PATHS [CUSTOM_OP_CONFIG_PATHS ...]]
                          [--op_package_lib OP_PACKAGE_LIB]
                          [--converter_op_package_lib CONVERTER_OP_PACKAGE_LIB]
                          [-p PACKAGE_NAME | --op_package_config CUSTOM_OP_CONFIG_PATHS [CUSTOM_OP_CONFIG_PATHS ...]]

Script to convert TFLite model into DLC

required arguments:
    -d INPUT_NAME INPUT_DIM, --input_dim INPUT_NAME INPUT_DIM
                        The names and dimensions of the network input layers specified in the format
                        [input_name comma-separated-dimensions], for example:
                            &#39;data&#39; 1,224,224,3
                        Note that the quotes should always be included in order to handlespecial
                        characters, spaces, etc.
                        For multiple inputs specify multiple --input_dim on the command line like:
                            --input_dim &#39;data1&#39; 1,224,224,3 --input_dim &#39;data2&#39; 1,50,100,3
    --input_network INPUT_NETWORK, -i INPUT_NETWORK
                        Path to the source framework model.

optional arguments:
optional arguments:
    --out_node OUT_NAMES, --out_name OUT_NAMES
                        Name of the graph&#39;s output Tensor Names. Multiple output names should be
                        provided separately like:
                            --out_name out_1 --out_name out_2
    --input_type INPUT_NAME INPUT_TYPE, -t INPUT_NAME INPUT_TYPE
                        Type of data expected by each input op/layer. Type for each input is
                        |default| if not specified. For example: &quot;data&quot; image.Note that the quotes
                        should always be included in order to handle special characters, spaces,etc.
                        For multiple inputs specify multiple --input_type on the command line.
                        Eg:
                           --input_type &quot;data1&quot; image --input_type &quot;data2&quot; opaque
                        These options get used by DSP runtime and following descriptions state how
                        input will be handled for each option.
                        Image:
                        Input is float between 0-255 and the input&#39;s mean is 0.0f and the input&#39;s
                        max is 255.0f. We will cast the float to uint8ts and pass the uint8ts to the
                        DSP.
                        Default:
                        Pass the input as floats to the dsp directly and the DSP will quantize it.
                        Opaque:
                        Assumes input is float because the consumer layer(i.e next layer) requires
                        it as float, therefore it won&#39;t be quantized.
                        Choices supported:
                           image
                           default
                           opaque
    --input_dtype INPUT_NAME INPUT_DTYPE
                        The names and datatype of the network input layers specified in the format
                        [input_name datatype], for example:
                            &#39;data&#39; &#39;float32&#39;
                        Default is float32 if not specified
                        Note that the quotes should always be included in order to handlespecial
                        characters, spaces, etc.
                        For multiple inputs specify multiple --input_dtype on the command line like:
                            --input_dtype &#39;data1&#39; &#39;float32&#39; --input_dtype &#39;data2&#39; &#39;float32&#39;
    --input_encoding INPUT_ENCODING [INPUT_ENCODING ...], -e INPUT_ENCODING [INPUT_ENCODING ...]
                        Usage:     --input_encoding &quot;INPUT_NAME&quot; INPUT_ENCODING_IN
                        [INPUT_ENCODING_OUT]
                        Input encoding of the network inputs. Default is bgr.
                        e.g.
                           --input_encoding &quot;data&quot; rgba
                        Quotes must wrap the input node name to handle special characters,
                        spaces, etc. To specify encodings for multiple inputs, invoke
                        --input_encoding for each one.
                        e.g.
                            --input_encoding &quot;data1&quot; rgba --input_encoding &quot;data2&quot; other
                        Optionally, an output encoding may be specified for an input node by
                        providing a second encoding. The default output encoding is bgr.
                        e.g.
                            --input_encoding &quot;data3&quot; rgba rgb
                        Input encoding types:
                             image color encodings: bgr,rgb, nv21, nv12, ...
                            time_series: for inputs of rnn models;
                            other: not available above or is unknown.
                        Supported encodings:
                            bgr
                            rgb
                            rgba
                            argb32
                            nv21
                            nv12
                            time_series
                            other
    --input_layout INPUT_NAME INPUT_LAYOUT, -l INPUT_NAME INPUT_LAYOUT
                        Layout of each input tensor. If not specified, it will use the default
                        based on the Source Framework, shape of input and input encoding.
                        Accepted values are-
                            NCDHW, NDHWC, NCHW, NHWC, NFC, NCF, NTF, TNF, NF, NC, F, NONTRIVIAL
                        N = Batch, C = Channels, D = Depth, H = Height, W = Width, F = Feature, T = Time
                        NDHWC/NCDHW used for 5d inputs
                        NHWC/NCHW used for 4d image-like inputs
                        NFC/NCF used for inputs to Conv1D or other 1D ops
                        NTF/TNF used for inputs with time steps like the ones used for LSTM op
                        NF used for 2D inputs, like the inputs to Dense/FullyConnected layers
                        NC used for 2D inputs with 1 for batch and other for Channels (rarely used)
                        F used for 1D inputs, e.g. Bias tensor
                        NONTRIVIAL for everything elseFor multiple inputs specify multiple
                        --input_layout on the command line.
                        Eg:
                            --input_layout &quot;data1&quot; NCHW --input_layout &quot;data2&quot; NCHW
                        Note: This flag does not set the layout of the input tensor in the converted DLC.
    --custom_io CUSTOM_IO
                        Use this option to specify a yaml file for custom IO.
    --dump_relay DUMP_RELAY
                        Dump Relay ASM and Params at the path provided with the argument
                        Usage: --dump_relay &lt;path_to_dump&gt;
    --disable_batchnorm_folding
    --keep_disconnected_nodes
                        Disable Optimization that removes Ops not connected to the main graph.
                        This optimization uses output names provided over commandline OR
                        inputs/outputs extracted from the Source model to determine the main graph
    -h, --help          show this help message and exit
    --debug [DEBUG]     Run the converter in debug mode.
    -o OUTPUT_PATH, --output_path OUTPUT_PATH
                        Path where the converted Output model should be saved.If not specified, the
                        converter model will be written to a file with same name as the input model
    --copyright_file COPYRIGHT_FILE
                        Path to copyright file. If provided, the content of the file will be added
                        to the output model.
    --model_version MODEL_VERSION
                        User-defined ASCII string to identify the model, only first 64 bytes will be
                        stored
    --validation_target RUNTIME_TARGET PROCESSOR_TARGET
                        A combination of processor and runtime target against which model will be
                        validated.
                        Choices for RUNTIME_TARGET:
                           {cpu, gpu, dsp}.
                        Choices for PROCESSOR_TARGET:
                           {snapdragon_801, snapdragon_820, snapdragon_835}.
                        If not specified, will validate model against {snapdragon_820,
                        snapdragon_835} across all runtime targets.
    --strict            If specified, will validate in strict mode whereby model will not
                        be produced if it violates constraints of the specified validation target. If
                        not specified, will validate model in permissive mode against the specified
                        validation target.
    --udo_config_paths CUSTOM_OP_CONFIG_PATHS [CUSTOM_OP_CONFIG_PATHS ...], -udo CUSTOM_OP_CONFIG_PATHS
                        [CUSTOM_OP_CONFIG_PATHS ...]
                        Path to the UDO configs (space separated, if multiple)

Quantizer Options:
    --quantization_overrides QUANTIZATION_OVERRIDES
                        Use this option to specify a json file with parameters to use for
                        quantization. These will override any quantization data carried from
                        conversion (eg TF fake quantization) or calculated during the normal
                        quantization process. Format defined as per AIMET specification.
    --keep_quant_nodes  Use this option to keep activation quantization nodes in the graph rather
                        than stripping them.

Custom Op Package Options:
    --op_package_lib OP_PACKAGE_LIB, -opl OP_PACKAGE_LIB
                        Use this argument to pass an op package library for quantization. Must be in
                        the form &lt;op_package_lib_path:interfaceProviderName&gt; and be separated by a
                        comma for multiple package libs
    --converter_op_package_lib CONVERTER_OP_PACKAGE_LIB, -cpl CONVERTER_OP_PACKAGE_LIB
                        Path to converter op package library compiled by the OpPackage generator.
    -p PACKAGE_NAME, --package_name PACKAGE_NAME
                        A global package name to be used for each node in the Model.cpp file.
                        Defaults to Qnn header defined package name
    --op_package_config CUSTOM_OP_CONFIG_PATHS [CUSTOM_OP_CONFIG_PATHS ...], -opc CUSTOM_OP_CONFIG_PATHS [CUSTOM_OP_CONFIG_PATHS ...]
                        Path to a Qnn Op Package XML configuration file that contains user defined
                        custom operations.

Note: Only one of: {&#39;package_name&#39;, &#39;op_package_config&#39;} can be specified
</pre></div>
</div>
<div class="line-block">
<div class="line">For more information, see <a class="reference external" href="model_conv_tflite.html">TFLite Model Conversion</a></div>
<div class="line">Additional details:</div>
</div>
<ul>
<li><p><em>input_network argument:</em></p>
<blockquote>
<div><ul class="simple">
<li><p>The converter supports a single .tflite file.</p></li>
<li><p>The converter –input_network option specifies the path
to the .tflite file.</p></li>
<li><p>This argument is required.</p></li>
</ul>
</div></blockquote>
</li>
<li><p><em>input_dim argument:</em></p>
<blockquote>
<div><ul>
<li><p>Specifies the input dimensions of the graph’s input
node(s)</p></li>
<li><p>The converter requires a node name along with dimensions
as input from which it will create an input layer by
using the node output tensor dimensions. When defining a
graph, there is typically a placeholder name used as
input during training in the graph. The placeholder
tensor name is the name you must use as the argument. It
is also possible to use other types of nodes as input,
however the node used as input will not be used as part
of a layer other than the input layer.</p></li>
<li><p><strong>Multiple Inputs</strong></p>
<blockquote>
<div><ul class="simple">
<li><p>Networks with multiple inputs must provide
–<strong>input_dim</strong> INPUT_NAME INPUT_DIM, one for each
input node.</p></li>
</ul>
</div></blockquote>
</li>
<li><p>This argument is required.</p></li>
</ul>
</div></blockquote>
</li>
<li><p><em>output_path argument:</em></p>
<blockquote>
<div><ul class="simple">
<li><p>Specifies the output DLC file name.</p></li>
<li><p>This argument is optional. If not provided the converter
will create a DLC file with the same name as the tflite
file name, with a .dlc file extension.</p></li>
</ul>
</div></blockquote>
</li>
<li><p>saved_model_tag:</p>
<blockquote>
<div><ul class="simple">
<li><p>For Tensorflow 2.x networks, this option allows a
MetaGraph to be selected from the SavedModel specified by
input_network.</p></li>
<li><p>This argument is optional and will default to “serve” if left unset.</p></li>
</ul>
</div></blockquote>
</li>
<li><p>saved_model_signature:</p>
<blockquote>
<div><ul class="simple">
<li><p>For Tensorflow 2.x networks, this option specifies the
signature key for selecting inputs and outputs of a
Tensorflow 2.x SavedModel.</p></li>
<li><p>This argument is optional and will default to “serving_default” if unspecified.</p></li>
</ul>
</div></blockquote>
</li>
</ul>
<div class="line-block">
<div class="line"><br /></div>
</div>
</div>
<div class="section" id="qairt-converter">
<h3>qairt-converter<a class="headerlink" href="#qairt-converter" title="Permalink to this heading">¶</a></h3>
<p>The <strong>qairt-converter</strong> tool converts a model from the one of Onnx/TensorFlow/TFLite/PyTorch framework to
a DLC file representing the QNN graph format that can enable inference on Qualcomm AI IP/HW. The converter auto detects
the framework based on the source model extension.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Basic command line usage looks like:

usage: qairt-converter [--desired_input_shape INPUT_NAME INPUT_DIM] [--out_tensor_node OUT_NAMES]
                       [--source_model_input_datatype INPUT_NAME INPUT_DTYPE]
                       [--source_model_input_layout INPUT_NAME INPUT_LAYOUT]
                       [--desired_input_color_encoding  [ ...]]
                       [--dump_io_config_template DUMP_IO_CONFIG_TEMPLATE] [--io_config IO_CONFIG]
                       [--dry_run [DRY_RUN]] [--quantization_overrides QUANTIZATION_OVERRIDES]
                       [--onnx_no_simplification] [--onnx_batch BATCH]
                       [--onnx_define_symbol SYMBOL_NAME VALUE] [--tf_no_optimization]
                       [--tf_show_unconsumed_nodes] [--tf_saved_model_tag SAVED_MODEL_TAG]
                       [--tf_saved_model_signature_key SAVED_MODEL_SIGNATURE_KEY]
                       [--tf_validate_models] [--tflite_signature_name SIGNATURE_NAME]
                       --input_network INPUT_NETWORK [-h] [--debug [DEBUG]]
                       [--output_path OUTPUT_PATH] [--copyright_file COPYRIGHT_FILE]
                       [--float_bitwidth FLOAT_BITWIDTH] [--float_bias_bitwidth FLOAT_BIAS_BITWIDTH]
                       [--model_version MODEL_VERSION] [--converter_op_package_lib CONVERTER_OP_PACKAGE_LIB]
                       [--package_name PACKAGE_NAME | --op_package_config CUSTOM_OP_CONFIG_PATHS [CUSTOM_OP_CONFIG_PATHS ...]]

required arguments:
  --input_network INPUT_NETWORK, -i INPUT_NETWORK
                        Path to the source framework model.

optional arguments:
  --desired_input_shape INPUT_NAME INPUT_DIM, -d INPUT_NAME INPUT_DIM
                        The name and dimension of all the input buffers to the network specified in
                        the format [input_name comma-separated-dimensions],
                        for example: &#39;data&#39; 1,224,224,3.
                        Note that the quotes should always be included in order to handle special
                        characters, spaces, etc.
                        NOTE: Required for TensorFlow and PyTorch. Optional for Onnx and Tflite
                        In case of Onnx, this feature works only with Onnx 1.6.0 and above
  --out_tensor_node OUT_NAMES, --out_tensor_name OUT_NAMES
                        Name of the graph&#39;s output Tensor Names. Multiple output names should be
                        provided separately like:
                            --out_name out_1 --out_name out_2
                        NOTE: Required for TensorFlow. Optional for Onnx, Tflite and PyTorch
  --source_model_input_datatype INPUT_NAME INPUT_DTYPE
                        The names and datatype of the network input layers specified in the format
                        [input_name datatype], for example:
                            &#39;data&#39; &#39;float32&#39;
                        Default is float32 if not specified
                        Note that the quotes should always be included in order to handlespecial
                        characters, spaces, etc.
                        For multiple inputs specify multiple --input_dtype on the command line like:
                            --input_dtype &#39;data1&#39; &#39;float32&#39; --input_dtype &#39;data2&#39; &#39;float32&#39;
  --source_model_input_layout INPUT_NAME INPUT_LAYOUT, -l INPUT_NAME INPUT_LAYOUT
                        Layout of each input tensor. If not specified, it will use the default
                        based on the Source Framework, shape of input and input encoding.
                        Accepted values are-
                            NCDHW, NDHWC, NCHW, NHWC, NFC, NCF, NTF, TNF, NF, NC, F, NONTRIVIAL
                        N = Batch, C = Channels, D = Depth, H = Height, W = Width, F = Feature, T =
                        Time
                        NDHWC/NCDHW used for 5d inputs
                        NHWC/NCHW used for 4d image-like inputs
                        NFC/NCF used for inputs to Conv1D or other 1D ops
                        NTF/TNF used for inputs with time steps like the ones used for LSTM op
                        NF used for 2D inputs, like the inputs to Dense/FullyConnected layers
                        NC used for 2D inputs with 1 for batch and other for Channels (rarely used)
                        F used for 1D inputs, e.g. Bias tensor
                        NONTRIVIAL for everything elseFor multiple inputs specify multiple
                        --input_layout on the command line.
                        Eg:
                            --input_layout &quot;data1&quot; NCHW --input_layout &quot;data2&quot; NCHW
  --desired_input_color_encoding  [ ...], -e  [ ...]
                        Usage:     --input_color_encoding &quot;INPUT_NAME&quot; INPUT_ENCODING_IN
                        [INPUT_ENCODING_OUT]
                        Input encoding of the network inputs. Default is bgr.
                        e.g.
                           --input_color_encoding &quot;data&quot; rgba
                        Quotes must wrap the input node name to handle special characters,
                        spaces, etc. To specify encodings for multiple inputs, invoke
                        --input_color_encoding for each one.
                        e.g.
                            --input_color_encoding &quot;data1&quot; rgba --input_color_encoding &quot;data2&quot; other
                        Optionally, an output encoding may be specified for an input node by
                        providing a second encoding. The default output encoding is bgr.
                        e.g.
                            --input_color_encoding &quot;data3&quot; rgba rgb
                        Input encoding types:
                             image color encodings: bgr,rgb, nv21, nv12, ...
                            time_series: for inputs of rnn models;
                            other: not available above or is unknown.
                        Supported encodings:
                           bgr
                           rgb
                           rgba
                           argb32
                           nv21
                           nv12
  --dump_io_config_template DUMP_IO_CONFIG_TEMPLATE
                        Dumps the yaml template for I/O configuration. This file can be edited as
                        per the custom requirements and passed using the option --io_configUse this
                        option to specify a yaml file to which the IO config template is dumped.
  --io_config IO_CONFIG
                        Use this option to specify a yaml file for input and output options.
  --dry_run [DRY_RUN]   Evaluates the model without actually converting any ops, and returns
                        unsupported ops/attributes as well as unused inputs and/or outputs if any.
  -h, --help            show this help message and exit
  --debug [DEBUG]       Run the converter in debug mode.
  --output_path OUTPUT_PATH, -o OUTPUT_PATH
                        Path where the converted Output model should be saved.If not specified, the
                        converter model will be written to a file with same name as the input model
  --copyright_file COPYRIGHT_FILE
                        Path to copyright file. If provided, the content of the file will be added
                        to the output model.
  --float_bitwidth FLOAT_BITWIDTH
                        Use the --float_bitwidth option to convert the graph to the specified float
                        bitwidth, either 32 (default) or 16.
  --float_bias_bitwidth FLOAT_BIAS_BITWIDTH
                        Use the --float_bias_bitwidth option to select the bitwidth to use for float
                        bias tensor
  --model_version MODEL_VERSION
                        User-defined ASCII string to identify the model, only first 64 bytes will be
                        stored

Custom Op Package Options:
  --converter_op_package_lib CONVERTER_OP_PACKAGE_LIB, -cpl CONVERTER_OP_PACKAGE_LIB
                        Absolute path to converter op package library compiled by the OpPackage
                        generator. Must be separated by a comma for multiple package libraries.
                        Note: Order of converter op package libraries must follow the order of xmls.
                        Ex1: --converter_op_package_lib absolute_path_to/libExample.so
                        Ex2: -cpl absolute_path_to/libExample1.so,absolute_path_to/libExample2.so
  --package_name PACKAGE_NAME, -p PACKAGE_NAME
                        A global package name to be used for each node in the Model.cpp file.
                        Defaults to Qnn header defined package name
  --op_package_config CUSTOM_OP_CONFIG_PATHS [CUSTOM_OP_CONFIG_PATHS ...], -opc CUSTOM_OP_CONFIG_PATHS [CUSTOM_OP_CONFIG_PATHS ...]
                        Path to a Qnn Op Package XML configuration file that contains user defined
                        custom operations.

Quantizer Options:
  --quantization_overrides QUANTIZATION_OVERRIDES
                        Use this option to specify a json file with parameters to use for
                        quantization. These will override any quantization data carried from
                        conversion (eg TF fake quantization) or calculated during the normal
                        quantization process. Format defined as per AIMET specification.

Onnx Converter Options:
  --onnx_no_simplification
                        Do not attempt to simplify the model automatically. This may prevent some
                        models from properly converting
                        when sequences of unsupported static operations are present.
  --onnx_batch BATCH    The batch dimension override. This will take the first dimension of all
                        inputs and treat it as a batch dim, overriding it with the value provided
                        here. For example:
                        --batch 6
                        will result in a shape change from [1,3,224,224] to [6,3,224,224].
                        If there are inputs without batch dim this should not be used and each input
                        should be overridden independently using -d option for input dimension
                        overrides.
  --onnx_define_symbol SYMBOL_NAME VALUE
                        This option allows overriding specific input dimension symbols. For instance
                        you might see input shapes specified with variables such as :
                        data: [1,3,height,width]
                        To override these simply pass the option as:
                        --define_symbol height 224 --define_symbol width 448
                        which results in dimensions that look like:
                        data: [1,3,224,448]

TensorFlow Converter Options:
  --tf_no_optimization  Do not attempt to optimize the model automatically.
  --tf_show_unconsumed_nodes
                        Displays a list of unconsumed nodes, if there any are found. Nodeswhich are
                        unconsumed do not violate the structural fidelity of thegenerated graph.
  --tf_saved_model_tag SAVED_MODEL_TAG
                        Specify the tag to seletet a MetaGraph from savedmodel. ex:
                        --saved_model_tag serve. Default value will be &#39;serve&#39; when it is not
                        assigned.
  --tf_saved_model_signature_key SAVED_MODEL_SIGNATURE_KEY
                        Specify signature key to select input and output of the model. ex:
                        --saved_model_signature_key serving_default. Default value will be
                        &#39;serving_default&#39; when it is not assigned
  --tf_validate_models  Validate the original TF model against optimized TF model.
                        Constant inputs with all value 1s will be generated and will be used
                        by both models and their outputs are checked against each other.
                        The % average error and 90th percentile of output differences will be
                        calculated for this.
                        Note: Usage of this flag will incur extra time due to inference of the
                        models.

Tflite Converter Options:
  --tflite_signature_name SIGNATURE_NAME
                        Use this option to specify a specific Subgraph signature to convert
</pre></div>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
</div>
</div>
<div class="section" id="model-preparation">
<h2>Model Preparation<a class="headerlink" href="#model-preparation" title="Permalink to this heading">¶</a></h2>
<div class="section" id="snpe-dlc-graph-prepare">
<h3>snpe-dlc-graph-prepare<a class="headerlink" href="#snpe-dlc-graph-prepare" title="Permalink to this heading">¶</a></h3>
<p>snpe-dlc-graph-prepare is used to perform offline graph preparation on quantized dlcs to run on DSP/HTP runtimes.</p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>Command Line Options:
  [ -h, --help ]        Displays this help message.
  [ --version ]         Displays version information.
  [ --verbose ]         Enable verbose user messages.
  [ --quiet ]           Disables some user messages.
  [ --silent ]          Disables all but fatal user messages.
  [ --debug=&lt;val&gt; ]     Sets the debug log level.
  [ --debug1 ]          Enables level 1 debug messages.
  [ --debug2 ]          Enables level 2 debug messages.
  [ --debug3 ]          Enables level 3 debug messages.
  [ --log-mask=&lt;val&gt; ]  Sets the debug log mask to set the log level for one or more areas.
                        Example: &quot;.*=USER_ERROR, .*=INFO, NDK=DEBUG2, NCC=DEBUG3&quot;
  [ --log-file=&lt;val&gt; ]  Overrides the default name for the debug log file.
  [ --log-dir=&lt;val&gt; ]   Overrides the default directory path where debug log files are written.
  [ --log-file-include-hostname ]
                        Appends the name of this host to the log file name.
  --input_dlc=&lt;val&gt;     Path to the dlc container containing the model for which graph cache
                        should be generated. This argument is required.
  [ --output_dlc=&lt;val&gt; ]
                        Path at which the cached data included model container should be written.
                        If this argument is omitted, the quantized model will be written at
                        &lt;input_model_name&gt;_cached.dlc.
  [ --set_output_tensors=&lt;val&gt; ]
                        Specifies a comma separated list of tensors to be output after execution
                        without whitespace.
  [ --set_output_layers=&lt;val&gt; ]
                        Specifies a comma separated list of layers whose output buffers should be
                        output after execution, without whitespace.
  [ --input_list=&lt;val&gt; ]
                        Path to a file specifying input images as passed to snpe-net-run. Only
                        the graph output buffers information specified in the input list (line starting
                        with # or %, if any) will be used. Paths to the input images will be ignored
  [ --htp_socs=&lt;val&gt; ]  Specify SoC(s) to generate HTP Offline Cache for. SoCs are specified with an
                        ASIC identifier, in a comma seperated list without whitespace.
                        For example --htp_socs sm8350,sm8450,sm8550,sm8650,qcs6490,qcs8550.
                        This flag and --htp_archs are mutually exclusive.
                        Default ASIC identifier: sm8650
  [ --htp_archs=&lt;val&gt; ]
                        Specify DSP Architecture(s) to generate general HTP Offline Cache for.
                        Architectures are specified with an ASIC identifier, in a comma seperated list
                        without whitespace. For example, --htp_archs v68,v73. This flag cannot be
                        coupled with --htp_socs or --vtcm_override
  [ --vtcm_override=&lt;val&gt; ]
                        Specify a single value representing the VTCM size in MB for the generated HTP Offline Caches.
                        For example, --vtcm_override 4. When set to 0, the SoC maximum VTCM size is used and if cache
                        compatibility mode is set to STRICT the maximum value is checked. This flag can be used with
                        --htp_socs to override the default SOC VTCM size setting
  [ --optimization_level=&lt;val&gt; ]
                        Specify an optimization level. Valid values are 1, 2 and 3. Default is 2. Higher optimization levels incur
                        longer offline prepare time but yields more optimal graph and hence faster execution time for most graphs
  [ --buffer_data_type=&lt;val&gt; ]
                        Sets data type of IO buffers during prepare. Data Type can be the following:
                        float32, fixedPoint8, fixedPoint16. Arguments should be formatted as follows:
                        --buffer_data_type buffer_name1=buffer_name1_data_type
                        --buffer_data_type buffer_name2=buffer_name2_data_type
                        (Note: deprecated)
  [ --overwrite_cache_records ]
                        Allow this tool to overwrite over any cache record that exactly matches the requested SoC(s).
                        Default behavior is to skip (re)generating cache records when a matching cache already exists
  [ --use_float_io ]    Prepare quantized HTP Graph to operate with floating point inputs/outputs (Note: deprecated)
  [ --htp_dlbc=&lt;val&gt; ]  Specify Deep Learning Bandwidth Compression (DLBC) for this HTP graph. The default setting is OFF.
                        To turn on, specify it as --htp_dlbc=true
  [ --num_hvx_threads=&lt;val&gt; ]
                        Specify the number of HVX threads to reserve for this HTP graph. Must be greater than 0.
  [ --input_name=&lt;val&gt; ]
                        Specifies the name of input for which dimensions are specified.
  [ --input_dimensions=&lt;val&gt; ]
                        Specifies new dimensions for input whose name is specified in input_name. e.g. &quot;1,224,224,3&quot;.
                        For multiple inputs, specify --input_name and --input_dimensions multiple times.
  [ --memorymapped_buffer_hint=&lt;val&gt; ]
                        Specifies memory-mapped buffers hint. The default setting is OFF.
                        To turn on, specify it as --memorymapped_buffer_hint=true
  [ --udo_package_path=&lt;val&gt; ]
                        Use this option to specify path to the Registration Library for UDO Package(s). Usage is:
                        --udo_package_path=&lt;path_to_reg_lib&gt;
                        Optionally, user can provide multiple packages as a comma-separated list.
                        This option must be specified for Networks with UDO. All UDO&#39;s in Network must have host executable CPU Implementation
</pre></div>
</div>
<p>For detailed information on how to use the tool, please refer to <a class="reference external" href="offline_graph_caching.html">Offline Graph Caching for DSP Runtime on HTP</a></p>
</div>
<div class="section" id="snpe-dlc-quant">
<h3>snpe-dlc-quant<a class="headerlink" href="#snpe-dlc-quant" title="Permalink to this heading">¶</a></h3>
<p>snpe-dlc-quant converts non-quantized DLC models into quantized
DLC models.</p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>Command Line Options:
    [ -h,--help ]         Displays this help message.
    [ --version ]         Displays version information.
    [ --verbose ]         Enable verbose user messages.
    [ --quiet ]           Disables some user messages.
    [ --silent ]          Disables all but fatal user messages.
    [ --debug=&lt;val&gt; ]     Sets the debug log level.
    [ --debug1 ]          Enables level 1 debug messages.
    [ --debug2 ]          Enables level 2 debug messages.
    [ --debug3 ]          Enables level 3 debug messages.
    [ --log-mask=&lt;val&gt; ]  Sets the debug log mask to set the log level for one or more areas.
                        Example: &quot;.*=USER_ERROR, .*=INFO, NDK=DEBUG2, NCC=DEBUG3&quot;
    [ --log-file=&lt;val&gt; ]  Overrides the default name for the debug log file.
    [ --log-dir=&lt;val&gt; ]   Overrides the default directory path where debug log files are written.
    [ --log-file-include-hostname ]
                        Appends the name of this host to the log file name.
    [ --input_dlc=&lt;val&gt; ]
                        Path to the dlc container containing the model for which fixed-point encoding
                        metadata should be generated. This argument is required.
    [ --input_list=&lt;val&gt; ]
                        Path to a file specifying the trial inputs. This file should be a plain text file,
                        containing one or more absolute file paths per line. These files will be taken to constitute
                        the trial set. Each path is expected to point to a binary file containing one trial input
                        in the &#39;raw&#39; format, ready to be consumed by the tool without any further modifications.
                        This is similar to how input is provided to snpe-net-run application.
    [ --no_weight_quantization ]
                        Generate and add the fixed-point encoding metadata but keep the weights in
                        floating point. This argument is optional.
    [ --output_dlc=&lt;val&gt; ]
                        Path at which the metadata-included quantized model container should be written.
                        If this argument is omitted, the quantized model will be written at &lt;unquantized_model_name&gt;_quantized.dlc.
    [ --use_enhanced_quantizer ]
                        Use the enhanced quantizer feature when quantizing the model.  Regular quantization determines the range using the actual
                        values of min and max of the data being quantized.  Enhanced quantization uses an algorithm to determine optimal range.  It can be
                        useful for quantizing models that have long tails in the distribution of the data being quantized.
    [ --use_adjusted_weights_quantizer ]
                        Use the adjusted tf quantizer for quantizing the weights only. This might be helpful for improving the accuracy of some models,
                        such as denoise model as being tested. This option is only used when quantizing the weights with 8 bit.
    [ --optimizations ]   Use this option to enable new optimization algorithms. Usage is:
                        --optimizations &lt;algo_name1&gt; --optimizations &lt;algo_name2&gt;
                        The available optimization algorithms are:
                        cle - Cross layer equalization includes a number of methods for equalizing weights and biases across layers in order to rectify imbalances that cause quantization errors.
    [ --override_params ]
                        Use this option to override quantization parameters when quantization was provided from the original source framework (eg TF fake quantization)
    [ --use_encoding_optimizations ]
                        Use this option to enable quantization encoding optimizations. This can reduce requantization in the graph and may improve accuracy for some models
                        (Note: deprecated).
    [ --use_symmetric_quantize_weights ]
                        Use the symmetric quantizer feature when quantizing the weights of the model. It makes sure min and max have the
                        same absolute values about zero. Symmetrically quantized data will also be stored as int#_t data such that the offset is always 0.
    [ --use_native_dtype ]
                        Note: This option is deprecated, use --use_native_input_files option in future.
                          Use this option to indicate how to read input files,
                           1. float (default): reads inputs as floats and quantizes if necessary based on quantization parameters in the model.
                           2. native:          reads inputs assuming the data type to be native to the model. For ex., uint8_t.
    [ --use_native_input_files ]
                        Use this option to indicate how to read input files,
                           1. float (default): reads inputs as floats and quantizes if necessary based on quantization parameters in the model.
                           2. native:          reads inputs assuming the data type to be native to the model. For ex., uint8_t.
    [ --use_native_output_files ]
                        Use this option to indicate the data type of the output files,
                           1. float (default): generates the output file as float data.
                           2. native:          generates the output file as datatype native to the source model. i.e. uint8_t.
    [ --bias_bitwidth=&lt;val&gt; ]
                        Use the --bias_bitdwith option to select the bitwidth to use when quantizing the biases, either 8 (default) or 32. Using 32 bit biases may
                        sometimes provide a small improvement in accuracy. Can&#39;t mix with --bitwidth.
    [ --float_bitwidth=&lt;val&gt; ]
                        Use the --float_bitwidth option to select the bitwidth to use when using float
                        for parameters(weights/bias) and activations for all ops or specific
                        Op (via encodings) selected through encoding, either 32 (default) or 16.
    [ --float_bias_bitwidth=&lt;val&gt; ]
                            Use this option to specify the bitwidth for float bias tensors, either 32 or 16.
                            If not provided and bias is overridden to float in Quantizer, the overriding float tensor&#39;s bw will be used
    [ --act_bitwidth=&lt;val&gt; ]
                        Use the --act_bitwidth option to select the bitwidth to use when quantizing the activations, either 8 (default) or 16. 8w/16a is only supported
                        by the HTA currently. Can&#39;t mix with --bitwidth.
    [ --weights_bitwidth=&lt;val&gt; ]
                        Use the --weights_bitwidth option to select the bitwidth to use when quantizing the weights, either 8 (default) or 16. 8w/16a is only supported
                        by the HTA currently. Can&#39;t mix with --bitwidth.
    [ --bitwidth=&lt;val&gt; ]
                        Use the --bitwidth option to select the bitwidth to use when quantizing the weights/activation/bias, either 8 (default) or 16. Can&#39;t mix with
                        --weights_bitwidth or --act_bitwidth or --bias_bitdwith.
    [ --udo_package_path=&lt;val&gt; ]
                        Use this option to specify path to the Registration Library for a UDO Package. Usage is:
                        --udo_package_path=&lt;path_to_reg_lib&gt;
                        This option must be specified for Networks with UDO. All UDO&#39;s in Network must have host-executable CPU Implementation
    [ --axis_quant ]
                        Use the --axis_quant option to select per-axis-element quantization for the weights and biases of certain layer types.
                        Currently only Convolution, Deconvolution and FullyConnected are supported.


Description:
Generate 8 or 16 bit TensorFlow style fixed point weight and activations encodings for a floating point DLC.
</pre></div>
</div>
<div class="line-block">
<div class="line"><br /></div>
<div class="line">Additional details:</div>
</div>
<ul>
<li><p>For specifying input_list, refer to input_list
argument in
<a class="reference external" href="tools.html#snpe-net-run">snpe-net-run</a> for
supported input formats (in order to calculate output
activation encoding information for all layers, <strong>do not</strong>
include the line which specifies desired outputs).</p></li>
<li><p>The tool requires the batch dimension of the DLC input file
to be set to 1 during the original model conversion step.</p>
<blockquote>
<div><ul>
<li><p>An example of quantization using snpe-dlc-quant can be found
in the C/C++ Tutorial section: <a class="reference external" href="tutorial_inceptionv3.html">Running the Inception v3
Model</a>. For details on
quantization see <a class="reference external" href="quantized_models.html">Quantized vs Non-Quantized
Models</a>.</p></li>
<li><p>Outputs can be specified for snpe-dlc-quant by modifying the
input_list in the following ways:</p>
<blockquote>
<div><div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>#&lt;output_layer_name&gt;[&lt;space&gt;&lt;output_layer_name&gt;]
%&lt;output_tensor_name&gt;[&lt;space&gt;&lt;output_tensor_name&gt;]
&lt;input_layer_name&gt;:=&lt;input_layer_path&gt;[&lt;space&gt;&lt;input_layer_name&gt;:=&lt;input_layer_path&gt;]
…
</pre></div>
</div>
<p><strong>Note:</strong> Output tensors and layers can be specified
individually, but when specifying both, the order shown must
be used to specify each.</p>
</div></blockquote>
</li>
<li><p>When using the Qualcomm® Neural Processing SDK API:</p>
<blockquote>
<div><ul class="simple">
<li><p>Any output layers specified when snpe-dlc-quant was
called, need to be specified using the
<span class="xref std std-ref">Snpe_SNPEBuilder_SetOutputLayers()</span>
function.</p></li>
<li><p>Any output tensors specified when snpe-dlc-quant was
called, need to be specified using the
<span class="xref std std-ref">Snpe_SNPEBuilder_SetOutputTensors()</span>
function.</p></li>
</ul>
</div></blockquote>
</li>
</ul>
</div></blockquote>
</li>
</ul>
<div class="line-block">
<div class="line"><br /></div>
</div>
</div>
<div class="section" id="snpe-dlc-quantize">
<h3>snpe-dlc-quantize<a class="headerlink" href="#snpe-dlc-quantize" title="Permalink to this heading">¶</a></h3>
<p>snpe-dlc-quantize converts non-quantized DLC models into
quantized DLC models.</p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>Command Line Options:
    [ -h,--help ]         Displays this help message.
    [ --version ]         Displays version information.
    [ --verbose ]         Enable verbose user messages.
    [ --quiet ]           Disables some user messages.
    [ --silent ]          Disables all but fatal user messages.
    [ --debug=&lt;val&gt; ]     Sets the debug log level.
    [ --debug1 ]          Enables level 1 debug messages.
    [ --debug2 ]          Enables level 2 debug messages.
    [ --debug3 ]          Enables level 3 debug messages.
    [ --log-mask=&lt;val&gt; ]  Sets the debug log mask to set the log level for one or more areas.
                        Example: &quot;.*=USER_ERROR, .*=INFO, NDK=DEBUG2, NCC=DEBUG3&quot;
    [ --log-file=&lt;val&gt; ]  Overrides the default name for the debug log file.
    [ --log-dir=&lt;val&gt; ]   Overrides the default directory path where debug log files are written.
    [ --log-file-include-hostname ]
                        Appends the name of this host to the log file name.
    [ --input_dlc=&lt;val&gt; ]
                        Path to the dlc container containing the model for which fixed-point encoding
                        metadata should be generated. This argument is required.
    [ --input_list=&lt;val&gt; ]
                        Path to a file specifying the trial inputs. This file should be a plain text file,
                        containing one or more absolute file paths per line. These files will be taken to constitute
                        the trial set. Each path is expected to point to a binary file containing one trial input
                        in the &#39;raw&#39; format, ready to be consumed by the tool without any further modifications.
                        This is similar to how input is provided to snpe-net-run application.
    [ --no_weight_quantization ]
                        Generate and add the fixed-point encoding metadata but keep the weights in
                        floating point. This argument is optional.
    [ --output_dlc=&lt;val&gt; ]
                        Path at which the metadata-included quantized model container should be written.
                        If this argument is omitted, the quantized model will be written at &lt;unquantized_model_name&gt;_quantized.dlc.
    [ --enable_htp ]      Pack HTP information in quantized DLC.
    [ --htp_socs=&lt;val&gt; ]  Specify SoC to generate HTP Offline Cache for.
                        SoCs are specified with an ASIC identifier, in a comma separated list.
                        For example, --htp_socs sm8650
    [ --overwrite_cache_records ]
                        Overwrite HTP cache records present in the DLC.
    [ --use_float_io ]
                        Pack HTP information in quantized DLC (Note: deprecated).
    [ --use_enhanced_quantizer ]
                        Use the enhanced quantizer feature when quantizing the model.  Regular quantization determines the range using the actual
                        values of min and max of the data being quantized.  Enhanced quantization uses an algorithm to determine optimal range.  It can be
                        useful for quantizing models that have long tails in the distribution of the data being quantized.
    [ --use_adjusted_weights_quantizer ]
                        Use the adjusted tf quantizer for quantizing the weights only. This might be helpful for improving the accuracy of some models,
                        such as denoise model as being tested. This option is only used when quantizing the weights with 8 bit.
    [ --optimizations ]   Use this option to enable new optimization algorithms. Usage is:
                        --optimizations &lt;algo_name1&gt; --optimizations &lt;algo_name2&gt;
                        The available optimization algorithms are:
                        cle - Cross layer equalization includes a number of methods for equalizing weights and biases across layers in order to rectify imbalances that cause quantization errors.
    [ --override_params ]
                        Use this option to override quantization parameters when quantization was provided from the original source framework (eg TF fake quantization)
    [ --use_encoding_optimizations ]
                        Use this option to enable quantization encoding optimizations. This can reduce requantization in the graph and may improve accuracy for some models
                        (Note: this flag can be passed in, but is a no-op. Recognition of this flag will be removed in the future).
    [ --use_symmetric_quantize_weights ]
                        Use the symmetric quantizer feature when quantizing the weights of the model. It makes sure min and max have the
                        same absolute values about zero. Symmetrically quantized data will also be stored as int#_t data such that the offset is always 0.
    [ --use_native_dtype ]
                        Note: This option is deprecated, use --use_native_input_files option in future.
                          Use this option to indicate how to read input files,
                           1. float (default): reads inputs as floats and quantizes if necessary based on quantization parameters in the model.
                           2. native:          reads inputs assuming the data type to be native to the model. For ex., uint8_t.
    [ --use_native_input_files ]
                        Use this option to indicate how to read input files,
                           1. float (default): reads inputs as floats and quantizes if necessary based on quantization parameters in the model.
                           2. native:          reads inputs assuming the data type to be native to the model. For ex., uint8_t.
    [ --use_native_output_files ]
                        Use this option to indicate the data type of the output files,
                           1. float (default): generates the output file as float data.
                           2. native:          generates the output file as datatype native to the source model. i.e. uint8_t.
    [ --bias_bitwidth=&lt;val&gt; ]
                        Use the --bias_bitdwith option to select the bitwidth to use when quantizing the biases, either 8 (default) or 32. Using 32 bit biases may
                        sometimes provide a small improvement in accuracy. Can&#39;t mix with --bitwidth.
    [ --float_bitwidth=&lt;val&gt; ]
                        Use the --float_bitwidth option to select the bitwidth to use when using float
                        for parameters(weights/bias) and activations for all ops or specific
                        Op (via encodings) selected through encoding, either 32 (default) or 16.
    [ --float_bias_bitwidth=&lt;val&gt; ]
                            Use this option to specify the bitwidth for float bias tensors, either 32 or 16.
                            If not provided and bias is overridden to float in Quantizer, the overriding float tensor&#39;s bw will be used
    [ --act_bitwidth=&lt;val&gt; ]
                        Use the --act_bitwidth option to select the bitwidth to use when quantizing the activations, either 8 (default) or 16. 8w/16a is only supported
                        by the HTA currently. Can&#39;t mix with --bitwidth.
    [ --weights_bitwidth=&lt;val&gt; ]
                        Use the --weights_bitwidth option to select the bitwidth to use when quantizing the weights, either 8 (default) or 16. 8w/16a is only supported
                        by the HTA currently. Can&#39;t mix with --bitwidth.
    [ --bitwidth=&lt;val&gt; ]
                        Use the --bitwidth option to select the bitwidth to use when quantizing the weights/activation/bias, either 8 (default) or 16. Can&#39;t mix with
                        --weights_bitwidth or --act_bitwidth or --bias_bitdwith.
    [ --udo_package_path=&lt;val&gt; ]
                        Use this option to specify path to the Registration Library for a UDO Package. Usage is:
                        --udo_package_path=&lt;path_to_reg_lib&gt;
                        This option must be specified for Networks with UDO. All UDO&#39;s in Network must have host-executable CPU Implementation
    [ --axis_quant ]
                        Use the --axis_quant option to select per-axis-element quantization for the weights and biases of certain layer types.
                        Currently only Convolution, Deconvolution and FullyConnected are supported.


Description:
Generate 8 or 16 bit TensorFlow style fixed point weight and activations encodings for a floating point DLC model.
</pre></div>
</div>
<div class="line-block">
<div class="line"><br /></div>
<div class="line">Additional details:</div>
</div>
<ul>
<li><p>For specifying input_list, refer to input_list
argument in
<a class="reference external" href="tools.html#snpe-net-run">snpe-net-run</a> for
supported input formats (in order to calculate output
activation encoding information for all layers, <strong>do not</strong>
include the line which specifies desired outputs).</p></li>
<li><p>The tool requires the batch dimension of the DLC input file
to be set to 1 during the original model conversion step.</p></li>
<li><p>An example of quantization using snpe-dlc-quantize can be
found in the C/C++ Tutorial section: <a class="reference external" href="tutorial_inceptionv3.html">Running the Inception
v3 Model</a>. For details on
quantization see <a class="reference external" href="quantized_models.html">Quantized vs Non-Quantized
Models</a>.</p></li>
<li><p>Using snpe-dlc-quantize is mandatory for running on HTA.</p></li>
<li><p>Using snpe-dlc-quantize is mandatory for running on DSP
runtime on Snapdragon 865. It is recommended that offline
cache generation be used. It is specified by using
<strong>–enable_htp</strong> option for snpe-dlc-quantize.</p></li>
<li><p>When using offline cache generation for HTP, the same
input(s) tensors or layers and output(s) tensors or layers
should be specified when using snpe-dlc-quantize and to run
inference on the model using Qualcomm® Neural Processing SDK APIs or snpe-net-run. Not
doing so will cause the cache to be invalidated, and graph
initialization will take longer.</p></li>
<li><p>Outputs can be specified for snpe-dlc-quantize by modifying
the input_list in the following ways:</p>
<blockquote>
<div><div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>#&lt;output_layer_name&gt;[&lt;space&gt;&lt;output_layer_name&gt;]
%&lt;output_tensor_name&gt;[&lt;space&gt;&lt;output_tensor_name&gt;]
&lt;input_layer_name&gt;:=&lt;input_layer_path&gt;[&lt;space&gt;&lt;input_layer_name&gt;:=&lt;input_layer_path&gt;]
…
</pre></div>
</div>
<p><strong>Note:</strong> Output tensors and layers can be specified
individually, but when specifying both, the order shown must
be used to specify each.</p>
</div></blockquote>
</li>
<li><p>When running a model with an offline generated cache using
snpe-net-run:</p>
<blockquote>
<div><ul class="simple">
<li><p>Any output layers specified when snpe-dlc-quantize was
called, need to be specified using the input list as
shown in the input_list argument to
<a class="reference external" href="tools.html#snpe-net-run">snpe-net-run</a>.</p></li>
<li><p>Any output tensors specified when snpe-dlc-quantized was
called, need to be specified using the
<em>–set_output_tensors</em> argument to snpe-net-run. Refer to
<a class="reference external" href="tools.html#snpe-net-run">snpe-net-run</a> for
documentation.</p></li>
</ul>
</div></blockquote>
</li>
<li><p>When using the Qualcomm® Neural Processing SDK API:</p>
<blockquote>
<div><ul class="simple">
<li><p>Any output layers specified when snpe-dlc-quantize was
called, need to be specified using the
<span class="xref std std-ref">Snpe_SNPEBuilder_SetOutputLayers()</span>
function.</p></li>
<li><p>Any output tensors specified when snpe-dlc-quantize was
called, need to be specified using the
<span class="xref std std-ref">Snpe_SNPEBuilder_SetOutputTensors()</span>
function.</p></li>
</ul>
</div></blockquote>
</li>
</ul>
<div class="line-block">
<div class="line"><br /></div>
</div>
</div>
<div class="section" id="snpe-udo-package-generator">
<h3>snpe-udo-package-generator<a class="headerlink" href="#snpe-udo-package-generator" title="Permalink to this heading">¶</a></h3>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>DESCRIPTION:
------------
This tool generates a UDO (User Defined Operation) package using a
user provided config file.

USAGE:
------------
snpe-udo-package-generator [-h] --config_path CONFIG_PATH [--debug]
                                    [--output_path OUTPUT_PATH] [-f]
OPTIONAL ARGUMENTS:
-------------------
    -h, --help            show this help message and exit
    --debug               Returns debugging information from generating the package
    --output_path OUTPUT_PATH, -o OUTPUT_PATH
                        Path where the package should be saved
    -f, --force-generation
                        This option will delete the existing package
                        Note appropriate file permissions must be set to use
                        this option.

REQUIRED_ARGUMENTS:
-------------------
    --config_path CONFIG_PATH, -p CONFIG_PATH
                        The path to a config file that defines a UDO.
</pre></div>
</div>
</div>
<div class="section" id="qairt-quantizer">
<h3>qairt-quantizer<a class="headerlink" href="#qairt-quantizer" title="Permalink to this heading">¶</a></h3>
<p>The <strong>qairt-quantizer</strong> tool converts non-quantized DLC models into quantized DLC models.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Basic</span> <span class="n">command</span> <span class="n">line</span> <span class="n">usage</span> <span class="n">looks</span> <span class="n">like</span><span class="p">:</span>

<span class="n">usage</span><span class="p">:</span> <span class="n">qairt</span><span class="o">-</span><span class="n">quantizer</span> <span class="o">--</span><span class="n">input_dlc</span> <span class="n">INPUT_DLC</span> <span class="p">[</span><span class="o">-</span><span class="n">h</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">output_dlc</span> <span class="n">OUTPUT_DLC</span><span class="p">]</span>
                   <span class="p">[</span><span class="o">--</span><span class="n">input_list</span> <span class="n">INPUT_LIST</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">float_fallback</span><span class="p">]</span>
                   <span class="p">[</span><span class="o">--</span><span class="n">algorithms</span> <span class="n">ALGORITHMS</span> <span class="p">[</span><span class="n">ALGORITHMS</span> <span class="o">...</span><span class="p">]]</span> <span class="p">[</span><span class="o">--</span><span class="n">bias_bitwidth</span> <span class="n">BIAS_BITWIDTH</span><span class="p">]</span>
                   <span class="p">[</span><span class="o">--</span><span class="n">act_bitwidth</span> <span class="n">ACT_BITWIDTH</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">weights_bitwidth</span> <span class="n">WEIGHTS_BITWIDTH</span><span class="p">]</span>
                   <span class="p">[</span><span class="o">--</span><span class="n">float_bitwidth</span> <span class="n">FLOAT_BITWIDTH</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">float_bias_bitwidth</span> <span class="n">FLOAT_BIAS_BITWIDTH</span><span class="p">]</span>
                   <span class="p">[</span><span class="o">--</span><span class="n">ignore_encodings</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">use_per_channel_quantization</span><span class="p">]</span>
                   <span class="p">[</span><span class="o">--</span><span class="n">use_per_row_quantization</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">use_native_input_files</span><span class="p">]</span>
                   <span class="p">[</span><span class="o">--</span><span class="n">use_native_output_files</span><span class="p">]</span>
                   <span class="p">[</span><span class="o">--</span><span class="n">restrict_quantization_steps</span> <span class="n">ENCODING_MIN</span><span class="p">,</span> <span class="n">ENCODING_MAX</span><span class="p">]</span>
                   <span class="p">[</span><span class="o">--</span><span class="n">act_quantizer_calibration</span> <span class="n">ACT_QUANTIZER_CALIBRATION</span><span class="p">]</span>
                   <span class="p">[</span><span class="o">--</span><span class="n">param_quantizer_calibration</span> <span class="n">PARAM_QUANTIZER_CALIBRATION</span><span class="p">]</span>
                   <span class="p">[</span><span class="o">--</span><span class="n">act_quantizer_schema</span> <span class="n">ACT_QUANTIZER_SCHEMA</span><span class="p">]</span>
                   <span class="p">[</span><span class="o">--</span><span class="n">param_quantizer_schema</span> <span class="n">PARAM_QUANTIZER_SCHEMA</span><span class="p">]</span>
                   <span class="p">[</span><span class="o">--</span><span class="n">percentile_calibration_value</span> <span class="n">PERCENTILE_CALIBRATION_VALUE</span><span class="p">]</span>
                   <span class="p">[</span><span class="o">--</span><span class="n">use_aimet_quantizer</span><span class="p">]</span>
                   <span class="p">[</span><span class="o">--</span><span class="n">op_package_lib</span> <span class="n">OP_PACKAGE_LIB</span><span class="p">]</span>
                   <span class="p">[</span><span class="o">--</span><span class="n">dump_encoding_json</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">debug</span> <span class="p">[</span><span class="n">DEBUG</span><span class="p">]]</span>

<span class="n">required</span> <span class="n">arguments</span><span class="p">:</span>
  <span class="o">--</span><span class="n">input_dlc</span> <span class="n">INPUT_DLC</span>
                        <span class="n">Path</span> <span class="n">to</span> <span class="n">the</span> <span class="n">dlc</span> <span class="n">container</span> <span class="n">containing</span> <span class="n">the</span> <span class="n">model</span> <span class="k">for</span> <span class="n">which</span> <span class="n">fixed</span><span class="o">-</span><span class="n">point</span>
                        <span class="n">encoding</span> <span class="n">metadata</span> <span class="n">should</span> <span class="n">be</span> <span class="n">generated</span><span class="o">.</span> <span class="n">This</span> <span class="n">argument</span> <span class="ow">is</span> <span class="n">required</span>

<span class="n">optional</span> <span class="n">arguments</span><span class="p">:</span>
  <span class="o">-</span><span class="n">h</span><span class="p">,</span> <span class="o">--</span><span class="n">help</span>            <span class="n">show</span> <span class="n">this</span> <span class="n">help</span> <span class="n">message</span> <span class="ow">and</span> <span class="n">exit</span>
  <span class="o">--</span><span class="n">output_dlc</span> <span class="n">OUTPUT_DLC</span>
                        <span class="n">Path</span> <span class="n">at</span> <span class="n">which</span> <span class="n">the</span> <span class="n">metadata</span><span class="o">-</span><span class="n">included</span> <span class="n">quantized</span> <span class="n">model</span> <span class="n">container</span> <span class="n">should</span> <span class="n">be</span>
                        <span class="n">written</span><span class="o">.</span><span class="n">If</span> <span class="n">this</span> <span class="n">argument</span> <span class="ow">is</span> <span class="n">omitted</span><span class="p">,</span> <span class="n">the</span> <span class="n">quantized</span> <span class="n">model</span> <span class="n">will</span> <span class="n">be</span> <span class="n">written</span> <span class="n">at</span>
                        <span class="o">&lt;</span><span class="n">unquantized_model_name</span><span class="o">&gt;</span><span class="n">_quantized</span><span class="o">.</span><span class="n">dlc</span>
  <span class="o">--</span><span class="n">input_list</span> <span class="n">INPUT_LIST</span>
                        <span class="n">Path</span> <span class="n">to</span> <span class="n">a</span> <span class="n">file</span> <span class="n">specifying</span> <span class="n">the</span> <span class="nb">input</span> <span class="n">data</span><span class="o">.</span> <span class="n">This</span> <span class="n">file</span> <span class="n">should</span> <span class="n">be</span> <span class="n">a</span> <span class="n">plain</span> <span class="n">text</span>
                        <span class="n">file</span><span class="p">,</span> <span class="n">containing</span> <span class="n">one</span> <span class="ow">or</span> <span class="n">more</span> <span class="n">absolute</span> <span class="n">file</span> <span class="n">paths</span> <span class="n">per</span> <span class="n">line</span><span class="o">.</span> <span class="n">Each</span> <span class="n">path</span> <span class="ow">is</span>
                        <span class="n">expected</span> <span class="n">to</span> <span class="n">point</span> <span class="n">to</span> <span class="n">a</span> <span class="n">binary</span> <span class="n">file</span> <span class="n">containing</span> <span class="n">one</span> <span class="nb">input</span> <span class="ow">in</span> <span class="n">the</span> <span class="s2">&quot;raw&quot;</span> <span class="nb">format</span><span class="p">,</span>
                        <span class="n">ready</span> <span class="n">to</span> <span class="n">be</span> <span class="n">consumed</span> <span class="n">by</span> <span class="n">the</span> <span class="n">quantizer</span> <span class="n">without</span> <span class="nb">any</span> <span class="n">further</span> <span class="n">preprocessing</span><span class="o">.</span>
                        <span class="n">Multiple</span> <span class="n">files</span> <span class="n">per</span> <span class="n">line</span> <span class="n">separated</span> <span class="n">by</span> <span class="n">spaces</span> <span class="n">indicate</span> <span class="n">multiple</span> <span class="n">inputs</span> <span class="n">to</span> <span class="n">the</span>
                        <span class="n">network</span><span class="o">.</span> <span class="n">See</span> <span class="n">documentation</span> <span class="k">for</span> <span class="n">more</span> <span class="n">details</span><span class="o">.</span> <span class="n">Must</span> <span class="n">be</span> <span class="n">specified</span> <span class="k">for</span>
                        <span class="n">quantization</span><span class="o">.</span> <span class="n">All</span> <span class="n">subsequent</span> <span class="n">quantization</span> <span class="n">options</span> <span class="n">are</span> <span class="n">ignored</span> <span class="n">when</span> <span class="n">this</span> <span class="ow">is</span>
                        <span class="ow">not</span> <span class="n">provided</span><span class="o">.</span>
  <span class="o">--</span><span class="n">float_fallback</span>      <span class="n">Use</span> <span class="n">this</span> <span class="n">option</span> <span class="n">to</span> <span class="n">enable</span> <span class="n">fallback</span> <span class="n">to</span> <span class="n">floating</span> <span class="n">point</span> <span class="p">(</span><span class="n">FP</span><span class="p">)</span> <span class="n">instead</span> <span class="n">of</span> <span class="n">fixed</span>
                        <span class="n">point</span><span class="o">.</span>
                        <span class="n">This</span> <span class="n">option</span> <span class="n">can</span> <span class="n">be</span> <span class="n">paired</span> <span class="k">with</span> <span class="o">--</span><span class="n">float_bitwidth</span> <span class="n">to</span> <span class="n">indicate</span> <span class="n">the</span> <span class="n">bitwidth</span> <span class="k">for</span>
                        <span class="n">FP</span> <span class="p">(</span><span class="n">by</span> <span class="n">default</span> <span class="mi">32</span><span class="p">)</span><span class="o">.</span>
                        <span class="n">If</span> <span class="n">this</span> <span class="n">option</span> <span class="ow">is</span> <span class="n">enabled</span><span class="p">,</span> <span class="n">then</span> <span class="nb">input</span> <span class="nb">list</span> <span class="n">must</span> <span class="ow">not</span> <span class="n">be</span> <span class="n">provided</span> <span class="ow">and</span>
                        <span class="o">--</span><span class="n">ignore_encodings</span> <span class="n">must</span> <span class="ow">not</span> <span class="n">be</span> <span class="n">provided</span><span class="o">.</span>
                        <span class="n">The</span> <span class="n">external</span> <span class="n">quantization</span> <span class="n">encodings</span> <span class="p">(</span><span class="n">encoding</span> <span class="n">file</span><span class="o">/</span><span class="n">FakeQuant</span> <span class="n">encodings</span><span class="p">)</span>
                        <span class="n">might</span> <span class="n">be</span> <span class="n">missing</span> <span class="n">quantization</span> <span class="n">parameters</span> <span class="k">for</span> <span class="n">some</span> <span class="n">interim</span> <span class="n">tensors</span><span class="o">.</span>
                        <span class="n">First</span> <span class="n">it</span> <span class="n">will</span> <span class="k">try</span> <span class="n">to</span> <span class="n">fill</span> <span class="n">the</span> <span class="n">gaps</span> <span class="n">by</span> <span class="n">propagating</span> <span class="n">across</span> <span class="n">math</span><span class="o">-</span><span class="n">invariant</span>
                        <span class="n">functions</span><span class="o">.</span> <span class="n">If</span> <span class="n">the</span> <span class="n">quantization</span> <span class="n">params</span> <span class="n">are</span> <span class="n">still</span> <span class="n">missing</span><span class="p">,</span>
                        <span class="n">then</span> <span class="n">it</span> <span class="n">will</span> <span class="n">apply</span> <span class="n">fallback</span> <span class="n">to</span> <span class="n">nodes</span> <span class="n">to</span> <span class="n">floating</span> <span class="n">point</span><span class="o">.</span>
  <span class="o">--</span><span class="n">algorithms</span> <span class="n">ALGORITHMS</span> <span class="p">[</span><span class="n">ALGORITHMS</span> <span class="o">...</span><span class="p">]</span>
                        <span class="n">Use</span> <span class="n">this</span> <span class="n">option</span> <span class="n">to</span> <span class="n">enable</span> <span class="n">new</span> <span class="n">optimization</span> <span class="n">algorithms</span><span class="o">.</span> <span class="n">Usage</span> <span class="ow">is</span><span class="p">:</span>
                        <span class="o">--</span><span class="n">algorithms</span> <span class="o">&lt;</span><span class="n">algo_name1</span><span class="o">&gt;</span> <span class="o">...</span> <span class="n">The</span> <span class="n">available</span> <span class="n">optimization</span> <span class="n">algorithms</span> <span class="n">are</span><span class="p">:</span>
                        <span class="s2">&quot;cle&quot;</span> <span class="o">-</span> <span class="n">Cross</span> <span class="n">layer</span> <span class="n">equalization</span> <span class="n">includes</span> <span class="n">a</span> <span class="n">number</span> <span class="n">of</span> <span class="n">methods</span> <span class="k">for</span> <span class="n">equalizing</span>
                        <span class="n">weights</span> <span class="ow">and</span> <span class="n">biases</span> <span class="n">across</span> <span class="n">layers</span> <span class="ow">in</span> <span class="n">order</span> <span class="n">to</span> <span class="n">rectify</span> <span class="n">imbalances</span> <span class="n">that</span> <span class="n">cause</span>
                        <span class="n">quantization</span> <span class="n">errors</span><span class="o">.</span>
  <span class="o">--</span><span class="n">bias_bitwidth</span> <span class="n">BIAS_BITWIDTH</span>
                        <span class="n">Use</span> <span class="n">the</span> <span class="o">--</span><span class="n">bias_bitwidth</span> <span class="n">option</span> <span class="n">to</span> <span class="n">select</span> <span class="n">the</span> <span class="n">bitwidth</span> <span class="n">to</span> <span class="n">use</span> <span class="n">when</span> <span class="n">quantizing</span>
                        <span class="n">the</span> <span class="n">biases</span><span class="p">,</span> <span class="n">either</span> <span class="mi">8</span> <span class="p">(</span><span class="n">default</span><span class="p">)</span> <span class="ow">or</span> <span class="mf">32.</span>
  <span class="o">--</span><span class="n">act_bitwidth</span> <span class="n">ACT_BITWIDTH</span>
                        <span class="n">Use</span> <span class="n">the</span> <span class="o">--</span><span class="n">act_bitwidth</span> <span class="n">option</span> <span class="n">to</span> <span class="n">select</span> <span class="n">the</span> <span class="n">bitwidth</span> <span class="n">to</span> <span class="n">use</span> <span class="n">when</span> <span class="n">quantizing</span>
                        <span class="n">the</span> <span class="n">activations</span><span class="p">,</span> <span class="n">either</span> <span class="mi">8</span> <span class="p">(</span><span class="n">default</span><span class="p">)</span> <span class="ow">or</span> <span class="mf">16.</span>
  <span class="o">--</span><span class="n">weights_bitwidth</span> <span class="n">WEIGHTS_BITWIDTH</span>
                        <span class="n">Use</span> <span class="n">the</span> <span class="o">--</span><span class="n">weights_bitwidth</span> <span class="n">option</span> <span class="n">to</span> <span class="n">select</span> <span class="n">the</span> <span class="n">bitwidth</span> <span class="n">to</span> <span class="n">use</span> <span class="n">when</span>
                        <span class="n">quantizing</span> <span class="n">the</span> <span class="n">weights</span><span class="p">,</span> <span class="n">either</span> <span class="mi">4</span> <span class="ow">or</span> <span class="mi">8</span> <span class="p">(</span><span class="n">default</span><span class="p">)</span><span class="o">.</span>
  <span class="o">--</span><span class="n">float_bitwidth</span> <span class="n">FLOAT_BITWIDTH</span>
                        <span class="n">Use</span> <span class="n">the</span> <span class="o">--</span><span class="n">float_bitwidth</span> <span class="n">option</span> <span class="n">to</span> <span class="n">select</span> <span class="n">the</span> <span class="n">bitwidth</span> <span class="n">to</span> <span class="n">use</span> <span class="k">for</span> <span class="nb">float</span>
                        <span class="n">tensors</span><span class="p">,</span><span class="n">either</span> <span class="mi">32</span> <span class="p">(</span><span class="n">default</span><span class="p">)</span> <span class="ow">or</span> <span class="mf">16.</span>
  <span class="o">--</span><span class="n">float_bias_bitwidth</span> <span class="n">FLOAT_BIAS_BITWIDTH</span>
                        <span class="n">Use</span> <span class="n">the</span> <span class="o">--</span><span class="n">float_bias_bitwidth</span> <span class="n">option</span> <span class="n">to</span> <span class="n">select</span> <span class="n">the</span> <span class="n">bitwidth</span> <span class="n">to</span> <span class="n">use</span> <span class="n">when</span>
                        <span class="n">biases</span> <span class="n">are</span> <span class="ow">in</span> <span class="nb">float</span><span class="p">,</span> <span class="n">either</span> <span class="mi">32</span> <span class="ow">or</span> <span class="mf">16.</span>
  <span class="o">--</span><span class="n">ignore_encodings</span>    <span class="n">Use</span> <span class="n">only</span> <span class="n">quantizer</span> <span class="n">generated</span> <span class="n">encodings</span><span class="p">,</span> <span class="n">ignoring</span> <span class="nb">any</span> <span class="n">user</span> <span class="ow">or</span> <span class="n">model</span> <span class="n">provided</span>
                        <span class="n">encodings</span><span class="o">.</span>
                        <span class="n">Note</span><span class="p">:</span> <span class="n">Cannot</span> <span class="n">use</span> <span class="o">--</span><span class="n">ignore_encodings</span> <span class="k">with</span> <span class="o">--</span><span class="n">quantization_overrides</span>
  <span class="o">--</span><span class="n">use_per_channel_quantization</span>
                        <span class="n">Use</span> <span class="n">this</span> <span class="n">option</span> <span class="n">to</span> <span class="n">enable</span> <span class="n">per</span><span class="o">-</span><span class="n">channel</span> <span class="n">quantization</span> <span class="k">for</span> <span class="n">convolution</span><span class="o">-</span><span class="n">based</span> <span class="n">op</span>
                        <span class="n">weights</span><span class="o">.</span>
                        <span class="n">Note</span><span class="p">:</span> <span class="n">This</span> <span class="n">will</span> <span class="n">replace</span> <span class="n">built</span><span class="o">-</span><span class="ow">in</span> <span class="n">model</span> <span class="n">QAT</span> <span class="n">encodings</span> <span class="n">when</span> <span class="n">used</span> <span class="k">for</span> <span class="n">a</span> <span class="n">given</span>
                        <span class="n">weight</span><span class="o">.</span>
  <span class="o">--</span><span class="n">use_per_row_quantization</span>
                        <span class="n">Use</span> <span class="n">this</span> <span class="n">option</span> <span class="n">to</span> <span class="n">enable</span> <span class="n">rowwise</span> <span class="n">quantization</span> <span class="n">of</span> <span class="n">Matmul</span> <span class="ow">and</span> <span class="n">FullyConnected</span>
                        <span class="n">ops</span><span class="o">.</span>
  <span class="o">--</span><span class="n">use_native_input_files</span>
                        <span class="n">Boolean</span> <span class="n">flag</span> <span class="n">to</span> <span class="n">indicate</span> <span class="n">how</span> <span class="n">to</span> <span class="n">read</span> <span class="nb">input</span> <span class="n">files</span><span class="p">:</span>
                        <span class="mf">1.</span> <span class="nb">float</span> <span class="p">(</span><span class="n">default</span><span class="p">):</span> <span class="n">reads</span> <span class="n">inputs</span> <span class="k">as</span> <span class="n">floats</span> <span class="ow">and</span> <span class="n">quantizes</span> <span class="k">if</span> <span class="n">necessary</span> <span class="n">based</span>
                        <span class="n">on</span> <span class="n">quantization</span> <span class="n">parameters</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">model</span><span class="o">.</span>
                        <span class="mf">2.</span> <span class="n">native</span><span class="p">:</span>          <span class="n">reads</span> <span class="n">inputs</span> <span class="n">assuming</span> <span class="n">the</span> <span class="n">data</span> <span class="nb">type</span> <span class="n">to</span> <span class="n">be</span> <span class="n">native</span> <span class="n">to</span> <span class="n">the</span>
                        <span class="n">model</span><span class="o">.</span> <span class="n">For</span> <span class="n">ex</span><span class="o">.</span><span class="p">,</span> <span class="n">uint8_t</span><span class="o">.</span>
  <span class="o">--</span><span class="n">use_native_output_files</span>
                        <span class="n">Use</span> <span class="n">this</span> <span class="n">option</span> <span class="n">to</span> <span class="n">indicate</span> <span class="n">the</span> <span class="n">data</span> <span class="nb">type</span> <span class="n">of</span> <span class="n">the</span> <span class="n">output</span> <span class="n">files</span>
                        <span class="mf">1.</span> <span class="nb">float</span> <span class="p">(</span><span class="n">default</span><span class="p">):</span> <span class="n">output</span> <span class="n">the</span> <span class="n">file</span> <span class="k">as</span> <span class="n">floats</span><span class="o">.</span>
                        <span class="mf">2.</span> <span class="n">native</span><span class="p">:</span>          <span class="n">outputs</span> <span class="n">the</span> <span class="n">file</span> <span class="n">that</span> <span class="ow">is</span> <span class="n">native</span> <span class="n">to</span> <span class="n">the</span> <span class="n">model</span><span class="o">.</span> <span class="n">For</span> <span class="n">ex</span><span class="o">.</span><span class="p">,</span>
                        <span class="n">uint8_t</span><span class="o">.</span>
  <span class="o">--</span><span class="n">restrict_quantization_steps</span> <span class="n">ENCODING_MIN</span><span class="p">,</span> <span class="n">ENCODING_MAX</span>
                        <span class="n">Specifies</span> <span class="n">the</span> <span class="n">number</span> <span class="n">of</span> <span class="n">steps</span> <span class="n">to</span> <span class="n">use</span> <span class="k">for</span> <span class="n">computing</span> <span class="n">quantization</span> <span class="n">encodings</span>
                        <span class="n">such</span> <span class="n">that</span> <span class="n">scale</span> <span class="o">=</span> <span class="p">(</span><span class="nb">max</span> <span class="o">-</span> <span class="nb">min</span><span class="p">)</span> <span class="o">/</span> <span class="n">number</span> <span class="n">of</span> <span class="n">quantization</span> <span class="n">steps</span><span class="o">.</span>
                        <span class="n">The</span> <span class="n">option</span> <span class="n">should</span> <span class="n">be</span> <span class="n">passed</span> <span class="k">as</span> <span class="n">a</span> <span class="n">space</span> <span class="n">separated</span> <span class="n">pair</span> <span class="n">of</span> <span class="n">hexadecimal</span> <span class="n">string</span>
                        <span class="n">minimum</span> <span class="ow">and</span> <span class="n">maximum</span> <span class="n">valuesi</span><span class="o">.</span><span class="n">e</span><span class="o">.</span> <span class="o">--</span><span class="n">restrict_quantization_steps</span> <span class="s2">&quot;MIN MAX&quot;</span><span class="o">.</span>
                         <span class="n">Please</span> <span class="n">note</span> <span class="n">that</span> <span class="n">this</span> <span class="ow">is</span> <span class="n">a</span> <span class="n">hexadecimal</span> <span class="n">string</span> <span class="n">literal</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">a</span> <span class="n">signed</span>
                        <span class="n">integer</span><span class="p">,</span> <span class="n">to</span> <span class="n">supply</span> <span class="n">a</span> <span class="n">negative</span> <span class="n">value</span> <span class="n">an</span> <span class="n">explicit</span> <span class="n">minus</span> <span class="n">sign</span> <span class="ow">is</span> <span class="n">required</span><span class="o">.</span>
                        <span class="n">E</span><span class="o">.</span><span class="n">g</span><span class="o">.--</span><span class="n">restrict_quantization_steps</span> <span class="s2">&quot;-0x80 0x7F&quot;</span> <span class="n">indicates</span> <span class="n">an</span> <span class="n">example</span> <span class="mi">8</span> <span class="n">bit</span>
                        <span class="nb">range</span><span class="p">,</span>
                            <span class="o">--</span><span class="n">restrict_quantization_steps</span> <span class="s2">&quot;-0x8000 0x7F7F&quot;</span> <span class="n">indicates</span> <span class="n">an</span> <span class="n">example</span> <span class="mi">16</span>
                        <span class="n">bit</span> <span class="nb">range</span><span class="o">.</span>
                        <span class="n">This</span> <span class="n">argument</span> <span class="ow">is</span> <span class="n">required</span> <span class="k">for</span> <span class="mi">16</span><span class="o">-</span><span class="n">bit</span> <span class="n">Matmul</span> <span class="n">operations</span><span class="o">.</span>
  <span class="o">--</span><span class="n">act_quantizer_calibration</span> <span class="n">ACT_QUANTIZER_CALIBRATION</span>
                        <span class="n">Specify</span> <span class="n">which</span> <span class="n">quantization</span> <span class="n">calibration</span> <span class="n">method</span> <span class="n">to</span> <span class="n">use</span> <span class="k">for</span> <span class="n">activations</span>
                        <span class="n">supported</span> <span class="n">values</span><span class="p">:</span> <span class="nb">min</span><span class="o">-</span><span class="nb">max</span> <span class="p">(</span><span class="n">default</span><span class="p">),</span> <span class="n">sqnr</span><span class="p">,</span> <span class="n">entropy</span><span class="p">,</span> <span class="n">mse</span><span class="p">,</span> <span class="n">percentile</span>
                        <span class="n">This</span> <span class="n">option</span> <span class="n">can</span> <span class="n">be</span> <span class="n">paired</span> <span class="k">with</span> <span class="o">--</span><span class="n">act_quantizer_schema</span> <span class="n">to</span> <span class="n">override</span> <span class="n">the</span>
                        <span class="n">quantization</span>
                        <span class="n">schema</span> <span class="n">to</span> <span class="n">use</span> <span class="k">for</span> <span class="n">activations</span> <span class="n">otherwise</span> <span class="n">default</span> <span class="n">schema</span><span class="p">(</span><span class="n">asymmetric</span><span class="p">)</span> <span class="n">will</span> <span class="n">be</span>
                        <span class="n">used</span>
  <span class="o">--</span><span class="n">param_quantizer_calibration</span> <span class="n">PARAM_QUANTIZER_CALIBRATION</span>
                        <span class="n">Specify</span> <span class="n">which</span> <span class="n">quantization</span> <span class="n">calibration</span> <span class="n">method</span> <span class="n">to</span> <span class="n">use</span> <span class="k">for</span> <span class="n">parameters</span>
                        <span class="n">supported</span> <span class="n">values</span><span class="p">:</span> <span class="nb">min</span><span class="o">-</span><span class="nb">max</span> <span class="p">(</span><span class="n">default</span><span class="p">),</span> <span class="n">sqnr</span><span class="p">,</span> <span class="n">entropy</span><span class="p">,</span> <span class="n">mse</span><span class="p">,</span> <span class="n">percentile</span>
                        <span class="n">This</span> <span class="n">option</span> <span class="n">can</span> <span class="n">be</span> <span class="n">paired</span> <span class="k">with</span> <span class="o">--</span><span class="n">param_quantizer_schema</span> <span class="n">to</span> <span class="n">override</span> <span class="n">the</span>
                        <span class="n">quantization</span>
                        <span class="n">schema</span> <span class="n">to</span> <span class="n">use</span> <span class="k">for</span> <span class="n">parameters</span> <span class="n">otherwise</span> <span class="n">default</span> <span class="n">schema</span><span class="p">(</span><span class="n">asymmetric</span><span class="p">)</span> <span class="n">will</span> <span class="n">be</span>
                        <span class="n">used</span>
  <span class="o">--</span><span class="n">act_quantizer_schema</span> <span class="n">ACT_QUANTIZER_SCHEMA</span>
                        <span class="n">Specify</span> <span class="n">which</span> <span class="n">quantization</span> <span class="n">schema</span> <span class="n">to</span> <span class="n">use</span> <span class="k">for</span> <span class="n">activations</span>
                        <span class="n">supported</span> <span class="n">values</span><span class="p">:</span> <span class="n">asymmetric</span> <span class="p">(</span><span class="n">default</span><span class="p">),</span> <span class="n">symmetric</span>
  <span class="o">--</span><span class="n">param_quantizer_schema</span> <span class="n">PARAM_QUANTIZER_SCHEMA</span>
                        <span class="n">Specify</span> <span class="n">which</span> <span class="n">quantization</span> <span class="n">schema</span> <span class="n">to</span> <span class="n">use</span> <span class="k">for</span> <span class="n">parameters</span>
                        <span class="n">supported</span> <span class="n">values</span><span class="p">:</span> <span class="n">asymmetric</span> <span class="p">(</span><span class="n">default</span><span class="p">),</span> <span class="n">symmetric</span>
  <span class="o">--</span><span class="n">percentile_calibration_value</span> <span class="n">PERCENTILE_CALIBRATION_VALUE</span>
                        <span class="n">Specify</span> <span class="n">the</span> <span class="n">percentile</span> <span class="n">value</span> <span class="n">to</span> <span class="n">be</span> <span class="n">used</span> <span class="k">with</span> <span class="n">Percentile</span> <span class="n">calibration</span> <span class="n">method</span>
                        <span class="n">The</span> <span class="n">specified</span> <span class="nb">float</span> <span class="n">value</span> <span class="n">must</span> <span class="n">lie</span> <span class="n">within</span> <span class="mi">90</span> <span class="ow">and</span> <span class="mi">100</span><span class="p">,</span> <span class="n">default</span><span class="p">:</span> <span class="mf">99.99</span>
  <span class="o">--</span><span class="n">use_aimet_quantizer</span>
                        <span class="n">Use</span> <span class="n">AIMET</span> <span class="n">Quantizer</span> <span class="ow">in</span> <span class="n">place</span> <span class="n">of</span> <span class="n">IR</span> <span class="n">Quantizer</span><span class="o">.</span> <span class="n">The</span> <span class="n">following</span> <span class="n">arguments</span> <span class="n">are</span>
                        <span class="ow">not</span> <span class="n">allowed</span> <span class="n">together</span> <span class="k">with</span> <span class="n">this</span> <span class="n">option</span><span class="p">,</span> <span class="o">--</span><span class="n">float_bw</span><span class="p">,</span> <span class="o">--</span><span class="n">float_bitwidth</span><span class="p">,</span> <span class="o">--</span><span class="n">float_bias_bw</span>
                        <span class="o">--</span><span class="n">float_bias_bitwidth</span><span class="p">,</span> <span class="o">--</span><span class="n">disable_relu_squashing</span><span class="p">,</span> <span class="o">--</span><span class="n">restrict_quantization_steps</span><span class="p">,</span>
                        <span class="o">--</span><span class="n">float_fallback</span><span class="p">,</span> <span class="o">--</span><span class="n">use_dynamic_16_bit_weights</span><span class="p">,</span> <span class="o">--</span><span class="n">pack_4_bit_weights</span><span class="p">,</span> <span class="o">--</span><span class="n">op_package_lib</span>
  <span class="o">--</span><span class="n">op_package_lib</span> <span class="n">OP_PACKAGE_LIB</span><span class="p">,</span> <span class="o">-</span><span class="n">opl</span> <span class="n">OP_PACKAGE_LIB</span>
                        <span class="n">Use</span> <span class="n">this</span> <span class="n">argument</span> <span class="n">to</span> <span class="k">pass</span> <span class="n">an</span> <span class="n">op</span> <span class="n">package</span> <span class="n">library</span> <span class="k">for</span> <span class="n">quantization</span><span class="o">.</span> <span class="n">Must</span> <span class="n">be</span> <span class="ow">in</span>
                        <span class="n">the</span> <span class="n">form</span> <span class="o">&lt;</span><span class="n">op_package_lib_path</span><span class="p">:</span><span class="n">interfaceProviderName</span><span class="o">&gt;</span> <span class="ow">and</span> <span class="n">be</span> <span class="n">separated</span> <span class="n">by</span> <span class="n">a</span>
                        <span class="n">comma</span> <span class="k">for</span> <span class="n">multiple</span> <span class="n">package</span> <span class="n">libs</span>
  <span class="o">--</span><span class="n">dump_encoding_json</span>  <span class="n">Use</span> <span class="n">this</span> <span class="n">argument</span> <span class="n">to</span> <span class="n">dump</span> <span class="n">encoding</span> <span class="n">of</span> <span class="nb">all</span> <span class="n">the</span> <span class="n">tensors</span> <span class="ow">in</span> <span class="n">a</span> <span class="n">json</span> <span class="n">file</span>
  <span class="o">--</span><span class="n">debug</span> <span class="p">[</span><span class="n">DEBUG</span><span class="p">]</span>       <span class="n">Run</span> <span class="n">the</span> <span class="n">quantizer</span> <span class="ow">in</span> <span class="n">debug</span> <span class="n">mode</span><span class="o">.</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="execution">
<h2>Execution<a class="headerlink" href="#execution" title="Permalink to this heading">¶</a></h2>
<div class="section" id="snpe-net-run">
<h3>snpe-net-run<a class="headerlink" href="#snpe-net-run" title="Permalink to this heading">¶</a></h3>
<p>snpe-net-run loads a DLC file, loads the data for the input
tensor(s), and executes the network on the specified runtime.</p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>DESCRIPTION:
------------
Tool that loads and executes a neural network using the SDK API.


REQUIRED ARGUMENTS:
-------------------
--container  &lt;FILE&gt; Path to the DL container containing the network.
--input_list &lt;FILE&gt; Path to a file listing the inputs for the network.
                    Optionally the file can have &quot;#&quot; starting line to specify the layer names or &quot;%&quot; to sepecify the output tensor
                    names for which output tensor files are to be produced. For more details about the input_list file format,
                    please refer to SDK html documentation (docs/general/tools.html#snpe-net-run input_list argument).


OPTIONAL ARGUMENTS:
-------------------
--use_gpu           Use the GPU runtime.
--use_dsp           Use the DSP fixed point runtime.
--use_aip           Use the AIP fixed point runtime.
--debug             Specifies that output from all layers of the network
                    will be saved.
--output_dir=&lt;val&gt;
                    The directory to save output to. Defaults to ./output
--storage_dir=&lt;val&gt;
                    The directory to store metadata files
--encoding_type=&lt;val&gt;
                    Specifies the encoding type of input file. Valid settings are &quot;nv21&quot;.
                    Cannot be combined with --userbuffer*.
--use_native_input_files
                    Specifies to consume the input file(s) in their native data type(s).
                    Must be used with --userbuffer_xxx.
--use_native_output_files
                    Specifies to write the output file(s) in their native data type(s).
                    Must be used with --userbuffer_xxx.
--userbuffer_auto
                    Specifies to use userbuffer for input and output, with auto detection of types enabled.
                    Must be used with user specified buffer. Cannot be combined with --encoding_type.
--userbuffer_float
                    Specifies to use userbuffer for inference, and the input type is float.
                    Cannot be combined with --encoding_type.
--userbuffer_floatN=&lt;val&gt;
                    Specifies to use userbuffer for inference, and the input type is float 16 or float 32.
                    Cannot be combined with --encoding_type.
--userbuffer_tf8      Specifies to use userbuffer for inference, and the input type is tf8exact0.
                    Cannot be combined with --encoding_type.
--userbuffer_tfN=&lt;val&gt;
                    Overrides the userbuffer output used for inference, and the output type is tf8exact0 or tf16exact0.
                    Must be used with user specified buffer.
--userbuffer_float_output
                    Overrides the userbuffer output used for inference, and the output type is float. Must be used with user
                    specified buffer.
--userbuffer_floatN_output=&lt;val&gt;
                    Overrides the userbuffer output used for inference, and the output type is float 16 or float 32. Must be used with user
                    specified buffer.
--userbuffer_tfN_output=&lt;val&gt;
                    Overrides the userbuffer output used for inference, and the output type is tf8exact0 or tf16exact0.
                    Must be used with user specified buffer.
--userbuffer_tf8_output
                    Overrides the userbuffer output used for inference, and the output type is tf8exact0.
--userbuffer_uintN_output=&lt;val&gt;
                    Overrides the userbuffer output used for inference, and the output type is Uint N. Must be used with user
                    specified buffer.
--userbuffer_memorymapped
                    Specifies to use memory-mapped (zero-copy) user buffer. Must be used with --userbuffer_float or
                    --userbuffer_tf8 or userbuffer_tfN or userbuffer_auto etc. Cannot be combined with --encoding_type.
                    Note: Passing this option will turn all input and output userbuffers into memory mapped buffer.
--static_min_max  Specifies to use quantization parameters from the model instead of
                    input specific quantization. Used in conjunction with --userbuffer_tf8.
--resizable_dim=&lt;val&gt;
                    Specifies the maximum number that resizable dimensions can grow into.
                    Used as a hint to create UserBuffers for models with dynamic sized outputs. Should be a
                    positive integer and is not applicable when using ITensor.
--userbuffer_glbuffer
                    [EXPERIMENTAL]  Specifies to use userbuffer for inference, and the input source is OpenGL buffer.
                    Cannot be combined with --encoding_type.
                    GL buffer mode is only supported on Android OS.
--data_type_map=&lt;val&gt;
                    Sets data type of IO buffers during prepare.
                    Arguments should be provided in the following format:
                    --data_type_map buffer_name1=buffer_name1_data_type --data_type_map buffer_name2=buffer_name2_data_type
                    Data Type can have the following values: float16, float32, fixedPoint8, fixedPoint16, int8, int16, int32, int64, uint8, uint16, uint32, uint64, bool8
                    Note: Must use this option with --tensor_mode.
--tensor_mode=&lt;val&gt;
                    Sets type of tensor to use.
                    Arguments should be provided in the following format:
                    --tensor_mode itensor
                    Data Type can have the following values: userBuffer, itensor
--perf_profile=&lt;val&gt;
                    Specifies perf profile to set. Valid settings are &quot;low_balanced&quot; , &quot;balanced&quot; , &quot;default&quot;,
                    &quot;high_performance&quot; ,&quot;sustained_high_performance&quot;, &quot;burst&quot;, &quot;low_power_saver&quot;, &quot;power_saver&quot;,
                    &quot;high_power_saver&quot;, &quot;extreme_power_saver&quot;, and &quot;system_settings&quot;.
--perf_config_yaml  Specifies the path to the yaml file containing the perf profile settings.
--profiling_level=&lt;val&gt;
                    Specifies the profiling level.  Valid settings are &quot;off&quot;, &quot;basic&quot;, &quot;moderate&quot;, &quot;detailed&quot;, and &quot;linting&quot;.
                    Default is detailed.
--enable_cpu_fallback
                    Enables cpu fallback functionality. Defaults to disable mode.
--input_name=&lt;val&gt;
                    Specifies the name of graph and the name of input for which dimensions are specified
                    e.g. --input_name=&quot;&lt;graph name&gt; &lt;input name&gt;&quot;
--input_dimensions=&lt;val&gt;
                    Specifies new dimensions for input whose name is specified in input_name. e.g. --input_dimension 1,224,224,3
                    For multiple inputs, specify --input_name=&quot;&lt;graph name&gt; &lt;input name&gt;&quot; and --input_dimensions multiple times.&quot;
--gpu_mode=&lt;val&gt;  Specifies gpu operation mode. Valid settings are &quot;default&quot;, &quot;float16&quot;.
                    default = float32 math and float16 storage (equiv. use_gpu arg).
                    float16 = float16 math and float16 storage.
--enable_init_cache
                    Enable init caching mode to accelerate the network building process. Defaults to disable.
--platform_options=&lt;val&gt;
                    Specifies value to pass as platform options.
--priority_hint=&lt;val&gt;
                    Specifies hint for priority level.  Valid settings are &quot;low&quot;, &quot;normal&quot;, &quot;normal_high&quot;, &quot;high&quot;. Defaults to normal.
                    Note: &quot;normal_high&quot; is only available on DSP.
--inferences_per_duration=&lt;val&gt;
                    Specifies the number of inferences in specific duration (in seconds). e.g. &quot;10,20&quot;.
--runtime_order=&lt;val&gt;
                    Specifies the order of precedence for runtime e.g  cpu_float32, dsp_fixed8_tf etc
                    Valid values are:-
                    cpu_float32 (Snapdragon CPU)       = Data &amp; Math: float 32bit
                    gpu_float32_16_hybrid (Adreno GPU) = Data: float 16bit Math: float 32bit
                    dsp_fixed8_tf (Hexagon DSP)        = Data &amp; Math: 8bit fixed point Tensorflow style format
                    gpu_float16 (Adreno GPU)           = Data: float 16bit Math: float 16bit
--set_output_tensors=&lt;val&gt;
                    Optionally, Specifies a comma separated list of tensors to be output after execution.
                    If using Multi Graph DLC, use --set_output_tensors for each graph.
                    e.g --set_output_tensors=&quot;graphA tensorA1,tensorA2&quot; --set_output_tensors=&quot;graphB tensorB1,tensorB2&quot;
--set_unconsumed_as_output
                    Sets all unconsumed tensors as outputs.
                    aip_fixed8_tf (Snapdragon HTA+HVX) = Data &amp; Math: 8bit fixed point Tensorflow style format
                    cpu (Snapdragon CPU)               = Same as cpu_float32
                    gpu (Adreno GPU)                   = Same as gpu_float32_16_hybrid
                    dsp (Hexagon DSP)                  = Same as dsp_fixed8_tf
                    aip (Snapdragon HTA+HVX)           = Same as aip_fixed8_tf
--udo_package_path=&lt;val&gt;
                    Path to the registration library for UDO package(s).
                    Optionally, user can provide multiple packages as a comma-separated list.
--duration=&lt;val&gt;    Specified the duration of the run in seconds. Loops over the input_list until this amount of time has transpired.
--dbglogs
--timeout=&lt;val&gt;     Execution terminated when exceeding time limit (in microseconds). Only valid for HTP (dsp v68+) runtime.
--userlogs=&lt;val&gt;    Specifies the user level logging as level,&lt;optional logPath&gt;.
                    Valid values are: &quot;warn&quot;, &quot;verbose&quot;, &quot;info&quot;, &quot;error&quot;, &quot;fatal&quot;
--cache_compatibility_mode=&lt;val&gt;
                    Specifies the cache compatibility check mode; valid values are: &quot;permissive&quot; (default), &quot;strict&quot;, and &quot;always_generate_new&quot;.
                    Only valid for HTP (dsp v68+) runtime.
--validate_cache    Perform an additional validation step just before building SNPE to check the validity of the selected cache record in the DLC.
                    Upon success, app will proceed as usual. On validation failure, the app will report the validation error before exiting.
--graph_init=&lt;val&gt;
                    List of comma seperated graphs in the current DLC that is set to be inited.
--graph_execute=&lt;val&gt;
                    List of comma seperated graphs in the current DLC that is set to be executed.
--help              Show this help message.
--version           Show SDK Version Number.
</pre></div>
</div>
<div class="line-block">
<div class="line">This binary outputs raw output tensors into the output folder
by default. Examples of using snpe-net-run can be found in
<a class="reference external" href="tutorial_inceptionv3.html">Running the Inception v3 Model</a> tutorial.</div>
<div class="line">Additional details:</div>
</div>
<ul>
<li><p><em>Running batched inputs:</em></p>
<blockquote>
<div><ul>
<li><p>snpe-net-run is able to automatically batch the input
data. The batch size is indicated in the model container
(DLC file) but can also be set using the
“input_dimensions” argument passed to snpe-net-run. Users
do not need to batch their input data. If the input data
is not batch, the input size needs to be a multiple of
the size of the input data files. snpe-net-run would
group the provided inputs into batches and pad the
incomplete batches (if present) with zeros.</p>
<p>In the example below, the model is set to accept batches
of three inputs. So, the inputs are automatically grouped
together to form batches by snpe-net-run and padding is
done to the final batch. Note that there are five output
files generated by snpe-net-run:</p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span> …
Processing DNN input(s):
cropped/notice_sign.raw
cropped/trash_bin.raw
cropped/plastic_cup.raw
Processing DNN input(s):
cropped/handicap_sign.raw
cropped/chairs.raw
Applying padding
</pre></div>
</div>
</li>
</ul>
</div></blockquote>
</li>
<li><p><em>input_list argument:</em></p>
<blockquote>
<div><ul>
<li><p>snpe-net-run can take multiple input files as input data
per iteration, and specify multiple output names, in an
input list file formated as below:</p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>#&lt;output_name&gt;[&lt;space&gt;&lt;output_name&gt;]
&lt;input_layer_name&gt;:=&lt;input_layer_path&gt;[&lt;space&gt;&lt;input_layer_name&gt;:=&lt;input_layer_path&gt;]
…
</pre></div>
</div>
<p>The first line starting with a “#” specifies the output
layers’ names. If there is more than one output, a
whitespace should be used as a delimiter. Following the
first line, you can use multiple lines to supply input
files, one line per iteration, and each line only supply
one layer. If there is more than one input per line, a
whitespace should be used as a delimiter.</p>
<p>Here is an example, where the layer names are “Input_1”
and “Input_2”, and inputs are located in the path
“Placeholder_1/real_input_inputs_1/”. Its input list file
should look like this:</p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>#Output_1 Output_2
Input_1:=Placeholder_1/real_input_inputs_1/0-0#e6fb51.rawtensor Input_2:=Placeholder_1/real_input_inputs_1/0-1#8a171b.rawtensor
Input_1:=Placeholder_1/real_input_inputs_1/1-0#67c965.rawtensor Input_2:=Placeholder_1/real_input_inputs_1/1-1#54f1ff.rawtensor
Input_1:=Placeholder_1/real_input_inputs_1/2-0#b42dc6.rawtensor Input_2:=Placeholder_1/real_input_inputs_1/2-1#346a0e.rawtensor
</pre></div>
</div>
<p>Similar to above a first line starting with “%” specifies the output tensor names</p>
<p><strong>Note:</strong> If the batch dimension of the model is greater
than 1, the number of batch elements in the input file
has to either match the batch dimension specified in the
DLC or it has to be one. In the latter case, snpe-net-run
will combine multiple lines into a single input tensor.</p>
</li>
</ul>
</div></blockquote>
</li>
<li><p><em>Running AIP Runtime:</em></p>
<blockquote>
<div><ul class="simple">
<li><p>AIP Runtime requires a DLC which was quantized, and HTA
sections were generated offline.</p></li>
<li><p>AIP Runtime does not support debug_mode</p></li>
<li><p>AIP Runtime requires a DLC with all the layers
partitioned to HTA to support batched inputs</p></li>
</ul>
</div></blockquote>
</li>
<li><p><em>Set cache compatibility mode:</em></p>
<blockquote>
<div><ul>
<li><p>A DLC can include more than one cache record and users can set
the compatibility mode to check whether the best cache record is
optimal for the device. The available modes indicate binary cache
compatibility as follows.</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>permissive – Compatible if it could run on the device.</p></li>
<li><p>strict – Compatible if it could run on the device and
fully utilize hardware capability.</p></li>
<li><p>always_generate_new – Always incompatible; SNPE will
generate a new cache.</p></li>
</ol>
</div></blockquote>
</li>
</ul>
</div></blockquote>
</li>
</ul>
<div class="line-block">
<div class="line"><br /></div>
</div>
</div>
<div class="section" id="snpe-parallel-run">
<h3>snpe-parallel-run<a class="headerlink" href="#snpe-parallel-run" title="Permalink to this heading">¶</a></h3>
<p>snpe-parallel-run loads a DLC file, loads the data for the
input tensor(s), and executes the network on the specified
runtime. This app is similar to snpe-net-run, but is able to
run multiple threads of inference on the same network for
benchmarking purposes.</p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>DESCRIPTION:
------------
Tool that loads and executes one or more neural networks on different threads with optional asynchronous input/output processing using SDK APIs.


REQUIRED ARGUMENTS:
-------------------
--container  &lt;FILE&gt;   Path to the DL container containing the network.
--input_list &lt;FILE&gt;   Path to a file listing the inputs for the network.
--perf_profile &lt;VAL&gt;
                    Specifies perf profile to set. Valid settings are &quot;balanced&quot; , &quot;default&quot; , &quot;high_performance&quot; , &quot;sustained_high_performance&quot; , &quot;burst&quot; , &quot;power_saver&quot;, &quot;low_power_saver&quot;, &quot;high_power_saver&quot;, &quot;extreme_power_saver&quot;, &quot;low_balanced&quot;, and &quot;system_settings&quot;.
                    NOTE: &quot;balanced&quot; and &quot;default&quot; are the same.  &quot;default&quot; is being deprecated in the future.
--cpu_fallback        Enables cpu fallback functionality. Valid settings are &quot;false&quot;, &quot;true&quot;.
--runtime_order &lt;VAL,VAL,VAL,..&gt;
                    Specifies the order of precedence for runtime e.g cpu,gpu etc. Valid values are:-
                                cpu_float32 (Snapdragon CPU)       = Data &amp; Math: float 32bit
                                gpu_float32_16_hybrid (Adreno GPU) = Data: float 16bit Math: float 32bit
                                dsp_fixed8_tf (Hexagon DSP)        = Data &amp; Math: 8bit fixed point Tensorflow style format
                                gpu_float16 (Adreno GPU)           = Data: float 16bit Math: float 16bit
                                aip_fixed8_tf (Snapdragon HTA+HVX) = Data &amp; Math: 8bit fixed point Tensorflow style format
                                cpu (Snapdragon CPU)               = Same as cpu_float32
                                gpu (Adreno GPU)                   = Same as gpu_float32_16_hybrid
                                dsp (Hexagon DSP)                  = Same as dsp_fixed8_tf
                                aip (Snapdragon HTA+HVX)           = Same as aip_fixed8_tf
--use_cpu             Use the CPU runtime.
--use_gpu             Use the GPU float32 runtime.
--use_gpu_fp16        Use the GPU float16 runtime.
--use_dsp             Use the DSP fixed point runtime.
--use_aip             Use the AIP fixed point runtime.


OPTIONAL ARGUMENTS:
-------------------
--userbuffer_float    Specifies to use userbuffer for inference, and the input type is float.
--userbuffer_tf8      Specifies to use userbuffer for inference, and the input type is tf8exact0.
--userbuffer_auto     Specifies to use userbuffer with automatic input and output type detection for inference.
--use_native_input_files
                    Specifies to consume the input file(s) in their native data type(s).
                    Must be used with --userbuffer_xxx.
--use_native_output_files
                    Specifies to write the output file(s) in their native data type(s).
                    Must be used with --userbuffer_xxx.
--input_name &lt;INPUT_NAME&gt;
                    Specifies the name of input for which dimensions are specified.
--input_dimensions &lt;INPUT_DIM&gt;
                    Specifies new dimensions for input whose name is specified in input_name. e.g. &quot;1,224,224,3&quot;.
--output_dir &lt;DIR&gt;    The directory to save result files
--static_min_max      Specifies to use quantization parameters from the model instead of
                    input specific quantization. Used in conjunction with --userbuffer_tf8.
--userbuffer_float_output
                    Overrides the userbuffer output used for inference, and the output type is float.
                    Must be used with user specified buffer.
--userbuffer_tf8_output
                    Overrides the userbuffer output used for inference, and the output type is tf8exact0.
                    Must be used with user specified buffer.
--userbuffer_memorymapped
                    Specifies to use memory-mapped (zero-copy) user buffer. Must be used with --userbuffer_float or
                    --userbuffer_tf8 or userbuffer_tfN or userbuffer_auto etc. Cannot be combined with --encoding_type.
                    Note: Passing this option will turn all input and output userbuffers into memory mapped buffer.
--userbuffer_memorymapped_shared
                    Specifies to use memory-mapped (zero-copy) user buffer with shared memory chunk.
                    Must be used with --userbuffer_float or --userbuffer_tf8 or userbuffer_tfN or
                    userbuffer_auto etc. Cannot be combined with --encoding_type or --userbuffer_memorymapped.
                    Note: Passing this option will turn all input and output userbuffers into memory mapped buffer
--enable_init_cache   Enable init caching mode to accelerate the network building process. Defaults to disable.
--profiling_level     Specifies the profiling level.  Valid settings are &quot;off&quot;, &quot;basic&quot;, &quot;moderate&quot;, &quot;detailed&quot;, and &quot;linting&quot;.
                      Default is off.
--platform_options    Specifies value to pass as platform options.  Valid settings: &quot;HtaDLBC:ON/OFF&quot;, &quot;unsignedPD:ON/OFF&quot;.
--platform_options_local
                      Specifies the value to pass for the current SNPE instance. Valid settings: &quot;HtpDLBC:ON/OFF&quot;, &quot;HtaDLBC:ON/OFF;HtpDLBC:ON/OFF&quot;.
--set_output_tensors  Specifies a comma separated list of tensors to be output after execution.
--userlogs &lt;VAL&gt;      Specifies the user level logging as level,&lt;optional logPath&gt;.
--version             Show SDK Version Number.
--help                Show this help message.
</pre></div>
</div>
<div class="line-block">
<div class="line">Additional details:</div>
</div>
<ul>
<li><p><em>Required runtime argument:</em></p>
<blockquote>
<div><ul class="simple">
<li><p>For the required arguments pertaining to runtime
specification, either –runtime_order OR –use_cpu OR –use_gpu etc.
needs to be specified. The following example demonstrates
an equivalent command using either of these options.</p></li>
</ul>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>snpe-parallel-run --container container.dlc --input_list input_list.txt
--perf_profile burst --cpu_fallback true --use_dsp --use_gpu --userbuffer_auto
</pre></div>
</div>
<p>is equivalent to</p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>snpe-parallel-run --container container.dlc --input_list input_list.txt
--perf_profile burst --cpu_fallback true --runtime_order dsp,gpu --userbuffer_auto
</pre></div>
</div>
</div></blockquote>
</li>
<li><p><em>Spawning multiple threads:</em></p>
<blockquote>
<div><ul class="simple">
<li><p>snpe-parallel-run is able to create multiple threads to
execute identical inference passes.</p></li>
</ul>
</div></blockquote>
</li>
</ul>
<p>In the example below, the given command has the required
arguments for container and input list given. After these
2 options, the remaining options form a repeating
sequence that corresponds to each thread. In this
example, we have varied the runtimes specified for each
thread (one for dsp, another for gpu, and the last one
for dsp).</p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>snpe-parallel-run --container container.dlc --input_list input_list.txt
--perf_profile burst --cpu_fallback true --use_dsp --userbuffer_auto
--perf_profile burst --cpu_fallback true --use_gpu --userbuffer_auto
--perf_profile burst --cpu_fallback true --use_dsp --userbuffer_auto
</pre></div>
</div>
<p>When this command is executed, the following section of
output is observed:</p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>...
Processing DNN input(s):
input.raw
PSNPE start executing...
runtimes: dsp_fixed8_tf gpu_float32_16_hybrid dsp_fixed8_tf - Mode :0- Number of images processed: x
    Build time: x seconds.
...
</pre></div>
</div>
<p>Note that the number of runtimes listed corresponds to
the number of threads specified, as well as the order in
which those threads were specified.</p>
<div class="line-block">
<div class="line"><br /></div>
</div>
</div>
<div class="section" id="snpe-throughput-net-run">
<h3>snpe-throughput-net-run<a class="headerlink" href="#snpe-throughput-net-run" title="Permalink to this heading">¶</a></h3>
<p>snpe-throughput-net-run concurrently runs multiple instances of
SNPE for a certain duration of time and measures inference
throughput. Each instance of SNPE can have its own model,
designated runtime and performance profile. Please note that
the <cite>–duration</cite> parameter is common for all instances of SNPE
created.</p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>DESCRIPTION:
------------
Tool to load and execute concurrent SNPE objects using the SDK API.


REQUIRED ARGUMENTS:
-------------------
    --container  &lt;FILE&gt;              Path to the DL container containing the network.
    --duration   &lt;VAL&gt;               Duration of time (in seconds) to run network execution.
    --use_cpu                        Use the CPU runtime.
    --use_gpu                        Use the GPU float32 runtime.
    --use_gpu_fp16                   Use the GPU float16 runtime.
    --use_dsp                        Use the DSP fixed point runtime.
    --use_aip                        Use the AIP fixed point runtime.
    --perf_profile &lt;VAL&gt;             Specifies perf profile to set. Valid settings are &quot;balanced&quot; , &quot;default&quot; , &quot;high_performance&quot; ,
                                     &quot;sustained_high_performance&quot; , &quot;burst&quot; , &quot;power_saver&quot;, &quot;low_power_saver&quot;, &quot;high_power_saver&quot;, &quot;extreme_power_saver&quot;, &quot;low_balanced&quot;, and &quot;system_settings&quot;.
                                     NOTE: &quot;balanced&quot; and &quot;default&quot; are the same.  &quot;default&quot; is being deprecated in the future.
    --runtime_order &lt;VAL,VAL,VAL,..&gt; Specifies the order of precedence for runtime e.g cpu,gpu etc. Valid values are:-
                                     cpu_float32 (Snapdragon CPU)       = Data &amp; Math: float 32bit
                                     gpu_float32_16_hybrid (Adreno GPU) = Data: float 16bit Math: float 32bit
                                     dsp_fixed8_tf (Hexagon DSP)        = Data &amp; Math: 8bit fixed point Tensorflow style format
                                     gpu_float16 (Adreno GPU)           = Data: float 16bit Math: float 16bit
                                     aip_fixed8_tf (Snapdragon HTA+HVX) = Data &amp; Math: 8bit fixed point Tensorflow style format
                                     cpu (Snapdragon CPU)               = Same as cpu_float32
                                     gpu (Adreno GPU)                   = Same as gpu_float32_16_hybrid
                                     dsp (Hexagon DSP)                  = Same as dsp_fixed8_tf
                                     aip (Snapdragon HTA+HVX)           = Same as aip_fixed8_tf

OPTIONAL ARGUMENTS:
-------------------
    --debug                             Specifies that output from all layers of the network
                                        will be saved.
    --userbuffer_auto                   Specifies to use userbuffer for input and output, with auto detection of types enabled.
                                        Must be used with user specified buffer.
    --userbuffer_float                  Specifies to use userbuffer for inference, and the input type is float.
                                        Must be used with user specified buffer.
    --userbuffer_floatN                 Specifies to use userbuffer for inference, and the input type is float16 or float32.
                                        Must be used with user specified buffer.
    --userbuffer_tf8                    Specifies to use userbuffer for inference, and the input type is tf8exact0.
                                        Must be used with user specified buffer.
    --userbuffer_tfN                    Specifies to use userbuffer for inference, and the input type is tf8exact0 or tf16exact0.
                                        Must be used with user specified buffer.
    --userbuffer_memorymapped           Specifies to use memory-mapped (zero-copy) user buffer. Must be used with --userbuffer_float or
                                        --userbuffer_tf8 or userbuffer_tfN or userbuffer_auto etc. Cannot be combined with --encoding_type.
                                        Note: Passing this option will turn all input and output userbuffers into memory mapped buffer.
    --userbuffer_float_output           Overrides the userbuffer output used for inference, and the output type is float.
                                        Must be used with user specified buffer.
    --userbuffer_floatN_output          Overrides the userbuffer output used for inference, and the output type is float16 or float32.
                                        Must be used with user specified buffer.
    --userbuffer_tf8_output             Overrides the userbuffer output used for inference, and the output type is tf8exact0.
                                        Must be used with user specified buffer.
    --userbuffer_tfN_output             Overrides the userbuffer output used for inference, and the output type is tf8exact0 or tf16exact0.
                                        Must be used with user specified buffer.
    --storage_dir &lt;DIR&gt;                 The directory to store metadata files
    --version                           Show SDK Version Number.
    --iterations &lt;VAL&gt;                  Number of times to iterate through entire input list
    --verbose                           Print more debug information.
    --skip_execute                      Don&#39;t do execution (just graph build/teardown)
    --enable_cpu_fallback               Enables cpu fallback functionality. Defaults to disable mode.
    --json  &lt;FILE&gt;                      Generated JSON report.
    --input_raw &lt;FILE&gt;                  Path to raw inputs for the network, seperated by &quot;,&quot;.
    --fixed_fps &lt;VAL&gt;                   Fix fps so as to control system loading, total FPS will be limited to around &lt;VAL&gt; Ex: 30,20,0(free run)
    --udo_package_path &lt;VAL,VAL&gt;        Path to UDO package with registration library for UDOs.
                                        Optionally, user can provide multiple packages as a comma-separated list.
    --enable_init_cache                 Enable init caching mode to accelerate the network building process. Defaults to disable.
    --platform_options &lt;VAL&gt;            Specifies value to pass as platform options for all SNPE instances.
    --platform_options_local &lt;VAL&gt;      Specifies the value to pass as per SNPE instance platform options for the current SNPE instance.
                                        if --platform_options is specified then it overwrites the global platform options for the current SNPE instance.
    --priority_hint &lt;VAL&gt;               Specifies hint for priority level. Valid settings are &quot;low&quot;, &quot;normal&quot;, &quot;normal_high&quot;, &quot;high&quot;. Defaults to normal.
                                        Note: &quot;normal_high&quot; is only available on DSP.
    --groupDuration &lt;VAL&gt;               Duration (in ms) of execution before next sleep.(Optional)
    --groupSleep &lt;VAL&gt;                  Sleep interval (in ms) after execution of group.(Optional)
    --set_output_layers &lt;VAL&gt;           Optionally, user can provide a comma separated list of layers to be output after execution.
                                        If using multi-graph DLC, provide &lt;graph name&gt; &lt;comma separated layers&gt; in double quotes.
                                        It should be defined for all instances or none at all.
                                        Use empty string for instances that doesn&#39;t need any layer outputs.
                                        e.g --set_output_layers &quot;graphA layer1,layer2,layer3&quot;
    --set_output_tensors &lt;VAL&gt;          Optionally, user can provide a comma separated list of tensors to be output after execution.
                                        If using multi-graph DLC, provide &lt;graph name&gt; &lt;comma separated tensors&gt; in double quotes.
                                        It should be defined for all instances or none at all.
                                        Use empty string for instances that doesn&#39;t need any layer outputs.
                                        e.g --set_output_tensors &quot;graphA tensor1,tensor2,tensor3&quot;
    --userlogs=&lt;val&gt;                    Specifies the user level logging as level,&lt;optional logPath&gt;.
                                        Valid values are: &quot;warn&quot;, &quot;verbose&quot;, &quot;info&quot;, &quot;error&quot;, &quot;fatal&quot;
    --enable_cpu_fxp                    Enable the fixed point execution on CPU runtime.
    --model_name &lt;VAL&gt;                  To add the model name to the logs (Optional)
    --cache_compatibility_mode=&lt;val&gt;    Specifies the cache compatibility check mode; valid values are: &quot;permissive&quot; (default), &quot;strict&quot;, and &quot;always_generate_new&quot;.
                                        Only valid for HTP (dsp v68+) runtime.
    --validate_cache                    Perform an additional validation step just before building SNPE to check the validity of the selected cache record in the DLC.
                                        Upon success, app will proceed as usual. On validation failure, the app will report the validation error before exiting.
    --graph_init                        Optionally, Specifies a comma separated list of specified graphs in the current DLC that is set to be inited.
                                        e.g --graph_init graph1, graph2, graph3
    --graph_execute                     Optionally, Specifies a comma separated list of specified graphs in the current DLC that is set to be executed.
                                        e.g --graph_execute graph1, graph2, graph3
    --help                              Show this help message.
</pre></div>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
</div>
</div>
<div class="section" id="analysis">
<h2>Analysis<a class="headerlink" href="#analysis" title="Permalink to this heading">¶</a></h2>
<div class="section" id="snpe-diagview">
<h3>snpe-diagview<a class="headerlink" href="#snpe-diagview" title="Permalink to this heading">¶</a></h3>
<p>snpe-diagview loads a DiagLog file generated by snpe-net-run
whenever it operates on input tensor data. The DiagLog file
contains timing information information for each layer as well
as the entire forward propagate time. If the run uses an input
list of input tensors, the timing info reported by
snpe-diagview is an average over the entire input set.</p>
<p>The snpe-net-run generates a file called “SNPEDiag_0.log”,
“SNPEDiag_1.log” … , “SNPEDiag_n.log”, where n corresponds to
the nth iteration of the snpe-net-run execution.</p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>usage: snpe-diagview --input_log DIAG_LOG [-h] [--output CSV_FILE]

Reads a diagnostic log and output the contents to stdout

required arguments:
    --input_log     DIAG_LOG
                Diagnostic log file (required)
optional arguments:
    --output        CSV_FILE
                Output CSV file with all diagnostic data (optional)

    --chrometrace   CHROMETRACE_FILE
                Output chrometrace JSON filename (w/out extension) for logs made using profiling level linting (optional)
</pre></div>
</div>
<p>The output generated has timings collected at different layers of the stack. Below is the description of the timing markers:</p>
<p>(Please note: Certain backends like DSP or GPU sometimes splits an op into multiple ops in the backend. Sometimes the backends fuse multiple ops into one. Hence a detailed profiling log can display mismatching no of ops and layer mapping.)</p>
<div class="docutils container">
<div class="figure align-default">
<img alt="../images/snpe_diagview_timing_diagram.png" src="../images/snpe_diagview_timing_diagram.png" />
</div>
</div>
</div>
<div class="section" id="snpe-dlc-diff">
<h3>snpe-dlc-diff<a class="headerlink" href="#snpe-dlc-diff" title="Permalink to this heading">¶</a></h3>
<p>snpe-dlc-diff compares two DLCs and by default outputs some of
the following differences in them in a tabular format:</p>
<ul class="simple">
<li><p>unique layers between the two DLCs</p></li>
<li><p>parameter differences in common layers</p></li>
<li><p>differences in dimensions of buffers associated with common
layers</p></li>
<li><p>weight differences in common layers</p></li>
<li><p>output tensor names differences in common layers</p></li>
<li><p>unique records between the two DLCs (currently checks for
AIP records only)</p></li>
</ul>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>usage: snpe-dlc-diff [-h] -i1 INPUT_DLC_ONE -i2 INPUT_DLC_TWO [-c] [-l] [-p]
                        [-d] [-w] [-o] [-i] [-x] [-s SAVE]

required arguments:
    -i1 INPUT_DLC_ONE, --input_dlc_one INPUT_DLC_ONE
                        path to the first dl container archive
    -i2 INPUT_DLC_TWO, --input_dlc_two INPUT_DLC_TWO
                        path to the second dl container archive

optional arguments:
    -h, --help            show this help message and exit
    -c, --copyrights      compare copyrights between models
    -l, --layers          compare unique layers between models
    -p, --parameters      compare parameter differences between identically
                        named layers
    -d, --dimensions      compare dimension differences between identically
                        named layers
    -w, --weights         compare weight differences between identically named
                        layers.
    -o, --outputs         compare output_tensor name differences names between
                        identically named layers
    -i, --diff_by_id      Overrides the default comparison strategy for diffing
                        2 models components. By default comparison is made
                        between identically named layers. With this option the
                        models are ordered by id and diff is done in order as
                        long as no more than 1 consecutive layers have
                        different layer types.
    -x, --hta             compare HTA records differences in Models
    -s SAVE, --save SAVE  Save the output to a csv file. Specify a target file
                        path.
</pre></div>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
</div>
<div class="section" id="snpe-dlc-info">
<h3>snpe-dlc-info<a class="headerlink" href="#snpe-dlc-info" title="Permalink to this heading">¶</a></h3>
<p>snpe-dlc-info outputs layer information from a DLC file, which
provides information about the network model.</p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>usage: snpe-dlc-info [-h] -i INPUT_DLC [-s SAVE]

required arguments:
    -i INPUT_DLC, --input_dlc INPUT_DLC
                        path to a DLC file

optional arguments:
    -s SAVE, --save SAVE
                        Save the output to a csv file. Specify a target file path.
</pre></div>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
</div>
<div class="section" id="snpe-dlc-viewer">
<h3>snpe-dlc-viewer<a class="headerlink" href="#snpe-dlc-viewer" title="Permalink to this heading">¶</a></h3>
<p>snpe-dlc-viewer visualizes the network structure of a DLC in a
web browser.</p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>usage: snpe-dlc-viewer [-h] -i INPUT_DLC [-s]

required arguments:
    -i INPUT_DLC, --input_dlc INPUT_DLC
                        Path to a DLC file

optional arguments:
    -s, --save            Save HTML file. Specify a file name and/or target save path
    -h, --help            Shows this help message and exits
</pre></div>
</div>
<p>Additional details:</p>
<p>The DLC viewer tool renders the specified network DLC in HTML
format that may be viewed on a web browser. On installations
that support a native web browser, a browser instance is opened
on which the network is automatically rendered. Users can
optionally save the HTML content anywhere on their systems
and open on a chosen web browser independently at a later time.</p>
<ul>
<li><p>Features:</p>
<blockquote>
<div><ul class="simple">
<li><p>Graph-based representation of network model with nodes
depicting layers and edges depicting buffer connections.</p></li>
<li><p>Colored legend to indicate layer types.</p></li>
<li><p>Zoom and drag options available for ease of
visualization.</p></li>
<li><p>Tool-tips upon mouse hover to describe detailed layer
parameters.</p></li>
<li><p>Sections showing metadata from DLC records</p></li>
</ul>
</div></blockquote>
</li>
<li><p>Supported browsers:</p>
<blockquote>
<div><ul class="simple">
<li><p>Google Chrome</p></li>
<li><p>Firefox</p></li>
<li><p>Internet Explorer on Windows</p></li>
<li><p>Microsoft Edge Browser on Windows</p></li>
<li><p>Safari on Mac</p></li>
</ul>
</div></blockquote>
</li>
</ul>
<div class="line-block">
<div class="line"><br /></div>
</div>
</div>
<div class="section" id="snpe-platform-validator">
<h3>snpe-platform-validator<a class="headerlink" href="#snpe-platform-validator" title="Permalink to this heading">¶</a></h3>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>DESCRIPTION:
------------
snpe-platform-validator is a tool to check the capabilities of a device. This tool runs on the device,
rather than on the host, and requires a few additional files to be pushed to the device besides its own executable.
Additional details below.


REQUIRED ARGUMENTS:
-------------------
    --runtime &lt;RUNTIME&gt;   Specify the runtime to validate. &lt;RUNTIME&gt; : gpu, dsp, aip, all.

OPTIONAL ARGUMENTS:
-------------------
    --coreVersion         Query the runtime core descriptor.
    --libVersion          Query the runtime core library API.
    --testRuntime         Run diagnostic tests on the specified runtime.
    --targetPath &lt;DIR&gt;    The directory to save output on the device. Defaults to /data/local/tmp/platformValidator/output.
    --debug               Turn on verbose logging.
    --help                Show this help message.
</pre></div>
</div>
<div class="line-block">
<div class="line">Additional details:</div>
</div>
<ul>
<li><p><em>Files needed to be pushed to device (Please note, we have to push the
Stub.so and Skel.so of appropriate DSP architecture version, e.g., v68, v73) :</em></p>
<blockquote>
<div><div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>// Android
bin/aarch64-android/snpe-platform-validator
lib/aarch64-android/libSnpeHtpV73CalculatorStub.so
lib/aarch64-android/libSnpeHtpV73Stub.so
lib/hexagon-${DSP_ARCH}/unsigned/libCalculator_skel.so
lib/hexagon-${DSP_ARCH}/unsigned/libSnpeHtpV73Skel.so

// Windows
bin/aarch64-windows-msvc/snpe-platform-validator.exe
lib/aarch64-windows-msvc/calculator_htp.dll
lib/aarch64-windows-msvc/SnpeHtpV73Stub.dll
lib/hexagon-${DSP_ARCH}/unsigned/libCalculator_skel.so
lib/hexagon-${DSP_ARCH}/unsigned/libSnpeHtpV73Skel.so
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>example: for pushing aarch64-android variant to /data/local/tmp/platformValidator</p>
<blockquote>
<div><div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>adb push $SNPE_ROOT/bin/aarch64-android/snpe-platform-validator /data/local/tmp/platformValidator/bin/snpe-platform-validator
adb push $SNPE_ROOT/lib/aarch64-android/ /data/local/tmp/platformValidator/lib
adb push $SNPE_ROOT/lib/dsp /data/local/tmp/platformValidator/dsp
</pre></div>
</div>
</div></blockquote>
</li>
</ul>
<div class="line-block">
<div class="line"><br /></div>
</div>
</div>
<div class="section" id="snpe-platform-validator-py">
<h3>snpe-platform-validator-py<a class="headerlink" href="#snpe-platform-validator-py" title="Permalink to this heading">¶</a></h3>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>DESCRIPTION:
------------
snpe-platform-validator is a tool to check the capabilities of a device. The output is saved in a CSV file in the
&quot;Output&quot; directory, in a csv format. Basic logs are also displayed on the console.

REQUIRED ARGUMENTS:
-------------------
    --runtime &lt;RUNTIME&gt;      Specify the runtime to validate. &lt;RUNTIME&gt; : gpu, dsp, aip, all.
    --directory &lt;ARTIFACTS&gt;  Path to the root of the unpacked SDK directory containing the executable and library files.

OPTIONAL ARGUMENTS:
-------------------
    --buildVariant &lt;VARIANT&gt;      Specify the build variant (e.g: aarch64-android) to be validated.
    --deviceId                    Uses the device for running the adb command. Defaults to first device in the adb devices list.
    --coreVersion                 Outputs the version of the runtime that is present on the target.
    --libVersion                  Outputs the library version of the runtime that is present on the target.
    --testRuntime                 Run diagnostic tests on the specified runtime.
    --targetPath &lt;PATH&gt;           The path to be used on the device. Defaults to /data/local/tmp/platformValidator
                                NOTE that this directory will be deleted before proceeding with validation.
    --remoteHost &lt;REMOTEHOST&gt;     Run on remote host through remote adb server. Defaults to localhost.
    --debug                       Set to turn on debug log.
</pre></div>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
</div>
<div class="section" id="snpe-bench-py">
<h3>snpe_bench.py<a class="headerlink" href="#snpe-bench-py" title="Permalink to this heading">¶</a></h3>
<p>python script snpe_bench.py runs a DLC neural network and
collects benchmark performance information.</p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>usage: snpe_bench.py [-h] -c CONFIG_FILE [-o OUTPUT_BASE_DIR_OVERRIDE]
                        [-v DEVICE_ID_OVERRIDE] [-r HOST_NAME] [-a]
                        [-t DEVICE_OS_TYPE_OVERRIDE] [-d] [-s SLEEP]
                        [-b USERBUFFER_MODE] [-p PERFPROFILE] [-l PROFILINGLEVEL]
                        [-json] [-cache]

Run the snpe_bench

required arguments:
    -c CONFIG_FILE, --config_file CONFIG_FILE
                        Path to a valid config file
                        Refer to sample config file config_help.json for more
                        detail on how to fill params in config file

optional arguments:
    -o OUTPUT_BASE_DIR_OVERRIDE, --output_base_dir_override OUTPUT_BASE_DIR_OVERRIDE
                        Sets the output base directory.
    -v DEVICE_ID_OVERRIDE, --device_id_override DEVICE_ID_OVERRIDE
                        Use this device ID instead of the one supplied in config
                        file. Cannot be used with -a
    -r HOST_NAME, --host_name HOST_NAME
                        Hostname/IP of remote machine to which devices are
                        connected.
    -a, --run_on_all_connected_devices_override
                        Runs on all connected devices, currently only support 1.
                        Cannot be used with -v
    -t DEVICE_OS_TYPE_OVERRIDE, --device_os_type_override DEVICE_OS_TYPE_OVERRIDE
                        Specify the target OS type, valid options are
                        [&#39;android-aarch64&#39;, &#39;le&#39;, &#39;le64_gcc4.9&#39;,
                        &#39;le_oe_gcc6.4&#39;, &#39;le64_oe_gcc6.4&#39;]
    -d, --debug           Set to turn on debug log
    -s SLEEP, --sleep SLEEP
                        Set number of seconds to sleep between runs e.g. 20
                        seconds
    -b USERBUFFER_MODE, --userbuffer_mode USERBUFFER_MODE
                        [EXPERIMENTAL] Enable user buffer mode, default to
                        float, can be tf8exact0
    -p PERFPROFILE, --perfprofile PERFPROFILE
                        Set the benchmark operating mode (balanced, default,
                        sustained_high_performance, high_performance,
                        power_saver, low_power_saver, high_power_saver,
                        extreme_power_saver, low_balanced, system_settings)
    -l PROFILINGLEVEL, --profilinglevel PROFILINGLEVEL
                        Set the profiling level mode (off, basic, moderate, detailed).
                        Default is basic.
    -json, --generate_json
                        Set to produce json output.
    -cache, --enable_init_cache
                        Enable init caching mode to accelerate the network
                        building process. Defaults to disable.
</pre></div>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="usergroup11.html" class="btn btn-neutral float-right" title="Debug Tools" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="accuracy.html" class="btn btn-neutral float-left" title="Inference Accuracy" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020-2023, Qualcomm Technologies, Inc..

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>