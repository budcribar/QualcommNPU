

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Using MobilenetSSD &mdash; Snapdragon Neural Processing Engine SDK</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/custom_css.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Using DeepLabv3" href="convert_deeplabv3.html" />
    <link rel="prev" title="Model Tips" href="usergroup4.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> Qualcomm® Neural Processing SDK
          

          
          </a>

          
            
            
              <div class="version">
                v2.22.6
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="revision_history.html">Revision History</a></li>
<li class="toctree-l1"><a class="reference internal" href="revision_history_windows.html">Revision History - Windows</a></li>
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="setup.html">Setup</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="usergroup1.html">Network Models</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="network_layers.html">Supported Network Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="supported_onnx_ops.html">Supported ONNX Ops</a></li>
<li class="toctree-l2"><a class="reference internal" href="quantized_models.html">Quantized vs Non-Quantized Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="usergroup2.html">User-defined Operations</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="usergroup3.html">Model Conversion</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="model_conv_tensorflow.html">TensorFlow Model Conversion</a></li>
<li class="toctree-l3"><a class="reference internal" href="tensorflow_graphs.html">Tensorflow Graph Compatibility</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_conv_tflite.html">TFLite Model Conversion</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_conv_pytorch.html">PyTorch Model Conversion</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_conv_onnx.html">ONNX Model Conversion</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_conversion.html">Quantizing a Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="offline_graph_caching.html">Offline Graph Caching for DSP Runtime on HTP</a></li>
<li class="toctree-l3"><a class="reference internal" href="qairt_converter.html">Qairt Converter</a></li>
<li class="toctree-l3"><a class="reference internal" href="qairt_quantizer.html">Qairt Quantizer</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="usergroup4.html">Model Tips</a><ul class="current">
<li class="toctree-l4 current"><a class="current reference internal" href="#">Using MobilenetSSD</a></li>
<li class="toctree-l4"><a class="reference internal" href="convert_deeplabv3.html">Using DeepLabv3</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="usergroup5.html">Input Data and Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="usergroup6.html">Tutorials and Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="usergroup10.html">Benchmarking and Accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="tools.html">Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="usergroup11.html">Debug Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="api.html">API</a></li>
<li class="toctree-l1"><a class="reference internal" href="limitations.html">Limitations</a></li>
<li class="toctree-l1"><a class="reference internal" href="appx_ref.html">References</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Qualcomm® Neural Processing SDK</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="usergroup1.html">Network Models</a> &raquo;</li>
        
          <li><a href="usergroup3.html">Model Conversion</a> &raquo;</li>
        
          <li><a href="usergroup4.html">Model Tips</a> &raquo;</li>
        
      <li>Using MobilenetSSD</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="using-mobilenetssd">
<h1>Using MobilenetSSD<a class="headerlink" href="#using-mobilenetssd" title="Permalink to this heading">¶</a></h1>
<div class="ui-resizable side-nav-resizable docutils container" id="side-nav">
<div class="docutils container" id="nav-tree">
<div class="docutils container" id="nav-tree-contents">
</div>
</div>
</div>
<div class="docutils container" id="doc-content">
<div class="header docutils container">
</div>
<div class="contents docutils container">
<div class="textblock docutils container">
<p class="rubric" id="tensorflow-mobilenetssd-model">Tensorflow MobilenetSSD model</p>
<p>Tensorflow Mobilenet SSD frozen graphs come in a couple of
flavors. The standard frozen graph and a quantization aware
frozen graph. The following example uses a quantization aware
frozen graph to ensure accurate results on the Qualcomm® Neural Processing SDK runtimes.</p>
<p><strong>Prerequisites</strong></p>
<p>The quantization aware model conversion process was tested
using Tensorflow v1.11 however other versions may also work.
The CPU version of Tensorflow was used to avoid out of memory
issues observed across various GPU cards during conversion.</p>
<p><strong>Setup the Tensorflow Object Detection Framework</strong></p>
<p>The quantization aware model is provided as a TFLite frozen
graph. However Qualcomm® Neural Processing SDK requires a Tensorflow frozen graph (.PB).
To convert the quantized model, the object detection framework
is used to export to a Tensorflow frozen graph. Follow these
steps to clone the object detection framework:</p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>mkdir ~/tfmodels
cd ~/tfmodels
git clone https://github.com/tensorflow/models.git
</pre></div>
</div>
<p>Checkout a tested object detection framework commit (SHA)</p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>git checkout ad386df597c069873ace235b931578671526ee00
</pre></div>
</div>
<p>Follow third party instructions to setup the Tensorflow
object detection framework</p>
<p><strong>Download the quantization aware model</strong></p>
<p>A specific version of the Tensorflow MobilenetSSD model has
been tested:
<strong>ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tar.gz</strong></p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>wget http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tar.gz
</pre></div>
</div>
<p>After downloading the model extract the contents to a
directory.</p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>tar xzvf ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tar.gz
</pre></div>
</div>
<p><strong>Export a trained graph from the object detection framework</strong></p>
<p>Follow these instructions to export the Tensorflow graph:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/exporting_models.md">https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/exporting_models.md</a></p></li>
</ul>
<p>or modify and execute this sample script</p>
<p>Create this file, export_train.sh, using your favorite editor.
Modify the paths to the correct directory location of the
downloaded quantization aware model files.</p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>#!/bin/bash
INPUT_TYPE=image_tensor
PIPELINE_CONFIG_PATH=&lt;path_to&gt;/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03/pipeline.config
TRAINED_CKPT_PREFIX=&lt;path_to&gt;/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03/model.ckpt
EXPORT_DIR=&lt;path_to&gt;/exported
pushd ~/tfmodels/models/tfmodels/research
python3 object_detection/export_inference_graph.py \
--input_type=${INPUT_TYPE} \
--pipeline_config_path=${PIPELINE_CONFIG_PATH} \
--trained_checkpoint_prefix=${TRAINED_CKPT_PREFIX} \
--output_directory=${EXPORT_DIR}
popd
</pre></div>
</div>
<p>Make the script executable</p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>chmod u+x export_train.sh
</pre></div>
</div>
<p>Run the script</p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>./export_train.sh
</pre></div>
</div>
<p>This should generate a frozen graph in
<code class="docutils literal notranslate"><span class="pre">&lt;path_to&gt;/exported/frozen_inference_graph.pb</span></code></p>
<p>Convert the frozen graph using the
<a class="reference external" href="tools.html#snpe-tensorflow-to-dlc">snpe-tensorflow-to-dlc</a>
converter.</p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>snpe-tensorflow-to-dlc --input_network &lt;path_to&gt;/exported/frozen_inference_graph.pb --input_dim Preprocessor/sub 1,300,300,3 --out_node detection_classes --out_node detection_boxes --out_node detection_scores ---output_path mobilenet_ssd.dlc --allow_unconsumed_nodes
</pre></div>
</div>
<p>After Qualcomm® Neural Processing SDK conversion you should have a mobilenet_ssd.dlc that
can be loaded and run in the Qualcomm® Neural Processing SDK runtimes.</p>
<p>The output layers for the model are:</p>
<ul class="simple">
<li><p>Postprocessor/BatchMultiClassNonMaxSuppression</p></li>
<li><p>add</p></li>
</ul>
<p>The output buffer names are:</p>
<ul class="simple">
<li><p>(classes) detection_classes:0 (+1 index offset)</p></li>
<li><p>(classes)
Postprocessor/BatchMultiClassNonMaxSuppression_classes (0
index offset)</p></li>
<li><p>(boxes) Postprocessor/BatchMultiClassNonMaxSuppression_boxes</p></li>
<li><p>(scores)
Postprocessor/BatchMultiClassNonMaxSuppression_scores</p></li>
</ul>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p><strong>Running the model in |Qualcomm(R)| Neural Processing SDK</strong></p>
<p>The following are limitations and suggestions for running DLC
model in Qualcomm® Neural Processing SDK:</p>
<ul class="simple">
<li><p>Batch dimension &gt; 1 is not supported.</p></li>
<li><p>DetectionOutput layer is supported on CPU runtime processor
only.
To run the model using different runtime processor, such as
GPU or DSP, CPU fallback mode must be enabled in Runtime
List (see
<span class="xref std std-ref">Snpe_SNPEBuilder_SetRuntimeProcessorOrder()</span>
description in Qualcomm® Neural Processing SDK API).
If using <a class="reference external" href="tools.html#snpe-net-run">snpe-net-run</a>
tool, use <code class="docutils literal notranslate"><span class="pre">–runtime_order</span></code> option</p></li>
<li><p>Configure DetectionOutput layer reasonably.
Performance of DetectionOutput layer (i.e. processing time)
is function of layer parameters: <code class="docutils literal notranslate"><span class="pre">top_k</span></code>, <code class="docutils literal notranslate"><span class="pre">keep_top_k</span></code>
and <code class="docutils literal notranslate"><span class="pre">confidence_threshold</span></code>.
For example, <code class="docutils literal notranslate"><span class="pre">top_k</span></code> parameters have practically
exponential impact on processing time; e.g. top_k=100 will
result in much smaller processing time vs. top_k=1000.
Smaller <code class="docutils literal notranslate"><span class="pre">confidence_threshold</span></code> will result in larger
number of boxes to output, and vice versa.</p></li>
<li><p>Resizing input dimensions at SNPE object creation/build time
is not allowed.
Note that input dimensions are embedded into DLC model
during conversion, but in some cases can be overridden via
<span class="xref std std-ref">Snpe_SNPEBuilder_SetInputDimensions()</span>
(see description in Qualcomm® Neural Processing SDK API) at SNPE object
creation/build time. Due to PriorBox layer folding in the
model converter, input/network resizing is not possible.</p></li>
</ul>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="convert_deeplabv3.html" class="btn btn-neutral float-right" title="Using DeepLabv3" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="usergroup4.html" class="btn btn-neutral float-left" title="Model Tips" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020-2023, Qualcomm Technologies, Inc..

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>