

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Revision History &mdash; Snapdragon Neural Processing Engine SDK</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/custom_css.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Revision History - Windows" href="revision_history_windows.html" />
    <link rel="prev" title="⌂" href="../index.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> Qualcomm® Neural Processing SDK
          

          
          </a>

          
            
            
              <div class="version">
                v2.22.6
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Revision History</a></li>
<li class="toctree-l1"><a class="reference internal" href="revision_history_windows.html">Revision History - Windows</a></li>
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="setup.html">Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="usergroup1.html">Network Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="usergroup5.html">Input Data and Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="usergroup6.html">Tutorials and Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="usergroup10.html">Benchmarking and Accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="tools.html">Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="usergroup11.html">Debug Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="api.html">API</a></li>
<li class="toctree-l1"><a class="reference internal" href="limitations.html">Limitations</a></li>
<li class="toctree-l1"><a class="reference internal" href="appx_ref.html">References</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Qualcomm® Neural Processing SDK</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Revision History</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="revision-history">
<h1>Revision History<a class="headerlink" href="#revision-history" title="Permalink to this heading">¶</a></h1>
<table class="docutils align-default">
<colgroup>
<col style="width: 7%" />
<col style="width: 9%" />
<col style="width: 84%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Version</p></th>
<th class="head"><p>Date</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>2.22.0</p></td>
<td><p>April 2024</p></td>
<td><ul class="simple">
<li><p>Tools: Converter: Adding reduction attribute as none in case of attribute is not available in original graph.</p></li>
<li><p>Tools: Converter: Div op support is added in Tensorflow converter.</p></li>
<li><p>Tool:snpe-accuracy-debugger: Enable windows x86 Native support for integrated Quant Checker for SNPE SDK.</p></li>
<li><p>Tools: Converters: Cleanup of old references for Binary Coarse op.</p></li>
<li><p>Core: Support 5D tensors for transpose fp16 op in HTP.</p></li>
<li><p>DSP: Introducing low level performance APIs for DSP thus enabling custom performance profile settings for init, inference,
de-init and from inference to inference. Also enabling the ability to overwrite partially a preset profile.</p></li>
<li><p>Tool:quantizer: Fixed issue observed when bias of conv op need to be per-channel quantized in mix-precision mode.</p></li>
<li><p>Tools: Converter: Fixed the small bug in onnx softmax translation.</p></li>
<li><p>Core: snpe-dlc-graph-prepare - Fixed a bug with multiple SoC prepare for certain networks.</p></li>
<li><p>API: Added new APIs to quantize and dequantize buffers - Snpe_Util_Convert_Float32ToTfN / IUserBufferFactory::Float32ToTfN,
Snpe_Util_Convert_TfNToFloat32 / TfNToFloat32. Please read the API docs for details on usage.</p></li>
<li><p>SDK: Updated Android NDK version to android-ndk-r26c for compiling SNPE/QNN SDK for Android based targets.</p></li>
<li><p>Tool: Converter: Onnx: Fix axis tracking issue for TransposeConv2d.</p></li>
<li><p>Core: Fixed multiple timestamps in logs in android logcat.</p></li>
<li><p>Tools: Pytorch &amp; TFlite Converter: Fix incorrect rounding behavior.</p></li>
<li><p>Tools: Converters: PyTorch: Added support for TransposeConv3d.</p></li>
<li><p>HTP: Fixed accuracy issue for the pattern: Batchnorm -&gt; Relu -&gt; Concat.</p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p>2.21.0</p></td>
<td><p>March 2024</p></td>
<td><ul class="simple">
<li><p>SNPE library migrated to use static libc++ for Android platform.</p></li>
<li><p>Tool: Quantizer: Added a new standalone qairt-quantizer tool equivalent to snpe-dlc-quant. This new tool takes a float DLC and
produce a Quantized or Mixed Precision DLC.</p></li>
<li><p>Tool:Converters: Added support for sparse tensors.</p></li>
<li><p>Core: Renamed DSP_v68 folder at &lt;SNPE_SDK&gt;/examples/SNPE/NativeCpp/UdoExample/&lt;OpName&gt;/src to HTP.</p></li>
<li><p>Tools: Converter: enable 16bits QuantizeLinear/DequantizerLinear in Onnx converter.</p></li>
<li><p>Tools: Converters: ONNX: Added support for ThresholdedRelu ONNX op.</p></li>
<li><p>Tools: Converter: Downcast to_type for Cast op from int64 to int32.</p></li>
<li><p>Tool: Converter: Added PyTorch ChannelShuffle support.</p></li>
<li><p>Tool: Converter: Added TFLite LOCAL_RESPONSE_NORMALIZATION support.</p></li>
<li><p>Tool: Converter: Onnx: Support FP16 model conversion.</p></li>
<li><p>Tools: Converter: TFLite: Fixed an encoding mismatch issue at input and output layers when converting a pre-quantized tflite network.</p></li>
<li><p>Tool:snpe-dlc-quantize: Fix the bug causing snpe-dlc-quantize to fail with multi-dot dlc filenames.</p></li>
<li><p>Core: Fixed Snpe_Util_SetSNPEStorageLocation/ SNPEFactory::setSNPEStorageLocation to not create duplicate kernel
repo file (GpuKernelRepo.pb) for GPU and delegate it to the backend (gpukernelcache.qti.aisw)</p></li>
<li><p>Core: Fixed resource release in burst and sustained_high_performance mode.</p></li>
<li><p>Tool: Converter: Onnx: Bug fixed to support different layout of axis in TopK.</p></li>
<li><p>Tool: Pytorch Converter: Fixed scalar indices issue for gather op.</p></li>
<li><p>Addressed SSR (SubSystem Restart) occurring during SNPE_Execute().</p></li>
<li><p>Tools: Converter: Added qairt-converter tool. This converter tool takes a Pytorch/Onnx/Tensorflow/TFLite network and
converts it to a DLC file representing the QNN graph format that can enable inference on Qualcomm AI IP/HW.
Please refer Documentation or AppNote for more details.</p></li>
</ul>
</td>
</tr>
<tr class="row-even"><td><p>2.20.0</p></td>
<td><p>February 2024</p></td>
<td><ul class="simple">
<li><p>Tools: Converters: Added Converter support for MaskedSoftmax Operator.</p></li>
<li><p>Core: Added new option –userbuffer_memorymapped_shared to snpe-parallel-run to enable sharing memory mapped user buffers.
allowing different tensors to register using the same address/file descriptor and a unique byte offset.</p></li>
<li><p>Tools:qnn-pytorch-converter: Enabled preserve_io feature.</p></li>
<li><p>Tools: Converters: support hardsigmoid in onnx converter.</p></li>
<li><p>Tools: snpe-dlc-info updated to display output tensors and unconsumed internal tensors in separate tables.</p></li>
<li><p>Tools: Added support for snpe-diagview tool for OE Linux GCC11.2 toolchain based targets.</p></li>
<li><p>Core : Added CPU INT8 support to Softmax UDO example.</p></li>
<li><p>Tools: qnn-pytorch-converter: add support of aten::upsample_linear1d for pytorch converter.</p></li>
<li><p>Tools: Converter: PyTorch: added PixelShuffle support for pytorch converter.</p></li>
<li><p>Core: Fixed snpe-diagview –chrometrace stats computations related to averaging when multiple input sets are involved.</p></li>
<li><p>Core: Updated the sample app MemoryMappedUserBuffer to reflect proper usage of memory mapped userbuffer.
registration/de-registration, once in SNPE’s lifecycle and not once per execute.</p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p>2.19.0</p></td>
<td><p>January 2024</p></td>
<td><ul class="simple">
<li><p>Core: Added support for sharing regular userbuffers (non memory-mapped) via single buffer and offsets.</p></li>
<li><p>SDK: Added supported SOC table to SDK documentation.</p></li>
<li><p>Core: Added new option –validate_cache to snpe-net-run and snpe-throughput-net-run to validate
HTP cache before network initialization.</p></li>
<li><p>Tools: Converters: Added the support for assigning input dtype in PyTorch converter.</p></li>
<li><p>Tool: Converter: Pytorch: Fixed the issue of duplicate buffer names for two identical transpose ops.</p></li>
<li><p>Tools: Fixed bug to correctly convert shared static tensor to FP16.</p></li>
<li><p>Tools: Converter: Supported optional initial_h and initial_c in Onnx bidirectional LSTM.</p></li>
<li><p>Tools: Converter: TFlite: Fixed data type mismatch issue for TFLite pre-quantized model.</p></li>
</ul>
</td>
</tr>
<tr class="row-even"><td><p>2.18.0</p></td>
<td><p>December 2023</p></td>
<td><ul class="simple">
<li><p>Tools: Converters: Updated squashing logic to avoid removing model outputs.</p></li>
<li><p>Core: Added new SNPE builder API s Snpe_SNPEBuilder_SetCacheCompatibilityMode() / SNPEBuilder::setCacheCompatibilityMode()
to set HTP cache compatibility mode for cache selection.</p></li>
<li><p>Tools: Converter: PyTorch: Raise an error for custom op support in SNPE product.</p></li>
<li><p>Core : Added Relu UDO example to SNPE sdk.</p></li>
<li><p>Tools: snpe-dlc-graph-prepare - Added a new option –num_hvx_threads for reserving no of HVX threads for a graph running on HTP.</p></li>
<li><p>Tools: Converters: support group_norm in pytorch converter.</p></li>
<li><p>Core: ArgMax example added to UDO examples in SNPE SDK.</p></li>
<li><p>Core: Updated CAPI Sample App for Memory Mapped User Buffer to demonstrate usage of dmabuf (libdmabufheap.so).</p></li>
<li><p>Tools: Converters: Add Gather and Take support in pytorch converter.</p></li>
<li><p>Tools: Converters: PyTorch: Added support for AvgPool1d/MaxPool1d/AvgPool3d/MaxPool3d/AdaptiveAvgPool1d/
AdaptiveMaxPool1d/GlobalAvgPool1d/GlobalMaxPool1d.</p></li>
<li><p>Tools: Converter: Onnx: Enforce h/c input buffers of LSTM to be NONTRIVIAL.</p></li>
<li><p>Tools: Converter: Support BROADCAST_TO in tvm tflite frontend.</p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p>2.17.0</p></td>
<td><p>November 2023</p></td>
<td><ul class="simple">
<li><p>Core: Added –userbuffer_memorymapped_shared option in snpe-net-run and snpe-throughput-net-run to exercise shared memory
mapped buffers.</p></li>
<li><p>Added product and OS info to SDK.yaml file.</p></li>
<li><p>Core: Added new APIs Snpe_UserMemoryMap_AddFdOffset / UserMemoryMap::add() and
Snpe_Util_CreateUserBufferShared() / IUserBufferFactory::createUserBufferShared() to enable sharing memory mapped user buffers
allowing different tensors to register using the same address/file descriptor and an unique byte offset.</p></li>
<li><p>Tools: Converters: Updated algorithm to fix Tensor Layout from Constant operator when it is located ahead of Concat Operator.</p></li>
<li><p>HTP: updated backend extensions config - changed graph object to graph array to allow different graphs have different set of
properties.</p></li>
<li><p>Tools: Converter: Fixed param name parsing issue in pytorch converter.</p></li>
<li><p>Tools: Pytorch converter: Added support for OneHot op.</p></li>
<li><p>SNPE Core: Added support for targetSdkVersion 32 in SNPE apk.</p></li>
<li><p>Core: Update MemoryMappedUserBuffer sample app to demonstrate shared buffer usage. Added support for named input parsing
in all the sample apps.</p></li>
<li><p>Core: Enhanced error messages for API failures in userlogs as well as Snpe_ErrorCode_GetLastErrorString() /
DlSystem::getLastErrorString()</p></li>
<li><p>Tools:ONNX Converter: Fixed WhereOp axis format issue.</p></li>
<li><p>Tools: Converter: fix tensorflow strided_slice conversion for out of range start/end.</p></li>
<li><p>DSP Runtime: Added DSP reset / Subsystem Reset (SSR) error handling for logging API: Snpe_Util_SetLogLevel() /
SNPEFactory::setLogLevel().</p></li>
<li><p>Core: Fixed registration of memorymapped user buffer with multiple addresses against the same tensor name.</p></li>
</ul>
</td>
</tr>
<tr class="row-even"><td><p>2.16.0</p></td>
<td><p>October 2023</p></td>
<td><ul class="simple">
<li><p>SDK: Add support for Mobile SoC: SM8650</p></li>
<li><p>Core: Added new APIs Snpe_UserMemoryMap_AddFdOffset / UserMemoryMap::add() to enable sharing memory mapped user buffers allowing
different tensors to register using the same address/file descriptor and an unique byte offset.</p></li>
<li><p>Core: Added pre-emption count captured as yield count in logs in SNPE.</p></li>
<li><p>Core: Fixed redundant rpc memory allocations during graph initialization for memory-mapped (zero copy) buffer usage by providing
hints during graph prepare. snpe-dlc-graph-prepare option –memorymapped_buffer_hint introduced.</p></li>
<li><p>SDK: Added sdk.yaml in SDK to capture build and other version info.</p></li>
<li><p>Tool: Common: Quantizer: Added a Quantizer pass to make static inputs of Elementwise Op float if the output is overridden to float.</p></li>
<li><p>Core: Performance improvements done in network initialization time - up to 1.5x speed up for certain networks with HTP offline cache.</p></li>
<li><p>Tools: Converters: Added broadcast support for layernorm op weights and bias.</p></li>
<li><p>Tools: Converters: Added rectangular SpaceToDepth op support to handle SpaceToDepth pattern in Pytorch model.</p></li>
<li><p>DSP Runtime: Added HTP DLBC(Deep Learning Bandwidth Compression) option for graph preparation.</p></li>
<li><p>Tools: Converters: Added batch_norm ND support in tflite/pytorch converter.</p></li>
<li><p>Tools: PyTorch Converter: Add support for custom op in QNN product.</p></li>
<li><p>Tools: enable LSTM operator in SNPE.</p></li>
<li><p>Tools: Converters: Onnx: Fixed conversion failure for gather op with scalar indices.</p></li>
<li><p>Tools: Quantizer: Fixed an issue by not converting Cast to Convert if next op is float.</p></li>
<li><p>Core: Fixed Snpe_SNPEBuilder_SetInputDimensions() / SNPEBuilder::setInputDimensions() /snpe-net-run option –input_name xxx
–input_dimensions yyy to evaluate new dims when a compatible dsp cache record is present. If new dims are accepted offline cache
will be rejected in favor of online preparation.</p></li>
<li><p>Tools: PyTorch Converter: Fixed parameter quantization override.</p></li>
<li><p>Tools: Converters: Fixed a conversion failure when folding Concat Ops.</p></li>
<li><p>Tools: Converters: Pytorch: Fixed an issue with applying overrides.</p></li>
<li><p>Tools: Converters: Onnx: Fixed a conversion failure when Onnx inferShape API returns an empty graph.</p></li>
<li><p>Tools: Converters: Onnx: Fixed a quantization failure for networks having Float16 activations.</p></li>
<li><p>Core: Implemented coexistence of DSP/HTP cache records prepared with different input dimensions. Added option to specify input
dimensions in snpe-dlc-graph-prepare. Cache selection logic updated to match dimensions passed during graph initialization.</p></li>
<li><p>Tools: Converters: Onnx: Add support for Gather Op with negative indices.</p></li>
<li><p>Tools: Converters: Updated the validation to see if the weights of FC and BN are eligible for optimization of BN into FC.</p></li>
<li><p>Core: Logging from backends is made conditional based on SNPE logging API invocation.</p></li>
<li><p>Docs: Updated inceptionv3 documentation to include LU / LE toolchains.</p></li>
<li><p>SNPE HTA: Added support of Pooling 16bit for large dimensions.</p></li>
<li><p>HTP: fixed graph prepare issue due to edge mod pad.</p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p>2.15.0</p></td>
<td><p>September 2023</p></td>
<td><ul class="simple">
<li><p>Tools: snpe-dlc-info tensor columns rearranged and HTP cache info section updated to display UDO information,
Optimization level etc.</p></li>
<li><p>Core: snpe-dlc-graph-prepare updated to overwrite existing cache with similar signature but from older cache version by default</p></li>
<li><p>SNPE GPU: Extreme power saver performance profile fixed to map to the lowest profile on the SoC instead of highest.</p></li>
<li><p>SNPE AIP: Extreme power saver performance profile now maps to lowest profile available on the SoC.</p></li>
<li><p>SDK: Fixed broken links in PSNPE C API documentation.</p></li>
<li><p>SNPE DSP: Extreme power saver performance profile for DSP v66 devices now maps to lowest performance profile available on the SoC.</p></li>
<li><p>Core: Memory Mapped Userbuffer Sample App - added error handling for incompatible data types.</p></li>
<li><p>Core: Fixed –debug not emitting intermediate tensors for offline cache based execution.</p></li>
</ul>
</td>
</tr>
<tr class="row-even"><td><p>2.14.0</p></td>
<td><p>August 2023</p></td>
<td><ul class="simple">
<li><p>Core: HTP offline cache records prepared for DSP architecture v68, v69 (sm8350/sm8450) will be rejected on SoCs
with DSP architecture v73 and above (sm8550).</p></li>
<li><p>Tools: Converter: Onnx: Added default attribute perm for Transpose Op.</p></li>
<li><p>Tools: Converter: Tensor with no consumers and not an actual graph output will be set to NATIVE for QNN Onnx Converter.</p></li>
<li><p>Tools: Converter: Allow only output tensors in the source model to be marked as QNN_TENSOR_TYPE_APP_READ. All other tensors with
zero consumers will change from being APP_READ to NATIVE.</p></li>
<li><p>Tools: Converters: Onnx: Added negative max_output_boxes_per_class parameter support for NonMaxSuppression.</p></li>
<li><p>Tools: Converter: update tvm version to support pytorch 1.13 version.</p></li>
<li><p>SDK: Update documentation contents for standalone SDK.</p></li>
<li><p>Tools: Converters: Added a Graph pass that matches Space2Depth Op (CRD &amp; DCR) from Reshape - Transpose - Reshape pattern.</p></li>
<li><p>Tools: quantizer: Avoid act’s bw changing according the weight/bias’s bw.</p></li>
<li><p>Op:HTP: added uint8 support for maxpool w77s44p00.</p></li>
<li><p>Core: SNPE de-initialization moved to a separate thread for all profiling levels to better affine to faster CPU core(s)
thus improving de-init time for most graphs.</p></li>
<li><p>Tools: Converters: Resolved OpValidation error related to LayerNorm Op caused due to the unsqueezed Gamma/Beta
tensor being &gt; 1D rank.</p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p>2.13.0</p></td>
<td><p>July 2023</p></td>
<td><ul class="simple">
<li><p>SDK: Updated Revision history formatting in SNPE docs.</p></li>
<li><p>Tools: Converter: GRU weights are shared across time unrolling step.</p></li>
<li><p>Documents: Update latest PyTorch Op support.</p></li>
<li><p>Tools: Converter: Allow only output tensors in the source model to be marked as QNN_TENSOR_TYPE_APP_READ.
All other tensors with zero consumers will change from being APP_READ to NATIVE.</p></li>
<li><p>SNPE DSP: added support for uint8 window7x7 stride3x3 maxpool ops on HTP.</p></li>
<li><p>Tools: snpe-dlc-graph-prepare - introducing new option –optimization_level. Higher optimization levels incur longer
prepare time but yields more optimal graph and hence faster execution time for most graphs.</p></li>
<li><p>Tools: Converter: Changed the logic for converting 1dOp into 2DOp by expanding along H dimension instead of W dimension.</p></li>
<li><p>Op:DSP: added support for logSoftmax.</p></li>
<li><p>Tools: Converter: Changed the translation of FloorDiv operator to ElementWiseDivide if the datatype of input is Int32.</p></li>
<li><p>Tools: Introducing –userbuffer_memory_mapped option in snpe-net-run, snpe-throughput-net-run and snpe-parallel-run for general
memory mapped userbuffer use cases(like ionbuffers in Android).</p></li>
<li><p>Tool: TF Converter: added support for conv2d_transpose layer with asymmetric strides.</p></li>
<li><p>Tools: TF Converter: Support optimized Gelu pattern that contains Mul instead of Realdiv.</p></li>
<li><p>API: Generic APIs added for memory-mapped userbuffers in lieu of existing ion buffer registration/de-registration APIs.</p></li>
<li><p>Core: Added CAPI based Sample Apps for userbuffer and memory-mapped buffers (like ion buffer).</p></li>
<li><p>HTP: Fixed bug in ReduceMean optimization during prepare.</p></li>
<li><p>Tools: snpe-diagview - fixed “Snpe Accelerator Time” and “Accelerator Time” data being larger than “Total Inference Time”.</p></li>
<li><p>SDK : libCalculator_Skel.so added to lib/hexagon-v68/unsigned and lib/hexagon-v69/unsigned folders of SNPE SDK.</p></li>
</ul>
</td>
</tr>
<tr class="row-even"><td><p>2.12.0</p></td>
<td><p>June 2023</p></td>
<td><ul class="simple">
<li><p>Tools: Converters: Added a new transformation to change MatMul into FullyConnected even without Bias.</p></li>
<li><p>Tools: Converter: TFlite: Added a fix to account for the difference in the offset sign and usage when quantizing tensors.</p></li>
<li><p>SNPE DSP: Introduced new extreme power saver performance profile to enable ultra low power inferencing usecases on HTP runtime.</p></li>
<li><p>SNPE Core: All binaries are now built with libc++, not libstdc++ for X86/Linux.</p></li>
<li><p>SNPE Core: All binaries are now built with clang9 instead of clang7.</p></li>
<li><p>Tools: Converters: Modified the output names generated by Pytorch Converter and TFlite Converters.
Also changed the axis tracking behavior to match the TF &amp; Onnx Converters.
This may change the name and layout for the output layer of the model.</p></li>
<li><p>API: Fixed Snpe_SNPEBuilder_SetTimeOut/SNPEBuilder::setTimeOut() to failure when Snpe_SNPE_ExecuteUserBuffers()/SNPE::execute()
fails to return within the timeout duration.</p></li>
<li><p>Tools: Quantizer: Fixed an issue that prevented weights &amp; bias inputs of Batchnorm from being set as FP16.</p></li>
<li><p>Tools: Quantizer: Fixed an error related to locking the WeakPtr associated with the Bias tensor to Convolution Op.</p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p>2.11.0</p></td>
<td><p>May 2023</p></td>
<td><ul class="simple">
<li><p>SDK: dependencies.sh changed to check-linux-dependency.sh and check_python_depends.sh changed to check-python-dependency.sh.
Also envcheck.sh added.</p></li>
<li><p>Core: SNPE C++ APIs are deprecated. To maintain backwards compatibility C++ header-only wrapper APIs are included in the SDK
that invokes SNPE C APIs internally.</p></li>
<li><p>Op: ONNX converter: added support for Mod.</p></li>
<li><p>CPU: INT8 support enabled for LA targets.</p></li>
<li><p>Core: Caffe source framework models are no longer supported in SNPE.</p></li>
<li><p>Core: arm-32 platform is no longer supported in SNPE.</p></li>
<li><p>SDK directory structure is updated.</p></li>
<li><p>Documentation refreshed with a new look and feel and contents are enhanced.</p></li>
<li><p>Core: New C API s added to match the deprecated C++ API capabilities.</p></li>
<li><p>Core: Init time improvements for most models with HTP offline cache record.
(Note that the offline cache needs to be regenerated to take advantage of this improvement)</p></li>
<li><p>Core :Native Cpp example for Platform Validator with C APIs is now functional.</p></li>
<li><p>Tools: snpe-net-run now allows –debug when input list has output op names (# )or output tensor names (% ) specified
in the first line.</p></li>
</ul>
</td>
</tr>
<tr class="row-even"><td><p>2.10.0</p></td>
<td><p>April 2023</p></td>
<td><ul class="simple">
<li><p>GPU Runtime: Support Pack operation with 1 input.</p></li>
<li><p>Core: Updated C API documentation for ITensor/Userbuffer creation indicating data size.</p></li>
<li><p>Core: setLogLevel() API hooked up to the runtimes for updating logging level after creating logger handle.</p></li>
<li><p>Tools: snpe-throughput-net-run now supports –userbuffer_auto option (similar to snpe-net-run) for automatic IO tensor
data type detection.</p></li>
<li><p>Tools: Converters: Added a new optimization sequence to squash BatchNorm into FullyConnected.</p></li>
<li><p>HTP: Fixed issue with ElementwiseSin.</p></li>
<li><p>Tools: Fix the converter issue for GRU op.</p></li>
<li><p>SNPE AIP: Fixed perf profile setting for multithread scenario.</p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p>2.9.0</p></td>
<td><p>March 2023</p></td>
<td><ul class="simple">
<li><p>Core: Added new C API Snpe_SNPE_GetInputDimensionsOfFirstTensor() to facilitate retrieving Input dimension without Input tensor name.</p></li>
<li><p>Tools: ONNX converter: Added support for NonMaxSuppression op.</p></li>
<li><p>Tools: snpe-dlc-graph-prepare fix benign error message during offline prepare for v68 based SoC s (–htp_socs sm8350, sm7350 etc)</p></li>
</ul>
</td>
</tr>
<tr class="row-even"><td><p>2.8.0</p></td>
<td><p>February 2023</p></td>
<td><ul class="simple">
<li><p>Tools: Converters: Onnx: Added support for Sign.</p></li>
<li><p>HTP: solve vtcm overflow issue happened when change data layout: from uint8 flat to uint8 crouton in tcm.</p></li>
<li><p>Tool:ONNX Converter: Fixed TransposeOp input axis format NT issue.</p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p>2.7.0</p></td>
<td><p>January 2023</p></td>
<td><ul class="simple">
<li><p>Tools: Converters: Fixed a bug in the optimization that merges Matmul + Reshape + Add to FC Op that would incorrectly insert the</p></li>
<li><p>FC Op before the Constant Bias Op.</p></li>
</ul>
</td>
</tr>
<tr class="row-even"><td><p>2.6.0</p></td>
<td><p>December 2022</p></td>
<td><ul class="simple">
<li><p>Tools: onnx converter: support conv’s input data is Initializer.</p></li>
<li><p>DSP: Improve execute time of dynamic depthwise convolution with uint8 weights.</p></li>
<li><p>Core: Added error handling based on buffer data size in execute().</p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p>2.5.0</p></td>
<td><p>December 2022</p></td>
<td><ul class="simple">
<li><p>Tools: Added new options for snpe-net-run and snpe-parallel-run –use_native_input_files and –use_native_output_files to support
inputs in their native format as opposed to default float32 format.</p></li>
<li><p>Tools: Added new flag –userbuffer_auto in snpe-parallel-run to automatically detect and use the right buffer type based on tensor
data type in the model.</p></li>
<li><p>Documentation: SNPE1 to SNPE2 migration guide is added.</p></li>
<li><p>Tools: snpe-throughput-net-run - capturing the status of lost thread in the result summary.</p></li>
<li><p>Tools: snpe-dlc-quant: Fixed abnormal DLC size increase when axis quantization is used.</p></li>
<li><p>Tools: Tensorflow Converter: Fixed issues with per-channel quantization of weights: set is_symmetric = true by default, added param
“axis” and “is_symmetric” into weight encodings info.</p></li>
<li><p>HTP: solve vtcm overflow for transposeconv2d layer whose groups &gt; 1, in depth= out depth, padding =0 and groups != in depth.</p></li>
</ul>
</td>
</tr>
<tr class="row-even"><td><p>2.4.1</p></td>
<td><p>October 2022</p></td>
<td><ul class="simple">
<li><p>Tools: New tools - snpe-architecture-checker &amp; snpe-quantization-checker are added.</p></li>
<li><p>snpe-net-run: Added new flag –userbuffer_auto to automatically detect and use the right buffer type based on tensor data type in the
model</p></li>
<li><p>SNPE Core: Enabled logging in Op validation.</p></li>
<li><p>SDK: Added missing documentation files for snpe-quantization-checker.</p></li>
<li><p>GPU Runtime: Improved network initialization time in subsequent runs on GPU when using setInitCacheMode.</p></li>
<li><p>Tools: ONNX Converter: fixed issue related to missing Cast operation.</p></li>
<li><p>Tools: Missing files for snpe-quantization-checker have been added to the SDK.</p></li>
<li><p>Tools: Fixed functional failure for snpe-architecture-checker.</p></li>
<li><p>Tools: Quantizer: Improve Error handling to remove ‘uncaught exception’ errors.</p></li>
<li><p>Tools: Fixed bug in snpe-dlc-quantize with option –axis_quant and –enable_htp when multiple socs are passed using –htp_socs.</p></li>
<li><p>GPU Runtime: Fixed validation errors for Concat op with large dimensions.</p></li>
<li><p>GPU Runtime: Improved accuracy in models having Concat op with large dimensions.</p></li>
<li><p>DSP Runtime: Bug fix in running HTP FP16 networks on non fp16 supported SoCs (like sm8350, sm7350)</p></li>
<li><p>GPU Runtime: Fixed verifier issue in Softmax2UdoPackage.</p></li>
<li><p>GPU Runtime: Improved network initialization time in subsequent runs on GPU when using netrun –storage_dir option.</p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p>2.3.1</p></td>
<td><p>September 2022</p></td>
<td><ul class="simple">
<li><p>Tools: Converters: Onnx: Added 5D tensor support for PoolMax3d.</p></li>
<li><p>Tools: GoogleNAS: Added support for utilizing the GoogleNAS service with SNPE hardware in the loop (HIL).</p></li>
<li><p>Tools: Quantizer: Added fix to use default activation bitwidth for static tensors instead of default parameter, except for static
tensor that are known to be parameters like convolution weights and bias</p></li>
<li><p>SNPE Core: Fix online dequantization of int4 axis quant dlc when ran on CPU/GPU.</p></li>
<li><p>SNPE Core: Fixed stability with concurrency use cases.</p></li>
<li><p>GPU Runtime: Fixed accuracy issues related to tensor memory optimization.</p></li>
<li><p>Tools: Quantizer: Fixed issue observed with int4 weight override support.</p></li>
</ul>
</td>
</tr>
<tr class="row-even"><td><p>2.2.1</p></td>
<td><p>August 2022</p></td>
<td><ul class="simple">
<li><p>Core: Added userlogs ( –userlogs=warn) for Op validation failures for both offline and online prepare thereby making it easier to
track fallback.</p></li>
<li><p>Core: HTP Offline Cache Blob backward compatibility - Snpe Version check relaxed from SNPE-2.2.1 onwards.</p></li>
<li><p>Tool: Converters: Added DepthToSpace DCR/CRD pattern that matched reshape, transpose, reshape nodes.</p></li>
<li><p>Core: Fixed dlc-info to display per axis encoding information for axis_quant dlcs.</p></li>
<li><p>Tools: Quantizer: Added support for CLE quantization algorithm.</p></li>
<li><p>Core: snpe-dlc-graph-prepare bug fixes-bound –vtcm_override to the maximum VTCM for each SOC chipset requested instead of
a hardcoded
8MB. Limit to 1 cache record per SoC in the dlc</p></li>
<li><p>Core: Fix runtime de-quantization of weights and biases for axis quantized dlcs when executing in floating point backends (CPU/GPU).</p></li>
<li><p>Tool: Onnx Converter: Added axis tracking edge case fixes for Concat and MatMul operations.</p></li>
<li><p>Core: Added protection for loading malicious dlc file.</p></li>
<li><p>Converter: change the output dims as the node output axis format order.</p></li>
<li><p>Core: SNPE::Execute() API updated to validate input/output buffer map size before proceeding.|</p></li>
<li><p>Core: snape-dlc-quantize - fixed error in handling % in input list.</p></li>
<li><p>Tools: snpe-dlc-quantize miscellaneous bug fixes with –output_dlc option.</p></li>
<li><p>Tools: Converter: Resolved bug that caused failure to override weight encodings for Conv Ops.</p></li>
<li><p>Tools: Quantizer: Fixed issues related to axis quantization when the model contains TransposeConv2D.</p></li>
<li><p>Tools: Converter: Fixed bug in elementwise min and max sequence optimization.</p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p>2.1.1</p></td>
<td><p>July 2022</p></td>
<td><ul class="simple">
<li><p>Core: Re-Enable LSTM support for CPU, GPU (HTP will follow).</p></li>
<li><p>DSP Runtime: Implemented rules for coexistence and selection of multiple cache records for HTP based on VTCM size, DSP Architecture,
and SoC</p></li>
<li><p>Tools: Converter: Added optimization to fold scalar min + max to ReluMinMax.</p></li>
<li><p>Tools: Quantizer: Re-enabled support for overriding activation quantization (overriding weight quantization will follow).</p></li>
<li><p>Tools: Quantizer: Fixed missing skip_quantization command line argument in the new snpe-dlc-quantize shell script.</p></li>
<li><p>Tools: Quantizer: Fixed axis quantization failure.</p></li>
<li><p>Tools: Quantizer: Fixed issues with quantizing inputs to the gather op.</p></li>
<li><p>Tools: Converter &amp; Quantizer: Update converter and quantizer to persist the command used in the DLC that can be displayed in
snpe-dlc-info.</p></li>
<li><p>Tools: DLC Viewer: Fixed to support the new DLC Format.</p></li>
<li><p>C API: Added new Snpe_DlContainer_OpenBuffer() to support loading a model from a buffer.</p></li>
<li><p>Docs: Fixed C API documentation related to creating a User Buffer.</p></li>
<li><p>Core: Change default option for SNPEFactory::isRuntimeAvailable() to UNSIGNEDPD_CHECK from NORMAL_CHECK. Note that this also affects
the C API.</p></li>
<li><p>Core: Re-enable NV21 input processing support.</p></li>
</ul>
</td>
</tr>
<tr class="row-even"><td><p>2.0.1</p></td>
<td><p>June 2022</p></td>
<td><ul class="simple">
<li><p>Added support for SM8550.</p></li>
<li><p>Added new C API. This API is in addition to the C++ API. Note that the APIs cannot be mixed, all code should use one or the other.</p></li>
<li><p>Updated the DLC internal format to use ‘ops’ rather than ‘layers’ to more closely align the graph definition with QNN.</p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p>1.64.0</p></td>
<td><p>June 2022</p></td>
<td><p>Tool: Onnx Converter : Reenabled converter command line input dtype to take precedence over model specified.
GPU: Improved accuracy in deepsort model, Resolved issues with Conv + elu op fusion.
Tools: Quantizer: Fixed issue observed with applying 8-bit overrides using 16-bit default activation quantization encodings.
SNPE Core: Fixed failure to select HTP offline cache for certain multi-subnet network topologies.</p></td>
</tr>
<tr class="row-even"><td><p>1.63.0</p></td>
<td><p>May 2022</p></td>
<td><p>SNPE Core: Support PRELU bias broadcasting in SNPE.
SNPE Core : snpe-diagview tool updated to display actual units (like cycles) instead of usec by default.
SNPE Core: Open GL buffers supported for GPU backend.
SNPE Core : Fixed Zip utility’s std::istream index to internal extensible array to be const for every container(DLC) load.</p></td>
</tr>
<tr class="row-odd"><td><p>1.62.0</p></td>
<td><p>April 2022</p></td>
<td><p>DSP Runtime: Perf improvement for FP16 models on HTP.
Added GatherV2 support for SNPE-QNN-DSP.
Tools: Converters: Added 5D tensor annotations NCDHW and NDHWC support.
Tools: Converters: TF: Fixed issue with translating explicit padding from Conv Op.
Tools: Converters: Onnx: Fixed Onnx Concat axis.
Tools: ONNX Converters: Fixed implementation details for Conv1D and Pool1D Ops.
Tools: Converters: Onnx: Added optimization folding continuous reshapes.</p></td>
</tr>
<tr class="row-even"><td><p>1.61.0</p></td>
<td><p>March 2022</p></td>
<td><p>Tools: Converters: Onnx: Enabled support to handle custom op inputs correctly when the default values are provided.
Tools: ONNX Converter: Added support to resolve static ONNX Cast operation as Constant.
CPU Runtime: Supported CRD mode for depthtospace(pixelshuffle).
Improved performance of loading DLC from a memory buffer.
Fixed scale calculation for ONNX Resize Operator for align_corner mode.
Also overrides Resize input axis format as per source axis order.</p></td>
</tr>
<tr class="row-odd"><td><p>1.60.0</p></td>
<td><p>February 2022</p></td>
<td><p>Tools: Converter: Added ONNX Gemm transA and transB support.
Native sample code is updated to take static quantization parameters for quantized input buffers.
libSNPE.so, libcalculator.so, libplatformValidatorShared.so, libnpe_dsp_domains_v2.so - libraries generated with gcc8.2 and
gcc9.3 toolchain - are now compiled with additional read-only relocation compiler flags.
HTP: Fixed issue with Cast op usage in certain configurations.
ONNX Converter: Improvements to handle different input axis layouts.</p></td>
</tr>
<tr class="row-even"><td><p>1.59.0</p></td>
<td><p>January 2022</p></td>
<td><p>DSP Runtime : Added support for edge padding from SNPE side.
Tools: ONNX Converter: Limited support for Expand operator when it can be interpreted as a noop from operator attributes.
Tools: ONNX Converter: Added support for ScatterND.
Tool: Quantizer: Fixed duplicate Convert layer Id issue observed in generated DLC when multiple Convert layers feed into a single layer.
Tool: ONNX Converter: Fixed handling of models with inputs of unknown shape.
Tools: ONNX Converter: Resolves issue where Shape operator translation could fail if the input was part of the initializer list.</p></td>
</tr>
<tr class="row-odd"><td><p>1.58.0</p></td>
<td><p>December 2021</p></td>
<td><p>Tools: Converter: Enabled broadcasting of weights and bias for BatchNorm layer to match channel dimensions.</p></td>
</tr>
<tr class="row-even"><td><p>1.57.0</p></td>
<td><p>November 2021</p></td>
<td><p>Tool: Onnx Converter: Added support in dry-run mode to handle reporting ops that are not in onnx schema domain.
Tool: Converter: - Updated inaccurate macs/params calculations for Ops per re-analysis.
CPU Runtime: Set the max detections to keep top K for Caffe SSD network.
Tool Converter: Removed obsolete ssd_permute_param parameter in caffe converter permute translation.
SNPE DSP: Fix axis quantization not adding all the fixedPointParam of output to bufferDeltas.
Tool Converter: Fixed coefficient input broadcasting issue for ONNX Prelu operation.
Tool Converter: Fixed axis tracking bug for permute when input is btf format.</p></td>
</tr>
<tr class="row-odd"><td><p>1.56.2</p></td>
<td><p>October 2021</p></td>
<td><p>DSP Runtime: Caffe SSD models can now run fully on HTP, but show some performance issues.
Tool: Converter: Added new layernorm sequence for pattern matching and added a constraint to enforce MatMul layer’s constant
second input to 8-bit tensor in quantized model.</p></td>
</tr>
<tr class="row-even"><td><p>1.55.0</p></td>
<td><p>September 2021</p></td>
<td><p>Added support for the OneHot operation across the SNPE converters with runtime support available on SNPE CPU.
Tool: ONNX Converter: Added support for LSTM &amp; CRNN.
DSP Runtime: Added support for LSTM.
Tools: Converters: Added support for Caffe Power scale/shift parameters.
SNPE DSP: Fixed the issue of invalid cache record added to DLC while doing offline prepare for HTP.
Tools: Converters: Fixed Softmax and Reduction Ops to have default case for output_buf axis format.</p></td>
</tr>
<tr class="row-odd"><td><p>1.54.0</p></td>
<td><p>August 2021</p></td>
<td><p>Tool: TF Converter: Added support for detecting eltwise pattern for batchnorm layer with fakequant inputs.
Tools: Converters: Adds support for Caffe Reduction layer Sum and Mean Ops.
Tool: Quantizer: Added support to make Convert Operator upscale and downscale quantization parameters loss free.
ONNX Converter: Add support for LSTM &amp; CRNN in converters.
DSP Runtime: Add support for LSTM.
Tool: Converters: Added batch dimension to anchor input data conversion from tensorflow corner style to center style for
DetectionOutput operation optimization.
Tool: ONNX Converter: Added support to pre-apply ONNX batchnorm scale and bias quantization encodings before getting consumed by
Converter to compute weights and bias.
Add support for reverse engineering SAME padding mode from the explicit pad values.</p></td>
</tr>
<tr class="row-even"><td><p>1.53.2</p></td>
<td><p>July 2021</p></td>
<td><p>Tool: Quantizer: Added support for fake quant operators in snpe-dlc-quantize.
Tools: TF Converter: Support for logical_and, equal, greater, greater_equal, less, less_equal, not_equal, logical_or, select.
Tool: TensorFlow Converter: Added support for Identity nodes that act as graph output nodes.
Tool:ONNX converter: Fixed incorrect default bias shape for ConvTranspose translation.</p></td>
</tr>
<tr class="row-odd"><td><p>1.52.0</p></td>
<td><p>June 2021</p></td>
<td><p>Tools: Converters: Removes pre-broadcasting of constant tensors resulting in smaller file sizes in converter output.
Tool: Converter: Added Converter support for Nd Reshape layer.
Tool: Converter: Added CastOp support for TF.
Tool: Converter: Added support for static subgraph resolutions at conversion time.
Tool: Converter: Added support for tensor dtype for TF fill op translation.
SNPE DSP: Fixed variance accuracy loss in InstanceNormalization on HTP.
SNPE GPU : Added optimized kernel for ReduceMean Operation.
Tool: Converter: Fixed bug in TF fullyconnected translation where input was intermittently out-of-order.
SNPE DSP: Fixed the issue of freeing the uninitialized pointer that is leading to random crash.
SNPE DSP: Optimized specific unpack-&gt;elementwise sequences for certain models on HTP.</p></td>
</tr>
<tr class="row-even"><td><p>1.51.0</p></td>
<td><p>May 2021</p></td>
<td><p>Tool:Converter: Added supported for Onnx WhereOp.
Added support for edge padding type for pad operation in GPU runtime.
SNPE DSP: Enabled support for ElementWiseUnary abs layer on HTP.
GPU Runtime: Added support for asymmetric reflect padding for pad operation.
UDO: Allow users to specify a different datatype for each core in single config file.
UDO: HTML documentation &amp; sample app is updated to provide example for loading UDO package.
DSP Runtime: Fixed the context leak on HTP targets during repeated init/deinit scenarios.
SNPE: Init stage is optimized to be done faster.
SNPE DSP: Optimized maxpool with stride 2x1 on HTP.
SNPE DSP: Optimized the big sized concat ops to fit into memory.
SNPE DSP: Optimized the init on HTP.
SNPE DSP: Graph prepare is optimized for HTP targets to be able to run bigger graphs.
SNPE DSP: Fixed the issue with CDSP not going to sleep when the model is de-initialized.</p></td>
</tr>
<tr class="row-odd"><td><p>1.50.0</p></td>
<td><p>April 2021</p></td>
<td><p>Tool: Quantizer: Added SNPE Quantizer support for is_symmetric field used in updated AIMET specification.
DSP Runtime: Improved instance norm op accuracy when input size is big.
DSP Runtime: Enabled edge padding support for v65/v66 targets.
Tool: Tensorflow Converter: Resolved Xiaomi issue where TF Mul was not being translated correctly.</p></td>
</tr>
<tr class="row-even"><td><p>1.49.0</p></td>
<td><p>March 2021</p></td>
<td><p>ONNX Converter: Added support for ONNX 1.6 (Opset 11)
TF Converter: Added support for TF2.3 models.
TFLite Converter: Add initial TFLite converter.
ONNX Converter: Add support for YOLOv2, YOLOv3, tiny-YOLOv3, and YOLOv5.
DSP Runtime: Optimize conversion performance to/from 16-bit quantized values on HTP.
Converters: Improve detection and removal of unconnected nodes.
ONNX Converter: Add support for DETR model.
AIP Runtime: Optimized the input and output data format conversion times for specific depth configurations for models having 16bit
activations.
DSP Runtime: Enabled support for Matmul on HTP.
snpe-throughput-net-run: Fix input_list processing when using multiple batches.
TF Converter: Fixed inconsistent network topology flow differing between runs for larger models with forking nodes.
DLC Quantizer: Fixed a race condition that might result in integer overflow.
Android Sample App: Fix to work correctly when multiple models are packaged, with only some requiring UDO.
snpe-diagview: Fixed crash bug when using circumstances AIP runtime networks with UB_FLOAT and UB_TF8 buffer modes with init caching.
DSP Runtime: Additional model support with offline prepare.</p></td>
</tr>
<tr class="row-odd"><td><p>1.48.0</p></td>
<td><p>February 2021</p></td>
<td><p>SDK: Migrated to use Ubuntu 18.04 as the host platform.
SDK: Updated dependencies.sh and check_python_dependencies.sh for the transition to Ubuntu 18.04 - Python 3.6, and libc++9.
SDK: Removed the system variants for the DSP stub libraries.
Tool: Switched diagnostic logging (SNPEDiag.log) to a new file format.
Added static buffer mapping support for frequently used buffers in DSP runtime.
Improved instance norm op accuracy when input size is big in DSP runtime.
Added support for Unsigned PD with the AIP runtime.
Tools: Converters: Fixed a bug which might prevent applying quantization overrides to a model.
Fixed NMS op code in HTP core.
Input node followed by concat node is optimized in HTP.
SNPE DSP: Fixed Unpack Layer indexing error on HTP.
DSP Core: Fixed overflow issue in instance norm op when variance is too small.</p></td>
</tr>
<tr class="row-even"><td><p>1.47.0</p></td>
<td><p>January 2021</p></td>
<td><p>Added support for TF 1.15 for NonMaxSuppressionV3 translation in TF converter.
Added support for Normalize layer translation in Caffe converter.</p></td>
</tr>
<tr class="row-odd"><td><p>1.46.0</p></td>
<td><p>December 2020</p></td>
<td><p>Improved CDSP power voting by using client specific context id.
SNPE DSP: Improved argmax op performance by optimizing l2 cache prefetch and replacing int to float cast op.
Improved the input/output data conversion times on AIP runtime for specific depth configurations.
Enabled the support for random inputs for networks having more than one input layer in SDK benchmarking scriptsTool:
qnn-tensorflow-converter: Removed ?allow_unconsumed_nodes option from TF converter as it is now the default.
Enabled elementwise sub and div on HTP.</p></td>
</tr>
<tr class="row-even"><td><p>1.45.0</p></td>
<td><p>November 2020</p></td>
<td><p>Beginning with SNPE 1.45.0, users must install libc++1-8 using apt-get or other package manager in order to perform offline cache
generation for HTP.
Optimized shallow convolution (depth &lt;= 4) in inputsupernode for v66 DSP.
Remapped previous converter translation of Caffe Tile layer as ConcatOp to TileOp in Caffe Converter.
Fixed small accuracy regression on VGG and Flownet models in DSP runtime.
Named the HTA threads created on CDSP appropriately.
Improved logging when libcdsprpc cannot be found.
Fixed the issue with AIP runtime being unavailable on the Android R platforms.
Fixed the issues with HTA metadata generation of conv2d op.</p></td>
</tr>
<tr class="row-odd"><td><p>1.44.0</p></td>
<td><p>October 2020</p></td>
<td><p>Optimized concat performance when input size is very big in DSP runtime.
Optimized slice performance when split 3 channel RGB input in DSP runtime.
Removed support for proposal layer in DSP runtime.
Added SNPE converter support for Softmax axis parameter.
Added support for consuming AIMET/custom quantization encodings to override quantizer generated encodings.
Fixed an issue on graphs where the final node in a graph was an elementwise operation with more than two inputs.
Fixed bug where output_shapes were calculated as float values for DepthToSpace and SpaceToDepth Ops.
Changed ONNX converter to not allow negative or placeholder dimensions.
Fixed potential issues with some models where QAT nodes may not get propagated properly to the final converted model.</p></td>
</tr>
<tr class="row-even"><td><p>1.43.0</p></td>
<td><p>September 2020</p></td>
<td><p>Improved the input/output conversion times for models having depth as 4 on AIP runtime.
Enabled initial support for constant layers along with elementwise Op on HTA.
Added support for opaque float concat operation in SNPE DSP concat layer.
Added support for Caffe’s “Clip” layer in the caffe converter.
Added int16 example to snpe-sample app.
Fixed the crash while running multi-threading applications with user buffer mode on AIP runtime.
Fixed bug in ONNX converter that used a hard-coded name for the sequence length input of the LSTM operator.
Fixed bug in ONNX converter for Unsqueeze layer, which got a key-error with static inputs.
Fixed the bug in l2_fetch usage during output conversion which improved the performance significantly for some models running on
AIP runtime.
Fixed the issue with generation of HTA enabled dlc for denoise model.
Fixed the segmentation fault issue during dlc generation with specific inputs on HTA.
Fixed issue with PlatformValidator.hpp reference to non-existent #include.</p></td>
</tr>
<tr class="row-odd"><td><p>1.42.2</p></td>
<td><p>September 2020</p></td>
<td><p>Fixed the bug in l2_fetch usage during output conversion which improved the performance significantly for some models running on
AIP runtime.</p></td>
</tr>
<tr class="row-even"><td><p>1.42.0</p></td>
<td><p>August 2020</p></td>
<td><p>Removed V60 DSP libs from SNPE SDK.
Enabled the AIP runtime support for generating the intermediate outputs from HTA with online compiler.
Enabled multithread for re-quantize process in DSP runtime.
Added optional parameter to set the hysteris period for sustained high and burst profiles in DSP runtime.
Added support for opaque float concat operation in SNPE DSP concat layer.
Fixed bug in UserBufferTF8 where retrieving the encoding would always return null.
Fixed box decoder performance issue on mobilenet v2 ssd model for DSP runtime.
Fixed tanh performance issue by replacing QuantizedTanh_8_ref with QuantizedTanh_8 op in DSP runtime.</p></td>
</tr>
<tr class="row-odd"><td><p>1.41.0</p></td>
<td><p>July 2020</p></td>
<td><p>Added MatMul support on the CPU runtime.
Added support for new version of 7250 with integrated PMIC module.
User Defined Operations(UDO) with weight parameters have been added to demonstrate both quantization and network execution on CPU
and DSP runtime cores
respectively.
Optimized tile Op in DSP runtime, that used 2d memcpy for w-d plane tiling and HVX for tiling along depth.
Fixed stack overflow issue in concat layer in DSP runtime.
Fixed issue with input for multibatch in DSP runtime.
Fixed issue in TF converter that prevented FusedBatchNorm operations from being merged into previous Convolution layer.
Fixed DSP crash issue due to stack overflow Concat layer preparation.</p></td>
</tr>
<tr class="row-even"><td><p>1.40.0</p></td>
<td><p>June 2020</p></td>
<td><p>Added DSP Graph Caching support for AIP models with HVX subnets.
Upgraded DSP to use Hexagon SDK 3.5.2 toolchain.
Added support for 16 bit UDO layers in DSP.
Added support for large average pooling, reduce_mean layer and improved elemetnwise_mul support for larger tensor size.
Fixed the issue with buffer ordering during the execution of batched models on AIP runtime.
Fixed issue with SsdDetectionOut when number of classes is only 1.
Fixed accuracy issue with Correlation 1D op.
Fixed improper processing when 16bit input quantization is used in certain cases.
Fixed scaling logic in convert_16 op.</p></td>
</tr>
<tr class="row-odd"><td><p>1.39.1</p></td>
<td><p>May 2020</p></td>
<td><p>Fixed the performance regression of Mobilenet SSD model on AIP runtime.</p></td>
</tr>
<tr class="row-even"><td><p>1.39.0</p></td>
<td><p>May 2020</p></td>
<td><p>The SNPE license (LICENSE.pdf) has been updated, please review it for more details. Additionally the REDIST.txt has been removed, as
redistribution is covered in the license.
Added graph caching support which improves init times for DSP &amp; AIP networks. (DSP subnet with in AIP is not supported)
Optimized Prelu to reduce saturation loss during re-quantization at prelu by using cubic approximation in AIP runtime.
Fixed the input conversions to allocate the required buffers during initialization itself, to improve the inference time for
AIP runtime.
Fixed potential bug with freeing threads in DSP runtime.
Added additional logging messages for debugging in DSP runtime.
Added support for the AIP runtime in the SNPE sample “snpe-sample”.
Added support for BBox transform layer in Caffe2 converter.
Added new opset support in the ONNX converter: ArgMax, ArgMin, Concat, PRelu, ReduceMean, ReduceMax, ReduceMin, ReduceSum,
Squeeze, Unsqueeze, MatMul, Flatten, Max, Split, Clip.
Added support for the fixed-point version of the MobileNetV3 model with H-Swish neuron in TF converter.
Improved support of resizing in Crop layer for TF and Caffe converter by introducing new ?counts? parameter.
Fixed issue of incorrect UDO tensor datatype in quantizer.
Fixed issue with setting the performance profile mode for HTA from AIP runtime in multi-threading use cases that could cause
performance to drop.
Fixed issue with snpe_bench.py memory profiling.</p></td>
</tr>
<tr class="row-odd"><td><p>1.38.0</p></td>
<td><p>April 2020</p></td>
<td><p>Enabled FC/MatMul to use VTCM if available in DSP.
Optimized 16-bit MeanVarianceNormalize in DSP runtime.
Added support batchwise scalar divide operation in DSP runtime.
Optimized Hard-swish operator for mobilenetV3.
Added support for EltwiseMin layer for ONNX converter and CPU runtime.
Added support for Onnx BatchNorm layer (OpVer 9, 12) in Onnx Converters.
Caffe preprocessing subtract_mean layer is added. If specified, converter will enable preprocessing specified by a data layer
transform_param subtract_mean.
ONNX softmax converter support only existed for rank &lt;= 2. Support for tensors rank &lt;= 4 was added.
Enabled the end-user / developer to request the use of an unsigned process domain to avoid the requirement of signed libraries for
SNPE execution on 8250 and newer devices.
Removed autoquantization for classes output in MultiClassNMS layer and added support for float addition in ElementwiseOp layer to
handle this case.
Fixed the issue with enabling stats for AIP runtime on models where number of layers in HTA subnet is more than SNPE layers.
Fixed the output conversions to allocate the required buffers during initialization itself in AIP runtime, to improve the
inference time.
Enabled honoring of padding information from the HTA driver which is pre-computed by AIP runtime earlier, to unblock execution of
more models.
Fixed the issue with output buffer id while converting depth2space to deconv on HTA.
Fixed a bug during graph transformation while folding the batchnorm on HTA.
Increased DCVS relaxed sleep latency duration, this will let power system know that CDSP can goto deeper sleep state. If there is no
active request for inferencing, it is better for system to go in deeper sleep state.</p></td>
</tr>
<tr class="row-even"><td><p>1.37.0</p></td>
<td><p>March 2020</p></td>
<td><p>Enabled the online compiler support for HTA 1.x family of devices.
AIP performance profiles behavior is aligned similar to DSP runtime for reduced power consumption in case of inference inactivity.
ONNX Converter: Added support for Onnx Pad layer (OpVer 11).
Added support for the h-swish layer used by MobileNet V3.
Removed support for the Generate Proposals, ROI Align, and ROI Proposal layers.
Added improved support for the reporting of Exceptions in the Java API.
Updated the DSP UDO header file to be compatible with SNPE 1.37.0.
The DSP UDO support is updated to be compatible with Hexagon SDK 3.5.1.
The network creation action was moved onto another thread to avoid impacting the affinity for the main thread of the calling program.
Snpe-dlc-info: Fixed issue in MACs calculation error for deconvolution layer.
Avoid crash on SDM845 and other v65 targets when unable to retrieve VTCM memory.
Fixed an issue in the TensorFlow converter where the weights in the Fully Connected layer were incorrectly transposed.
Fixed the support for using DSP UDO with the AIP runtime. Previously, the UDO packages would not be properly loaded in the AIP runtime.
Fixed DiagLog data for a UDO on GPU, where it did not report proper values for start and stop.
Enable support for keras batchnorm with empty mean and variance to a default values.
Fixed a memory leak when using IsRuntimeAvailable() with the VOLATILE_CHECK for the DSP runtime.</p></td>
</tr>
<tr class="row-odd"><td><p>1.36.0</p></td>
<td><p>February 2020</p></td>
<td><p>Added Java API extension to register UDO package with SNPE.
snpe-dlc-info now prints the command-line that was used to quantize the DLC if applicable.
Added support to handle UDO layers with multiple TF8 outputs with different quantization parameters.
Added support for an additional profiling level (moderate) for SNPE benchmarking script and associated snpe-net-run executable
for tracking initialization time metrics.
Upgraded DSP to use Hexagon SDK 3.5.1 toolchain.
Extend Platform Validator to detect HTA API version.
Add VOLATILE_CHECK Mode for SNPE DSP Runtime Checking to query runtime availability in each call instead of giving cached result.
Performance modes like LOW_POWER_SAVER, HIGH_POWER_SAVER, LOW_BALANCED added for CPU runtime.
Fixed bug with propagation of model version during conversion.
Fixed the issue with selecting the correct output shape during graph transformation while inserting1x1 conv2d for different
input format.
Fixed the issue with allocation of layer descriptor while loading the network on HTA.</p></td>
</tr>
<tr class="row-even"><td><p>1.35.0</p></td>
<td><p>January 2020</p></td>
<td><p>Introduce the User-Defined Operations (UDO) feature.
Added support for SDM720G/SM7125.
Added support to snpe-throughput-net-run for UserBuffer input tensors (both INT8 and INT16).
Input batching support is added for networks that can run completely on AIP runtime.
Add support for the tf.stack and tf.unstack ops to the DSP and CPU runtimes.
Add support for the tf.stack, tf.unstack, tf.floor, tf.minimum to the TF converter.
Fixed some small memory leaks that are seen when repeatedly calling dlopen()/dlclose() on libSNPE.so.
Updated the Deconvolution operation on DSP with a new kernel that improves performance on various kernel sizes and strides.
Fix ssd_detection CDSP crash on DSP runtime.
Updated the HTA to partition the input layer, if it has a connection to a layer that is not included in the same partition.
Improved the tiling configuration support for depth wise convolution layer.</p></td>
</tr>
<tr class="row-odd"><td><p>1.34.0</p></td>
<td><p>January 2020</p></td>
<td><p>Initial support for ops with 16-bit activations using HTA in both snpe-dlc-quantize and in the SNPE AIP runtime.
New option for snpe-net-run to automatically turn unconsumed tensors of the network (tensors that are not inputs to a layer)
into network outputs.
Fixed inconsistent results on SM8250 in certain cases for depthwise convolutions.
Add support for the depth2space operation on the GPU.
Using optimized Softmax implementation in AIP networks when input activation has more than 5000 elements.
Truncate detection output on DSP to return valid data only.
Ensure weights are properly flushed to DDR for use during inference in the DSP runtime.
Fix support for NV21 encoding in the DSP runtime.</p></td>
</tr>
<tr class="row-even"><td><p>1.33.2</p></td>
<td><p>November 2019</p></td>
<td><p>Address accuracy issues for Deconvolution in the AIP runtime.
Changed behavior of Crop layer resize, so it retains the number of copied elements on each dimension.
Make quantizer ?override_params work for AIP.
Reordered PerformanceProfile_t to be ABI compatible with 1.32.0.
Using optimized Softmax implementation in AIP networks when input activation has more than 5000 elements.</p></td>
</tr>
<tr class="row-odd"><td><p>1.33.1</p></td>
<td><p>November 2019</p></td>
<td><p>Fixed a build issue that incorrectly removed Symphony.</p></td>
</tr>
<tr class="row-even"><td><p>1.33.0</p></td>
<td><p>November 2019</p></td>
<td><p>New performance modes have been added:
LOW_POWER_SAVER: Run in lower clock than POWER_SAVER, at the expense of performance.
HIGH_POWER_SAVER: Run in higher clock and provides better performance than POWER_SAVER.
LOW_BALANCED: Run in lower balanced mode, provides lower performance than BALANCED.
snpe-dlc-info adds a summary of the layer types in use in the model.
Updated to use new BLAS functionality that leverages OpenMP. This adds a new dependency on the OpenMP shared library for
Linux platforms.
Added 32-bit bias support.
Support init caching for SSD output layer on DSP.
Fix memory leak causing increasing init time on DSP.
Add converter support for dilated convolution when used with fakequant nodes.
Multiple bugs fixed in snpe-onnx-to-dlc that were causing errors for models having torch.Mul op.
Extends TF converter support to NMSv1 Op in addition to existing support for v2 and v3 NMS Ops.
Tensorflow conversion bug fixed in infer_shape for StridedSlice Op. output_shape should not be a list of shapes but the shape
of the one output.
Fix bug with propagation of model version during conversion.
If burst mode is set, set thread affinity to Big Cores during init and de-init, and restore to the previous setting after the
actions are complete.
Fix segfault when using user buffers with a resizable dimension.</p></td>
</tr>
<tr class="row-odd"><td><p>1.32.0</p></td>
<td><p>Oct 2019</p></td>
<td><p>Add Caffe MVN Layer support in the Caffe Converter, CPU Runtime, and DSP Runtime
snpe-dlc-quantize: Enable the use of quantization parameters calculated during training when using dlc quantizer. To override the
SNPE generated
quantization parameters pass ?override_params to <a href="#id1"><span class="problematic" id="id2">|</span></a>snpe-dlc-quantize.
Removed deprecated command line arguments from converters. All three converters now require passing -i/?input_network for model
input paths.
snpe-dlc-diff: Added command-line option [?diff_by_id/-i] to snpe-dlc-diff. This option allows users to compare 2 models in
order (sorted by id)
Added support for L2Norm layer to TensorFlow converter
Optimized the DSP performance for the ‘Space To Depth’ layer
Add support in the Java API for setInitCacheEnabled(), and setStorageDirectory() to enable DLC caching support.
Allow graceful recovery after a fastrpc error - Recreate the userPD after the cDSP crashes so that the user can continue on the
SNPE process with subsequent
instances, instead of having to close the SNPE process. Note: <a href="#id3"><span class="problematic" id="id4">|</span></a>all the instance associated to the previous userPD will be lost.
snpe-dlc-viewer: Associate each layer type to a fixed color for consistency when using snpe-dlc-viewer
Split the SNPE isRuntimeAvailable method into two separate functions to improve backward compatibility with existing client
binaries that were built against the older signature.
TF Converter: Fix Elementwise Broadcast support
ONNX Converter: Fixed bug where output dimension was incorrect when keep_dims parameter was set to False for Argmax,
ReduceSum and ReduceMax.
ONNX Converter: Fixed bug where pad attribute was not properly parsed for Deconv Op.
Caffe Converter: Fixed bug when converting SSD-based models when using Python 3.
TF Converter: Fixed bug where converter was removing const Op input to reshape op when passed through identity op(s).
i.e const-&gt; identity -&gt; reshape.
Fixed bug where getOutputSize() would give the wrong result on output tensors in UserBuffer mode</p></td>
</tr>
<tr class="row-even"><td><p>1.31.0</p></td>
<td><p>September 2019</p></td>
<td><p>New patterns were added to enable running the CLE algorithm on more op patterns and model architectures.
Added Tensorflow converter support for Caffe-style SSD networks.
Added support for HeatmapMaxKeypoint layer in the CPU runtime.
Added support for ROI Align layer in CPU runtime.
Added initial L2Norm layer support in CPU runtime. No support for axis parameter yet: normalization is performed along the
inner-most dimension of the input tensor.
Support for single-input Concatenation layers was added to CPU, GPU and DSP.
Changed determination of number of batch dimensions in the Fully Connected layer so rank greater than 1 is always assumed to mean
that there is 1 batch dimension.
Removed constraint on the LSTM layer in the GPU runtime that prevented batch mode operation.
Added support for Leaky-RELU in the TensorFlow converter. Both the actual Leaky-Relu op and the elementwise op representation
are supported and map to SNPE’s Prelu op.
Added Argmax support to the Caffe converter, and optimized performance on the DSP runtime.
Added new column to snpe-dlc-info that displays the supported runtimes for each layer.
Fixed an edge case where in certain conditions OpenCL would return CL_INVALID_WORK_GROUP_SIZE.
Made isRuntimeAvailable Java API thread-safe.
Replace unstable image from sample Android classifier application data set with an image that is more consistent.</p></td>
</tr>
<tr class="row-odd"><td><p>1.30.0</p></td>
<td><p>August 2019</p></td>
<td><p>Documentation has been added to reflect the new common converter command line options for input processing; Converters now propagate
required batchnorm information for performing quantization optimizations; Support for the new bias correction quantization optimization
which adjusts biases by analyzing float vs quantized activation errors and adjusting the model to compensate; ONNX converter now
filters single input Concats as a no ops as SNPE didn’t support them; Converter input processing now uniformly handles different input
types and encodings; ONNX converter now supports the ConvTranspose ‘output_padding’ attribute by adding an additional pad layer
after the ConvTranspose op; Integrates the latest flatbuffer 1.11 library which brings speed improvements and options for model size
reduction; GPU size limitations with the ArgMax op (when setting the keepDims op attribute to false) can be worked around by enabling
CPU fallback; Fixed DSP error with MobileNet SSD on QCS403 and QCS405; Fixed the issue with partitioning of deconv layer in HTA;</p></td>
</tr>
<tr class="row-even"><td><p>1.29.0</p></td>
<td><p>July 2019</p></td>
<td><p>Added support for dlc reorder tool;Optimization of HTA d32 conversions;Added tf space_to_depth op for SNPE CPU and DSP runtime;
Benchmarking scripts enhanced for showing further break down of execution time, across various components;Added support for additional
ONNX binary element-wise ops;Optimized deconv layer for improving performance;Fixed an issue related to runtime error in DSP runtime;
Performance Optimization of SNPE GPU Runtime for Shufflenet V2 by using profiling level config</p></td>
</tr>
<tr class="row-odd"><td><p>1.28.0</p></td>
<td><p>June 2019</p></td>
<td><p>Added an optional argument to isRuntimeAvailable for the DSP runtime so that it doesn’t activate the DSP; Allow UB_T8 and UB_FLOAT
output for snpe-net-run; Added a new command line option for snpe-dlc-diff to check layer names; Updated the –dlc argument to
–output_path for snpe-caffe-to-dlc to align with the ONNX converter; Added –dry_run argument to snpe-onnx-to-dlc to allow evaluation
for successful conversion on an ONNX model; Added support for the gather op in the DSP runtime; Added support to convert the TF
MobileNet-V1-FPN-SSD model; Fixed a memory leak in the DSP runtime that is seen when repeatedly loading and unloading a network;
Addressed issues on V66 DSPs related to acquiring VTCM memory; Fixed an issue related to multiple inputs for the Caffe converter; Fixed
an issue in the TF converter related to element-wise sun and the atrous parameter; Fixed an issue in the TF converter related to
tf.crop_and_resize when there are only 2 inputs.; Fixed additional cases of uncaught exceptions with the aarch64-android-clang6.0
platform;</p></td>
</tr>
<tr class="row-even"><td><p>1.27.0</p></td>
<td><p>May 2019</p></td>
<td><p>Added new APIs support for setting output tensor names to snpeBuilder and to fetch output tensor names for a given output layer name;
Improved the peak memory usage with DLC v3 format; Fixed few issues with performance and runtime failures on DSP runtime; Fixed few
issues and improved error handling for platform validator; Fixed the issues with Pooling and Instance norm layers of Tensorflow
converter; Removed <a href="#id5"><span class="problematic" id="id6">*</span></a>-android-gcc4.9 platform support. This compiler has been retired for the Android NDK, so all support is
transitioning to using Clang for Android; Removed arm-linux-gcc4.8hf platform. The development platform has been retired;</p></td>
</tr>
<tr class="row-odd"><td><p>1.26.0</p></td>
<td><p>Apr 2019</p></td>
<td><p>Added support for the ONNX Gather Op in the ONNX Converter and CPU runtime; Optimized DeConvolution Layer for the DSP runtime; Support
for tf.nn.moments in the TF converter, CPU and DSP runtimes; Added TF Reflect Pad support for the DSP runtime; Add symmetric quantizer
option in snpe-dlc-quantize; Add support for batch &gt; 1 when using the Scale Layer on the DSP runtime; Updated Platform Validator python
script to be OS-independent; Added additional optimizations for HTA input conversion;</p></td>
</tr>
<tr class="row-even"><td><p>1.25.0</p></td>
<td><p>Mar 2019</p></td>
<td><p>Updated DLC format to improve load time performance and memory consumption. Old DLCs will continue to work as is, but new DLCs
generated from 1.25 will use the new format; Added support for optimized; MultiClassNms and ArgMax ops on DSP runtime; Added option to
request larger memory allocations on the DSP for improved init time, at the expense of more memory use; Improved concurrency for
multiple; SNPE objects running simultaneously on DSP; Improvements when using priority control on DSP; Added support for channel
shuffle and ArgMax in the ONNX converter; Support multiple subnets within the AIP runtime;</p></td>
</tr>
<tr class="row-odd"><td><p>1.24.0</p></td>
<td><p>Feb 2019</p></td>
<td><p>Adding setProfilingLevel API support for AIP and CPU runtimes; Various stability issues on aip runtimes are addressed;Added support for
Snapdragon 712;Support multi inputs and multiple outputs on each SNPE AIP?s subnet</p></td>
</tr>
<tr class="row-even"><td><p>1.23.0</p></td>
<td><p>Jan 2019</p></td>
<td><p>Upgrade to Android NDK r17c to build SNPE; Improving initialization and de-initialization times; Various DSP timing fixes; Addressed
some DSP concurrency edge cases that could impact output values; TF converter support for non max suppression, crop and resize Ops</p></td>
</tr>
<tr class="row-odd"><td><p>1.22.0</p></td>
<td><p>Nov 2018</p></td>
<td><p>Support for several new ops on DSP runtime; Upgrade to Android NDK r16b to build SNPE; setProfilingLevel API support in DSP runtime;
Added new tool snpe-throughput-net-run</p></td>
</tr>
<tr class="row-even"><td><p>1.21.0</p></td>
<td><p>Oct 2018</p></td>
<td><p>Tensorflow converter and CPU runtime support for various ops; DSP runtime support for Eltwise Realdiv and Square ops; GPU support for
resize_align_corners layer</p></td>
</tr>
<tr class="row-odd"><td><p>1.20.0</p></td>
<td><p>Sep 2018</p></td>
<td><p>Support for QCS605 LE platform; NDK version upgrade to r14b; Tensorflow converter support for elementwise sqrt and softmax with
dimension &gt; 2; Platform validation command line tool</p></td>
</tr>
<tr class="row-even"><td><p>1.19.0</p></td>
<td><p>Aug 2018</p></td>
<td><p>ELU op support for Tensorflow/Onnx Converters and CPU/GPU runtimes; BoxWithNMSLimit and BBoxTransform ops support in caffe2 converter;
Support for Caffe Power Layer in GPU</p></td>
</tr>
<tr class="row-odd"><td><p>1.18.0</p></td>
<td><p>Jul 2018</p></td>
<td><p>Support for pad and elementwise subtraction on GPU; ONNX converter support for shape and pad ops; Tensorflow converter support for
additional ops</p></td>
</tr>
<tr class="row-even"><td><p>1.17.0</p></td>
<td><p>Jun 2018</p></td>
<td><p>Support for Scale Layer in Caffe converter and DSP runtime, DSP support for batch&gt;1 and ChannelShuffle, Updated SDK examples for
Inception v3 2016 model</p></td>
</tr>
<tr class="row-odd"><td><p>1.16.2</p></td>
<td><p>May 2018</p></td>
<td><p>Remove linkage to libstdc++.so in DSP loader libraries</p></td>
</tr>
<tr class="row-even"><td><p>1.16.1</p></td>
<td><p>May 2018</p></td>
<td><p>Remove linkage to libstdc++.so, DSP runtime fixes, fix for 1D BatchNorm</p></td>
</tr>
<tr class="row-odd"><td><p>1.16.0</p></td>
<td><p>May 2018</p></td>
<td><p>Batch&gt;1 support (except DSP runtime); layer optimizations for DSP runtime; Caffe2 ChannelShuffle support (except DSP runtime)</p></td>
</tr>
<tr class="row-even"><td><p>1.15.2</p></td>
<td><p>Mar 2018</p></td>
<td><p>Fix for GPU runtime memory leak and reshape to/from 1D</p></td>
</tr>
<tr class="row-odd"><td><p>1.15.1</p></td>
<td><p>Apr 2018</p></td>
<td><p>Fix for converter for instance normalization followed by scale</p></td>
</tr>
<tr class="row-even"><td><p>1.15.0</p></td>
<td><p>Apr 2018</p></td>
<td><p>Support for instance normalization for Caffe and Caffe2, MobilenetSSD (Caffe)</p></td>
</tr>
<tr class="row-odd"><td><p>1.14.1</p></td>
<td><p>Mar 2018</p></td>
<td><p>Minor fixes</p></td>
</tr>
<tr class="row-even"><td><p>1.14.0</p></td>
<td><p>Mar 2018</p></td>
<td><p>ONNX converter (alpha), multiple enhancements and fixes</p></td>
</tr>
<tr class="row-odd"><td><p>1.13.0</p></td>
<td><p>Feb 2018</p></td>
<td><p>GPU and DSP v65 performance improvements. GPU floating point 16 support.</p></td>
</tr>
<tr class="row-even"><td><p>1.12.0</p></td>
<td><p>Jan 2018</p></td>
<td><p>Support for Android LLVM/libc++, MobilenetSSD (TensorFlow)</p></td>
</tr>
<tr class="row-odd"><td><p>1.10.1</p></td>
<td><p>Dec 2017</p></td>
<td><p>Fix a bug in the DSP runtime when using mixed userbuffer input types</p></td>
</tr>
<tr class="row-even"><td><p>1.10.0</p></td>
<td><p>Dec 2017</p></td>
<td><p>Support for Mobilenet on DSP, enhanced DSP runtime, Snapdragon Flight Board, updates for UserBuffers</p></td>
</tr>
<tr class="row-odd"><td><p>1.8.0</p></td>
<td><p>Nov 2017</p></td>
<td><p>Mobilenet support on CPU, GPU, Support for Snapdragon 636 and Android 64 bit</p></td>
</tr>
<tr class="row-even"><td><p>1.6.0</p></td>
<td><p>Oct 2017</p></td>
<td><p>Support for Snapdragon 450, minor updates and fixes</p></td>
</tr>
<tr class="row-odd"><td><p>1.4.0</p></td>
<td><p>Aug 2017</p></td>
<td><p>Support for Snapdragon 630, FasterRCNN and ADSP on AGL</p></td>
</tr>
<tr class="row-even"><td><p>1.2.2</p></td>
<td><p>July 2017</p></td>
<td><p>QDN release</p></td>
</tr>
<tr class="row-odd"><td><p>1.2.0</p></td>
<td><p>June 2017</p></td>
<td><p>Beta Caffe2 Converter</p></td>
</tr>
<tr class="row-even"><td><p>1.0.2</p></td>
<td><p>May 2017</p></td>
<td><p>Support for 820AGL platform, Snapdragon 660, and Compute DSP on Android</p></td>
</tr>
<tr class="row-odd"><td><p>1.0.1</p></td>
<td><p>Apr 2017</p></td>
<td><p>Documentation update only</p></td>
</tr>
<tr class="row-even"><td><p>1.0</p></td>
<td><p>Apr 2017</p></td>
<td></td>
</tr>
</tbody>
</table>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="revision_history_windows.html" class="btn btn-neutral float-right" title="Revision History - Windows" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="../index.html" class="btn btn-neutral float-left" title="⌂" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020-2023, Qualcomm Technologies, Inc..

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>