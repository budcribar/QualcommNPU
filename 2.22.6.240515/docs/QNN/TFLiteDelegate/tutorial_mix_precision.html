

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Tutorial - Use Mix-Precision Model with Qualcomm® AI Engine Direct Delegate &mdash; Qualcomm® AI Engine Direct Delegate</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/custom_css.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="_static/doctools.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Tutorial - Profile Custom Models using Qualcomm® AI Engine Direct Delegate" href="tutorial_qtld_profiler.html" />
    <link rel="prev" title="Tutorial - Running Inference Using Shared Memory" href="tutorial_htp_shared_memory.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> Qualcomm® AI Engine Direct Delegate
          

          
          </a>

          
            
            
              <div class="version">
                v2.22.6
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="setup.html">Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="qnn_libs.html">Qualcomm® AI Engine Direct Backend Library</a></li>
<li class="toctree-l1"><a class="reference internal" href="support.html">Acceleration Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="tools.html">Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_op.html">Custom Operator Support</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="tutorials.html">Tutorials</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="tutorial_qtld_net_run.html">Tutorial - Running Inference Using the Qualcomm® AI Engine Direct Delegate</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_skip_node.html">Tutorial - Skip Delegation Ops Using the Qualcomm® AI Engine Direct Delegate</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_benchmark_model.html">Tutorial - Benchmarking the Qualcomm® AI Engine Direct Delegate</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_htp_shared_memory.html">Tutorial - Running Inference Using Shared Memory</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Tutorial - Use Mix-Precision Model with Qualcomm® AI Engine Direct Delegate</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#prerequisites">Prerequisites</a></li>
<li class="toctree-l3"><a class="reference internal" href="#running-mix-precision-model-with-qtld-net-run">Running mix-precision model with qtld-net-run</a></li>
<li class="toctree-l3"><a class="reference internal" href="#running-mix-precision-model-with-benchmarking">Running mix-precision model with benchmarking</a></li>
<li class="toctree-l3"><a class="reference internal" href="#experiments">Experiments</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#testing-environment">Testing environment</a></li>
<li class="toctree-l4"><a class="reference internal" href="#testing-the-top-k-accuracy">Testing the top-K accuracy</a></li>
<li class="toctree-l4"><a class="reference internal" href="#testing-the-benchmark">Testing the benchmark</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_qtld_profiler.html">Tutorial - Profile Custom Models using Qualcomm® AI Engine Direct Delegate</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_version_history.html">API Version History</a></li>
<li class="toctree-l1"><a class="reference internal" href="api.html">API</a></li>
<li class="toctree-l1"><a class="reference internal" href="glossary.html">Glossary</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Qualcomm® AI Engine Direct Delegate</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="tutorials.html">Tutorials</a> &raquo;</li>
        
      <li>Tutorial - Use Mix-Precision Model with Qualcomm® AI Engine Direct Delegate</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="tutorial-use-mix-precision-model-with-qualcomm-r-ai-engine-direct-delegate">
<h1>Tutorial - Use Mix-Precision Model with Qualcomm® AI Engine Direct Delegate<a class="headerlink" href="#tutorial-use-mix-precision-model-with-qualcomm-r-ai-engine-direct-delegate" title="Permalink to this heading">¶</a></h1>
<p>Floating-point models often result in more accurate predictions compared to quantization models.
Quantization models, on the other hand, can significantly reduce the model size and
computational requirements, resulting in lower run latency than the corresponding floating-point model.
To strike a balance between the high accuracy of floating-point models and
the computational efficiency of quantization models, we can employ the mix-precision models.
This approach offers a compromise by achieving a reasonable level of accuracy while optimizing computational efficiency.</p>
<p>This tutorial demonstrates how to use the mix-precision model in Qualcomm® AI Engine Direct Delegate.
Additionally, we demonstrate one case on MobileNet v3 that highlight the benefits of employing mix-precision models.</p>
<div class="section" id="prerequisites">
<h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this heading">¶</a></h2>
<p>The following list of prerequisites must be met before starting this tutorial:</p>
<ol class="arabic simple">
<li><p>To generate the mix-precision model, please refer to
<a class="reference external" href="https://www.tensorflow.org/lite/performance/quantization_debugger">Quantization Debugger</a> tutorial on the TenserFlow website.</p></li>
<li><p>Read the <a class="reference internal" href="tutorial_qtld_net_run.html"><span class="doc">Tutorial for qtld-net-run</span></a>, <a class="reference internal" href="tutorial_benchmark_model.html"><span class="doc">Tutorial for benchmark_model</span></a>
to understand how to run inference and benchmark using the Qualcomm® AI Engine Direct Delegate.</p></li>
</ol>
</div>
<div class="section" id="running-mix-precision-model-with-qtld-net-run">
<h2>Running mix-precision model with qtld-net-run<a class="headerlink" href="#running-mix-precision-model-with-qtld-net-run" title="Permalink to this heading">¶</a></h2>
<p><a class="reference internal" href="tutorial_qtld_net_run.html"><span class="doc">Tutorial for qtld-net-run</span></a> demonstrates how to run the TFLite model using the Qualcomm® AI Engine Direct Delegate on the HTP backend.</p>
<p>Set the <code class="docutils literal notranslate"><span class="pre">htp_precision=1</span></code> when using a mixed-precision model.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>adb<span class="w"> </span>shell<span class="w"> </span><span class="s2">&quot;LD_LIBRARY_PATH=/data/local/tmp/qnn_delegate/:</span><span class="nv">$LD_LIBRARY_PATH</span><span class="s2"> &amp;&amp;</span>
<span class="s2">      ADSP_LIBRARY_PATH=/data/local/tmp/qnn_delegate/ &amp;&amp;</span>
<span class="s2">      /data/local/tmp/qnn_delegate/qtld-net-run \</span>
<span class="s2">      --model=/data/local/tmp/qnn_delegate/mix_precision_model.tflite  \</span>
<span class="s2">      --input=/data/local/tmp/qnn_delegate/input_list.txt \</span>
<span class="s2">      --output=/data/local/tmp/qnn_delegate/tensor_dump_output \</span>
<span class="s2">      --htp_precision=1 \</span>
<span class="s2">      --backend=htp&quot;</span>
</pre></div>
</div>
</div>
<div class="section" id="running-mix-precision-model-with-benchmarking">
<h2>Running mix-precision model with benchmarking<a class="headerlink" href="#running-mix-precision-model-with-benchmarking" title="Permalink to this heading">¶</a></h2>
<p><a class="reference internal" href="tutorial_benchmark_model.html"><span class="doc">Tutorial for benchmark</span></a> demonstrates how to benchmark models
running through the Qualcomm® AI Engine Direct Delegate using the TFLite benchmark_model application.</p>
<p>Set the <code class="docutils literal notranslate"><span class="pre">htp_precision:1</span></code>, when using a mixed-precision model.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>adb<span class="w"> </span>shell<span class="w"> </span><span class="s1">&#39;export LD_LIBRARY_PATH=/data/local/tmp/qnn_delegate/:$LD_LIBRARY_PATH &amp;&amp;</span>
<span class="s1">             export ADSP_LIBRARY_PATH=&quot;/data/local/tmp/qnn_delegate/&quot; &amp;&amp;</span>
<span class="s1">             /data/local/tmp/qnn_delegate/benchmark_model \</span>
<span class="s1">             --graph=mix_precision_model.tflite \</span>
<span class="s1">             --external_delegate_path=/data/local/tmp/qnn_delegate/libQnnTFLiteDelegate.so \</span>
<span class="s1">             --external_delegate_options=&quot;backend_type:htp;htp_precision:1&quot;&#39;</span>
</pre></div>
</div>
</div>
<div class="section" id="experiments">
<h2>Experiments<a class="headerlink" href="#experiments" title="Permalink to this heading">¶</a></h2>
<p>Here, we use two experiments to show you that the mixed-precision model results in more accurate
predictions compared to the quantized model, and result in lower run latency than the corresponding floating-point model.</p>
<div class="section" id="testing-environment">
<h3>Testing environment<a class="headerlink" href="#testing-environment" title="Permalink to this heading">¶</a></h3>
<p>Here is our testing environment.</p>
<ul class="simple">
<li><dl class="simple">
<dt>Base Model: We download the MobileNet v3 model from Tenserflow Hub.</dt><dd><ul>
<li><p>MobileNet v3: <a class="reference external" href="https://tfhub.dev/google/imagenet/mobilenet_v3_large_100_224/classification/5">https://tfhub.dev/google/imagenet/mobilenet_v3_large_100_224/classification/5</a></p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Representative dataset:</dt><dd><ul>
<li><p>We use tensorflow dataset, imagenet_v2, first 100 pictures.</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Testing dataset:</dt><dd><ul>
<li><p>We use tensorflow dataset, imagenet_v2, first 1000 pictures.</p></li>
</ul>
</dd>
</dl>
</li>
<li><p>Testing device: We test the MobileNet v3 model with different precision on Snapdragon 8 Gen 1+.</p></li>
<li><p>TensorFlow version: v2.10.0</p></li>
</ul>
</div>
<div class="section" id="testing-the-top-k-accuracy">
<h3>Testing the top-K accuracy<a class="headerlink" href="#testing-the-top-k-accuracy" title="Permalink to this heading">¶</a></h3>
<p>We apply top-K accuracy to verify if the mixed-precision model offers better accuracy than the fully-quantized model.</p>
<p>Results show that the mixed-precision model has higher top-K accuracy.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 50%" />
<col style="width: 18%" />
<col style="width: 17%" />
<col style="width: 15%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"></th>
<th class="head"><p>MobileNet v3 (full float)</p></th>
<th class="head"><p>MobileNet v3 (full quantization)</p></th>
<th class="head"><p>MobileNet v3  (mix-precision)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1000 testing data, Top-1 accuracy, running on htp, delegated by Qualcomm® AI Engine Direct op</p></td>
<td><p>59.2%</p></td>
<td><p>15.7%</p></td>
<td><p><strong>51.3%</strong></p></td>
</tr>
<tr class="row-odd"><td><p>1000 testing data, Top-1 accuracy, running on cpu</p></td>
<td><p>59.3%</p></td>
<td><p>19.5%</p></td>
<td><p><strong>53.7%</strong></p></td>
</tr>
<tr class="row-even"><td><p>1000 testing data, Top-5 accuracy, running on htp, delegated by Qualcomm® AI Engine Direct op</p></td>
<td><p>83.2%</p></td>
<td><p>33.0%</p></td>
<td><p><strong>74.1%</strong></p></td>
</tr>
<tr class="row-odd"><td><p>1000 testing data, Top-5 accuracy, running on cpu</p></td>
<td><p>83.3%</p></td>
<td><p>37.8%</p></td>
<td><p><strong>76.4%</strong></p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="testing-the-benchmark">
<h3>Testing the benchmark<a class="headerlink" href="#testing-the-benchmark" title="Permalink to this heading">¶</a></h3>
<p>We employ the TFLite benchmark_model application used in <a class="reference internal" href="tutorial_benchmark_model.html"><span class="doc">this tutorial</span></a> to check if the mixed-precision model operates with less latency than the floating-point model.</p>
<p>Results show that the mixed-precision model runs with lower latency than the corresponding floating-point model.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 50%" />
<col style="width: 18%" />
<col style="width: 17%" />
<col style="width: 15%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Inference timings (in us)</p></th>
<th class="head"><p>MobileNet v3 (full float)</p></th>
<th class="head"><p>MobileNet v3 (full quantization)</p></th>
<th class="head"><p>MobileNet v3  (mix-precision)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Init</p></td>
<td><p>967562</p></td>
<td><p>576929</p></td>
<td><p><strong>606613</strong></p></td>
</tr>
<tr class="row-odd"><td><p>Inference (avg)</p></td>
<td><p>4384.77</p></td>
<td><p>2944.59</p></td>
<td><p><strong>3148.82</strong></p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this heading">¶</a></h2>
<p>On this page, we have attempted to infer the MobileNet v3 model with different precision. In the experiments, we can see that the mixed-precision model has higher top-K accuracy than the quantized model, and result in lower run latency than the corresponding floating-point model.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="tutorial_qtld_profiler.html" class="btn btn-neutral float-right" title="Tutorial - Profile Custom Models using Qualcomm® AI Engine Direct Delegate" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="tutorial_htp_shared_memory.html" class="btn btn-neutral float-left" title="Tutorial - Running Inference Using Shared Memory" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021-2023, Qualcomm Technologies, Inc..

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>