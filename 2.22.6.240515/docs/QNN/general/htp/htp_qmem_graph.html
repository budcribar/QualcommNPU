

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>QNN HTP Qmem Graph &mdash; Qualcomm® AI Engine Direct</title>
  

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/custom_css.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/collapsible-lists/css/tree_view.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/collapsible-lists/js/CollapsibleLists.compressed.js"></script>
        <script src="../../_static/collapsible-lists/js/apply-collapsible-lists.js"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="HTA" href="../hta/hta_backend.html" />
    <link rel="prev" title="SubSystem Restart (SSR)" href="htp_ssr.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> Qualcomm® AI Engine Direct
          

          
          </a>

          
            
            
              <div class="version">
                v2.22.6
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../setup.html">Setup</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../backend.html">Backend</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../backend.html#backend-specific-pages">Backend Specific Pages</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../dsp/dsp_backend.html">DSP</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="htp_backend.html">HTP</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="htp_backend.html#api-specializations">API Specializations</a></li>
<li class="toctree-l4"><a class="reference internal" href="htp_backend.html#usage-expectations">Usage Expectations</a></li>
<li class="toctree-l4"><a class="reference internal" href="htp_backend.html#qnn-htp-supported-operations">QNN HTP Supported Operations</a></li>
<li class="toctree-l4"><a class="reference internal" href="htp_backend.html#qnn-htp-variable-batch">QNN HTP Variable Batch</a></li>
<li class="toctree-l4"><a class="reference internal" href="htp_backend.html#qnn-htp-backend-api">QNN HTP Backend API</a></li>
<li class="toctree-l4"><a class="reference internal" href="htp_backend.html#qnn-htp-performance-infrastructure-api">QNN HTP Performance Infrastructure API</a></li>
<li class="toctree-l4"><a class="reference internal" href="htp_backend.html#qnn-htp-precision">QNN HTP Precision</a></li>
<li class="toctree-l4"><a class="reference internal" href="htp_backend.html#qnn-htp-fp16-output-difference-between-sm8550-and-sm8650">QNN HTP FP16 output difference between SM8550 and SM8650</a></li>
<li class="toctree-l4"><a class="reference internal" href="htp_backend.html#qnn-htp-deep-learning-bandwidth-compression-dlbc">QNN HTP Deep Learning Bandwidth Compression (DLBC)</a></li>
<li class="toctree-l4"><a class="reference internal" href="htp_backend.html#qnn-htp-setting-number-of-hvx-threads">QNN HTP - Setting Number of HVX Threads</a></li>
<li class="toctree-l4"><a class="reference internal" href="htp_backend.html#qnn-htp-backend-extensions">QNN HTP Backend Extensions</a></li>
<li class="toctree-l4"><a class="reference internal" href="htp_backend.html#qnn-htp-profiling">QNN HTP Profiling</a></li>
<li class="toctree-l4"><a class="reference internal" href="htp_backend.html#qnn-context-binary-size">QNN Context Binary size</a></li>
<li class="toctree-l4"><a class="reference internal" href="htp_backend.html#op-writing-guidelines">Op Writing Guidelines</a></li>
<li class="toctree-l4"><a class="reference internal" href="htp_backend.html#recommendations-for-network-design">Recommendations for Network Design</a></li>
<li class="toctree-l4"><a class="reference internal" href="htp_backend.html#yielding-and-pre-emption">Yielding and Pre-Emption</a></li>
<li class="toctree-l4"><a class="reference internal" href="htp_backend.html#vtcm-sharing">VTCM Sharing</a></li>
<li class="toctree-l4"><a class="reference internal" href="htp_backend.html#subsystem-restart-ssr">SubSystem Restart (SSR)</a></li>
<li class="toctree-l4 current"><a class="reference internal" href="htp_backend.html#qmem-graph-shared-buffer-only-graph">Qmem Graph (shared_buffer only graph)</a></li>
<li class="toctree-l4"><a class="reference internal" href="htp_backend.html#htp-session-artifact-usage-guidlines">HTP Session &amp; Artifact Usage Guidlines</a></li>
<li class="toctree-l4"><a class="reference internal" href="htp_backend.html#graph-switching-beta">Graph Switching (Beta)</a></li>
<li class="toctree-l4"><a class="reference internal" href="htp_backend.html#benefits-of-batch-inference-and-multi-threaded-inference">Benefits of batch inference and multi-threaded inference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../hta/hta_backend.html">HTA</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lpai/lpai_backend.html">LPAI</a></li>
<li class="toctree-l3"><a class="reference internal" href="../cpu/cpu_backend.html">CPU</a></li>
<li class="toctree-l3"><a class="reference internal" href="../gpu/gpu_backend.html">GPU</a></li>
<li class="toctree-l3"><a class="reference internal" href="../saver/saver_backend.html">Saver</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../op_packages.html">Op Packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tools.html">Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../converters.html">Converters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmarking.html">Benchmarking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operations.html">Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api.html">API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../glossary.html">Glossary</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Qualcomm® AI Engine Direct</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../backend.html">Backend</a> &raquo;</li>
        
          <li><a href="htp_backend.html">HTP</a> &raquo;</li>
        
      <li>QNN HTP Qmem Graph</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="qnn-htp-qmem-graph">
<h1>QNN HTP Qmem Graph<a class="headerlink" href="#qnn-htp-qmem-graph" title="Permalink to this heading">¶</a></h1>
<p>Currently QnnGraph supports inferences for RAW buffers and MemHandles.
Raw buffers are not accessible from DSP side and at Graph creation, QNN HTP reserves extra RPCMem buffers to copy RAW buffers before inference (copy out output buffers after inference)
If the client uses MemHandles (handles to buffers allocated using rpcmem_alloc and accessible from DSP side) no copy is needed and those internal buffers are not used.
Qmem Graph allows the client to pass a hint at graph preparation that this use case will be using RPCMem buffers and there is no need to allocate internal extra RPCmem memory.
If the client uses Qmem Graph hint at graph creation time and still passes RAW buffers at inference time, QNN HTP will allocate extra buffers at run time (expected performance impact for that inference).</p>
<div class="section" id="online-prepare">
<h2>Online Prepare<a class="headerlink" href="#online-prepare" title="Permalink to this heading">¶</a></h2>
<p><strong>Preparation</strong></p>
<p>When doing online prepare, the hint (IO tensor memory type) that informs QnnHtp backend to reduce memory allocation is embedded in model.so file.
This information can be passed in by <code class="code docutils literal notranslate"><span class="pre">QnnTensor_createGraphTensor</span></code> and <code class="code docutils literal notranslate"><span class="pre">QnnGraph_addNode</span></code>.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="c1">// example graph. for detail, please refer to Sample App</span>
<span class="linenos"> 2</span><span class="n">Qnn_GraphHandle_t</span><span class="w"> </span><span class="n">graph</span><span class="p">;</span>
<span class="linenos"> 3</span><span class="c1">// IO tensors</span>
<span class="linenos"> 4</span><span class="n">Qnn_Tensor_t</span><span class="w"> </span><span class="n">inputTensor</span><span class="p">;</span>
<span class="linenos"> 5</span><span class="n">Qnn_Tensor_t</span><span class="w"> </span><span class="n">outputTensor</span><span class="p">;</span>
<span class="linenos"> 6</span><span class="c1">// Set up common setting for tensors ......</span>
<span class="linenos"> 7</span><span class="cm">/* There are 2 specific settings for shared buffer:</span>
<span class="linenos"> 8</span><span class="cm">*  1. memType should be QNN_TENSORMEMTYPE_MEMHANDLE;</span>
<span class="linenos"> 9</span><span class="cm">*  2. union member memHandle should be used instead of clientBuf, and it</span>
<span class="linenos">10</span><span class="cm">*  should be set to nullptr.</span>
<span class="linenos">11</span><span class="cm">*/</span>
<span class="linenos">12</span><span class="n">inputTensor</span><span class="p">.</span><span class="n">v1</span><span class="p">.</span><span class="n">memType</span><span class="w">        </span><span class="o">=</span><span class="w"> </span><span class="n">QNN_TENSORMEMTYPE_MEMHANDLE</span><span class="p">;</span>
<span class="linenos">13</span><span class="n">inputTensor</span><span class="p">.</span><span class="n">v1</span><span class="p">.</span><span class="n">clientBuf</span><span class="w">      </span><span class="o">=</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="linenos">14</span><span class="n">outputTensor</span><span class="p">.</span><span class="n">v1</span><span class="p">.</span><span class="n">memType</span><span class="w">       </span><span class="o">=</span><span class="w"> </span><span class="n">QNN_TENSORMEMTYPE_MEMHANDLE</span><span class="p">;</span>
<span class="linenos">15</span><span class="n">outputTensor</span><span class="p">.</span><span class="n">v1</span><span class="p">.</span><span class="n">clientBuf</span><span class="w">     </span><span class="o">=</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="linenos">16</span><span class="n">QnnTensor_createGraphTensor</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">inputTensor</span><span class="p">);</span>
<span class="linenos">17</span><span class="n">QnnTensor_createGraphTensor</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputTensor</span><span class="p">);</span>
<span class="linenos">18</span>
<span class="linenos">19</span><span class="c1">// create OpConfig_t with IO tensor just created</span>
<span class="linenos">20</span><span class="n">Qnn_OpConfig_t</span><span class="w"> </span><span class="n">opConfig</span><span class="p">;</span>
<span class="linenos">21</span><span class="n">QnnGraph_addNode</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span><span class="w"> </span><span class="n">opConfit</span><span class="p">);</span>
</pre></div>
</div>
<p><strong>Execution</strong></p>
<p>Please refer to: <a class="reference internal" href="htp_shared_buffer_tutorial.html#shared-buffer-tutorial"><span class="std std-ref">QNN HTP Shared Buffer Tutorial</span></a></p>
</div>
<div class="section" id="offline-prepare">
<h2>Offline Prepare<a class="headerlink" href="#offline-prepare" title="Permalink to this heading">¶</a></h2>
<p>When generating serialized.bin, it is recommended to generate serialized.bin with option <code class="code docutils literal notranslate"><span class="pre">--input_output_tensor_mem_type</span> <span class="pre">memhandle</span></code> to reduce the memory
footprint. With this option used, qnn-context-binary-generator will change IO tensor memory type to memhandle. When QnnHtp backend loads serialized.bin, it will
be able to skip memory allocation for IO tensor and understand that the user intends to use shared_buffer during execution.
Skipping this option will not impact inference performance.</p>
<p><strong>Preparation</strong></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span>//<span class="w"> </span>Prerequisites:<span class="w"> </span>model.so,<span class="w"> </span>qnn-context-binary-generator,<span class="w"> </span>QnnHtp<span class="w"> </span>backend<span class="w"> </span>.so<span class="w"> </span>library
<span class="linenos">2</span>
<span class="linenos">3</span>./qnn-context-binary-generator<span class="w"> </span>--model<span class="w"> </span>libqnn_model.so<span class="w"> </span>--backend<span class="w"> </span>libQnnHtp.so<span class="w"> </span>--binary_file<span class="w"> </span>qnngraph.serialized<span class="w"> </span>--output_dir<span class="w"> </span>output<span class="w"> </span>--input_output_tensor_mem_type<span class="w"> </span>memhandle
<span class="linenos">4</span>
<span class="linenos">5</span>//<span class="w"> </span>qnngraph.serialized.bin<span class="w"> </span>is<span class="w"> </span>generated<span class="w"> </span>and<span class="w"> </span>saved<span class="w"> </span>at<span class="w"> </span>output/qnngraph.serialized.bin
</pre></div>
</div>
<p><strong>Execution</strong></p>
<p>Please refer to: <a class="reference internal" href="htp_shared_buffer_tutorial.html#shared-buffer-tutorial"><span class="std std-ref">QNN HTP Shared Buffer Tutorial</span></a></p>
<p><strong>Mis-matching mem_type during preparation and execution</strong></p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Preparation</p></th>
<th class="head"><p>Execution</p></th>
<th class="head"><p>Behavior</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">raw</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">raw</span></code></p></td>
<td><ul class="simple">
<li><p>QNN HTP will allocate memory for IO buffer.</p></li>
<li><p>HTP will copy input and output at each inference.</p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">raw</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">memhandle</span></code></p></td>
<td><ul class="simple">
<li><p>QNN HTP will allocate memory for IO buffer.</p></li>
<li><p>Data copy avoided.</p></li>
</ul>
</td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">memhandle</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">raw</span></code></p></td>
<td><ul class="simple">
<li><p>QNN HTP will not allocate memory for IO buffer during preparation.</p></li>
<li><p>QNN HTP will allocate memory for IO buffer during first inference (<code class="docutils literal notranslate"><span class="pre">raw</span></code> passed in during execution), first inference time impact by memory allocation.</p></li>
<li><p>HTP will copy input and output at each inference.</p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">memhandle</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">memhandle</span></code></p></td>
<td><ul class="simple">
<li><p>QNN HTP will not allocate memory for IO buffer during preparation.</p></li>
<li><p>Data copy avoided.</p></li>
</ul>
</td>
</tr>
</tbody>
</table>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="../hta/hta_backend.html" class="btn btn-neutral float-right" title="HTA" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="htp_ssr.html" class="btn btn-neutral float-left" title="SubSystem Restart (SSR)" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020-2024, Qualcomm Technologies, Inc..

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>