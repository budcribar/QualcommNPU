

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Quantization &mdash; Qualcomm® AI Engine Direct</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/custom_css.css" type="text/css" />
  <link rel="stylesheet" href="../_static/collapsible-lists/css/tree_view.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/collapsible-lists/js/CollapsibleLists.compressed.js"></script>
        <script src="../_static/collapsible-lists/js/apply-collapsible-lists.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Tutorials" href="tutorials.html" />
    <link rel="prev" title="Converters" href="converters.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> Qualcomm® AI Engine Direct
          

          
          </a>

          
            
            
              <div class="version">
                v2.22.6
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="setup.html">Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="backend.html">Backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="op_packages.html">Op Packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="tools.html">Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="converters.html">Converters</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Quantization</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id1">Quantization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id2">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#details">Details</a></li>
<li class="toctree-l3"><a class="reference internal" href="#quantization-example">Quantization Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dequantization-example">Dequantization Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="#bitwidth-selection">Bitwidth Selection</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#quantization-modes">Quantization Modes</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#tf">TF</a></li>
<li class="toctree-l3"><a class="reference internal" href="#symmetric">Symmetric</a></li>
<li class="toctree-l3"><a class="reference internal" href="#enhanced">Enhanced</a></li>
<li class="toctree-l3"><a class="reference internal" href="#tf-adjusted">TF Adjusted</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#enhanced-quantization-techniques">Enhanced Quantization Techniques</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#enhanced-quantization-techniques-limitations">Enhanced Quantization Techniques: Limitations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#quantization-impacts">Quantization Impacts</a></li>
<li class="toctree-l2"><a class="reference internal" href="#quantization-overrides">Quantization Overrides</a></li>
<li class="toctree-l2"><a class="reference internal" href="#per-channel-quantization-overrides">Per-channel Quantization Overrides</a></li>
<li class="toctree-l2"><a class="reference internal" href="#quantizing-a-model">Quantizing a Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#examples">Examples</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#mixed-precision-and-fp16-support">Mixed Precision and FP16 Support</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#non-quantized-mode">Non-quantized Mode</a></li>
<li class="toctree-l3"><a class="reference internal" href="#quantized-mode">Quantized Mode</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#qairt-quantizer">qairt-quantizer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#additional-details">Additional details</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="tutorials.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmarking.html">Benchmarking</a></li>
<li class="toctree-l1"><a class="reference internal" href="operations.html">Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="api.html">API</a></li>
<li class="toctree-l1"><a class="reference internal" href="glossary.html">Glossary</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Qualcomm® AI Engine Direct</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Quantization</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="quantization">
<h1>Quantization<a class="headerlink" href="#quantization" title="Permalink to this heading">¶</a></h1>
<p>This page describes the general quantization process and supported algorithms and features.</p>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#overview" id="id3">Overview</a></p></li>
<li><p><a class="reference internal" href="#id1" id="id4">Quantization</a></p>
<ul>
<li><p><a class="reference internal" href="#id2" id="id5">Overview</a></p></li>
<li><p><a class="reference internal" href="#details" id="id6">Details</a></p></li>
<li><p><a class="reference internal" href="#quantization-example" id="id7">Quantization Example</a></p></li>
<li><p><a class="reference internal" href="#dequantization-example" id="id8">Dequantization Example</a></p></li>
<li><p><a class="reference internal" href="#bitwidth-selection" id="id9">Bitwidth Selection</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#quantization-modes" id="id10">Quantization Modes</a></p>
<ul>
<li><p><a class="reference internal" href="#tf" id="id11">TF</a></p></li>
<li><p><a class="reference internal" href="#symmetric" id="id12">Symmetric</a></p></li>
<li><p><a class="reference internal" href="#enhanced" id="id13">Enhanced</a></p></li>
<li><p><a class="reference internal" href="#tf-adjusted" id="id14">TF Adjusted</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#enhanced-quantization-techniques" id="id15">Enhanced Quantization Techniques</a></p>
<ul>
<li><p><a class="reference internal" href="#enhanced-quantization-techniques-limitations" id="id16">Enhanced Quantization Techniques: Limitations</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#quantization-impacts" id="id17">Quantization Impacts</a></p></li>
<li><p><a class="reference internal" href="#quantization-overrides" id="id18">Quantization Overrides</a></p></li>
<li><p><a class="reference internal" href="#per-channel-quantization-overrides" id="id19">Per-channel Quantization Overrides</a></p></li>
<li><p><a class="reference internal" href="#quantizing-a-model" id="id20">Quantizing a Model</a></p>
<ul>
<li><p><a class="reference internal" href="#examples" id="id21">Examples</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#mixed-precision-and-fp16-support" id="id22">Mixed Precision and FP16 Support</a></p>
<ul>
<li><p><a class="reference internal" href="#non-quantized-mode" id="id23">Non-quantized Mode</a></p></li>
<li><p><a class="reference internal" href="#quantized-mode" id="id24">Quantized Mode</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#qairt-quantizer" id="id25">qairt-quantizer</a></p>
<ul>
<li><p><a class="reference internal" href="#additional-details" id="id26">Additional details</a></p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="overview">
<h2><a class="toc-backref" href="#id3">Overview</a><a class="headerlink" href="#overview" title="Permalink to this heading">¶</a></h2>
<p>Non-quantized models files use 32 bit floating point representations of network parameters.
Quantized model files use fixed point representations of network parameters, generally 8 bit weights and 8 or 32bit biases.
The fixed point representation is the same used in Tensorflow quantized models.</p>
<p>Choosing Between a Quantized or Non-Quantized Model</p>
<ul class="simple">
<li><p>CPU - Choose a non-quantized model. Quantized models are currently incompatible with the CPU backend.</p></li>
<li><p>DSP - Choose a quantized model. Quantized models are required when running on the DSP backend.</p></li>
<li><p>GPU - Choose a non-quantized model. Quantized models are currently incompatible with the GPU backend.</p></li>
<li><p>HTP - Choose a quantized model. Quantized models are required when running on the HTP backend.</p></li>
<li><p>HTA - Choose a quantized model. Quantized models are required when running on the HTA backend.</p></li>
</ul>
</div>
<div class="section" id="id1">
<h2><a class="toc-backref" href="#id4">Quantization</a><a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h2>
<p>This section describes the concepts behind the quantization algorithm used in QNN.  These concepts are used by the converters when the developer decides to quantize a graph.</p>
<div class="section" id="id2">
<h3><a class="toc-backref" href="#id5">Overview</a><a class="headerlink" href="#id2" title="Permalink to this heading">¶</a></h3>
<p>QNN supports multiple quantization modes.  The basics of the quantization, regardless of mode, are described here.</p>
<ul>
<li><p>Quantization converts floating point data to the Tensorflow-style fixed point format using a provided bit width.</p></li>
<li><p>The following requirements are satisfied:</p>
<ul class="simple">
<li><p>Full range of input values is covered.</p></li>
<li><p>Minimum range of 0.0001 is enforced.</p></li>
<li><p>Floating point zero is exactly representable.</p></li>
</ul>
</li>
<li><p>Quantization algorithm inputs:</p>
<ul class="simple">
<li><p>Set of floating point values to be quantized.</p></li>
</ul>
</li>
<li><p>Quantization algorithm outputs:</p>
<ul class="simple">
<li><p>Set of 8-bit fixed point values.</p></li>
<li><p>Encoding parameters:</p>
<ul>
<li><p>encoding-min - minimum floating point value representable (by fixed point value 0)</p></li>
<li><p>encoding-max - maximum floating point value representable (by fixed point value 255)</p></li>
<li><p>scale - The step size for the given range (max - min) / (2^bw-1)</p></li>
<li><p>offset - The integer value which exactly represents 0. round(-min/scale)</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Algorithm</p>
<ol class="arabic simple">
<li><p>Compute the true range (min, max) of input data.</p></li>
<li><p>Compute the encoding-min and encoding-max.</p></li>
<li><p>Quantize the input floating point values.</p></li>
<li><p>Output:</p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p>fixed point values</p></li>
<li><p>encoding-min and encoding-max parameters</p></li>
<li><p>scale and offset parameters</p></li>
</ul>
</div></blockquote>
</li>
</ul>
</div>
<div class="section" id="details">
<h3><a class="toc-backref" href="#id6">Details</a><a class="headerlink" href="#details" title="Permalink to this heading">¶</a></h3>
<ol class="arabic simple">
<li><p>Compute the true range of the input floating point data.</p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p>finds the smallest and largest values in the input data</p></li>
<li><p>represents the true range of the input data</p></li>
</ul>
</div></blockquote>
<ol class="arabic simple" start="2">
<li><p>Compute the encoding-min and encoding-max.</p></li>
</ol>
<blockquote>
<div><ul>
<li><p>These parameters are used in the quantization step.</p></li>
<li><p>These parameters define the range and floating point values that will be representable by the fixed point format.</p>
<ul class="simple">
<li><p>encoding-min: specifies the smallest floating point value that will be represented by the fixed point value of 0</p></li>
<li><p>encoding-max: specifies the largest floating point value that will be represented by the fixed point value of 255</p></li>
<li><p>floating point values at every step size, where step size = (encoding-max - encoding-min) / (2^bw-1), will be representable</p></li>
<li><p>offset where zero is exactly represented</p></li>
</ul>
</li>
<li><p>encoding-min and encoding-max are first set to the true min and true max computed in the previous step</p></li>
<li><p>Requirements</p>
<ol class="arabic simple">
<li><p>Encoding range must be at least a minimum of 0.0001</p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p>encoding-max is adjusted to max(true max, true min + 0.0001)</p></li>
</ul>
</div></blockquote>
<ol class="arabic simple" start="2">
<li><p>Floating point value of 0 must be exactly representable</p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p>encoding-min or encoding-max may be further adjusted</p></li>
</ul>
</div></blockquote>
</li>
</ul>
</div></blockquote>
<ol class="arabic simple" start="3">
<li><p>Cases - Handling 0</p></li>
</ol>
<blockquote>
<div><ol class="arabic simple">
<li><p>Inputs are strictly positive</p>
<ul class="simple">
<li><p>the encoding-min is set to 0.0</p></li>
<li><p>zero floating point value is exactly representable by smallest fixed point value 0</p></li>
<li><p>e.g. input range = [5.0, 10.0]</p>
<ul>
<li><p>encoding-min = 0.0, encoding-max = 10.0</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Inputs are strictly negative</p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p>encoding-max is set to 0.0</p></li>
<li><p>zero floating point value is exactly representable by the largest fixed point value 255</p></li>
<li><p>e.g. input range = [-20.0, -6.0]</p>
<ul>
<li><p>encoding-min = -20.0, encoding-max = 0.0</p></li>
</ul>
</li>
</ul>
</div></blockquote>
<ol class="arabic simple" start="3">
<li><p>Inputs are both negative and positive</p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p>encoding-min and encoding-max are slightly shifted to make the floating point zero exactly representable</p></li>
<li><p>e.g. input range = [-5.1, 5.1]</p>
<ul>
<li><p>encoding-min and encoding-max are first set to -5.1 and 5.1, respectively</p></li>
<li><p>encoding range is 10.2 and the step size is 10.2/255 = 0.04</p></li>
<li><p>zero value is currently not representable. The closest values representable are -0.02 and +0.02 by fixed point values 127 and 128, respectively</p></li>
<li><p>encoding-min and encoding-max are shifted by -0.02. The new encoding-min is -5.12 and the new encoding-max is 5.08</p></li>
<li><p>floating point zero is now exactly representable by the fixed point value of 128</p></li>
</ul>
</li>
</ul>
</div></blockquote>
</div></blockquote>
<ol class="arabic simple" start="4">
<li><p>Quantize the input floating point values.</p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p>encoding-min and encoding-max parameter determined in the previous step are used to quantize all the input floating values to their fixed point representation</p></li>
<li><p>Quantization formula is:</p>
<ul>
<li><p>quantized value = round(255 * (floating point value - encoding.min) / (encoding.max - encoding.min))</p></li>
</ul>
</li>
<li><p>quantized value is also clamped to be within 0 and 2^bw-1</p></li>
</ul>
</div></blockquote>
<ol class="arabic simple" start="5">
<li><p>Outputs</p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p>the fixed point values</p></li>
<li><p>encoding-min, encoding-max, scale, and offset  parameters</p></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="quantization-example">
<h3><a class="toc-backref" href="#id7">Quantization Example</a><a class="headerlink" href="#quantization-example" title="Permalink to this heading">¶</a></h3>
<ol class="arabic simple">
<li><p>Inputs:</p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p>input values = [-1.8, -1.0, 0, 0.5]</p></li>
<li><p>encoding-min is set to -1.8 and encoding-max to 0.5</p></li>
<li><p>encoding range is 2.3, which is larger than the required 0.0001</p></li>
<li><p>encoding-min is adjusted to −1.803922 and encoding-max to 0.496078 to make zero exactly representable</p></li>
<li><p>step size is 0.009020</p></li>
<li><p>offset is 200</p></li>
</ul>
</div></blockquote>
<ol class="arabic simple" start="2">
<li><p>Outputs:</p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p>quantized values are [0, 89, 200, 255]</p></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="dequantization-example">
<h3><a class="toc-backref" href="#id8">Dequantization Example</a><a class="headerlink" href="#dequantization-example" title="Permalink to this heading">¶</a></h3>
<ol class="arabic simple">
<li><p>Inputs:</p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p>quantized values = [0, 89, 200, 255]</p></li>
<li><p>encoding-min = −1.803922, encoding-max = 0.496078</p></li>
<li><p>step size is 0.009020</p></li>
<li><p>offset is 200</p></li>
</ul>
</div></blockquote>
<ol class="arabic simple" start="2">
<li><p>Outputs:</p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p>dequantized values = [−1.8039, −1.0011, 0.0000, 0.4961]</p></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="bitwidth-selection">
<h3><a class="toc-backref" href="#id9">Bitwidth Selection</a><a class="headerlink" href="#bitwidth-selection" title="Permalink to this heading">¶</a></h3>
<p>QNN currently supports a default quantization bit width of 8 for both weights and biases. The weight, bias, and activation bit widths,
however, can be overriden by passing one of –weight_bw, –bias_bw, and/or –act_bw followed by the bitwidth. Please see the
converter documentation <a class="reference internal" href="tools.html"><span class="doc">here</span></a> for more details on the command line options.</p>
</div>
</div>
<div class="section" id="quantization-modes">
<h2><a class="toc-backref" href="#id10">Quantization Modes</a><a class="headerlink" href="#quantization-modes" title="Permalink to this heading">¶</a></h2>
<p>QNN supports four quantization modes: tf, symmetric, enhanced, and adjusted. The primary difference is between how they select the quantization range to be used.</p>
<div class="section" id="tf">
<h3><a class="toc-backref" href="#id11">TF</a><a class="headerlink" href="#tf" title="Permalink to this heading">¶</a></h3>
<p>The default mode has been described above, and uses the true min/max of the data being quantized, followed by an adjustment of the range to ensure a minimum range and to ensure 0.0 is exactly quantizable.</p>
</div>
<div class="section" id="symmetric">
<h3><a class="toc-backref" href="#id12">Symmetric</a><a class="headerlink" href="#symmetric" title="Permalink to this heading">¶</a></h3>
<p>Symmetric quantization follows the same basic principles as TF quantization but adjusted the range to be symmetric. It does this by selecting a new min and max from the original
range such that new_max=max(abs(min), abs(max)) and adjusts the range to be (-new_max, max) such that the range is symmetric around 0. This is typically only used for weights as it helps
to reduce computation overhead at runtime. This mode is enabled by passing –param_quantizer symmetric  to one of the <a class="reference internal" href="tools.html"><span class="doc">converters</span></a>.</p>
</div>
<div class="section" id="enhanced">
<h3><a class="toc-backref" href="#id13">Enhanced</a><a class="headerlink" href="#enhanced" title="Permalink to this heading">¶</a></h3>
<p>Enhanced quantization mode (invoked by passing “enhanced” to either the <code class="docutils literal notranslate"><span class="pre">--param_quantizer</span></code> or <code class="docutils literal notranslate"><span class="pre">--act_quantizer</span></code> options in one of the <a class="reference internal" href="tools.html"><span class="doc">converters</span></a>) uses an algorithm to try to determine a better
set of quantization parameters to improve accuracy.   The algorithm may pick a different min/max value than the default quantizer, and in some cases it may set
the range such that some of the original weights and/or activations cannot fall into that range.  However, this range does produce better accuracy than simply using
the true min/max. The enhanced quantizer can be enabled independently for weights and activations by appending either “weights” or “activations” after the option.</p>
<p>This is useful for some models where the weights and/or activations may have “long tails”. (Imagine a range with most values between -100 and 1000, but a few values much greater than 1000 or much less than -100.)  In some cases these long tails can be ignored and the range -100, 1000 can be used more effectively than the full range.</p>
<p>Enhanced quantizer still enforces a minimum range and ensures 0.0 is exactly quantizable.</p>
</div>
<div class="section" id="tf-adjusted">
<h3><a class="toc-backref" href="#id14">TF Adjusted</a><a class="headerlink" href="#tf-adjusted" title="Permalink to this heading">¶</a></h3>
<p>This mode is used only for quantizing weights to 8 bit fixed point (invoked by passing “adjusted” to either the <code class="docutils literal notranslate"><span class="pre">--param_quantizer</span></code> or <code class="docutils literal notranslate"><span class="pre">--act_quantizer</span></code> options in one of the <a class="reference internal" href="tools.html"><span class="doc">converters</span></a>)
to, which uses adjusted min or max of the data being quantized other than true min/max or the min/max
that exclude the long tail. This has been verified to be able to provide accuracy benefit for denoise model specifically. Using this
quantizer, the max will be expanded or the min will be decreased if necessary.</p>
<p>Adjusted weights quantizer still enforces a minimum range and ensures 0.0 is exactly quantizable.</p>
</div>
</div>
<div class="section" id="enhanced-quantization-techniques">
<h2><a class="toc-backref" href="#id15">Enhanced Quantization Techniques</a><a class="headerlink" href="#enhanced-quantization-techniques" title="Permalink to this heading">¶</a></h2>
<p>Quantization can be a difficult problem to solve due to the myriad of training techniques, model architectures, and layer types. In an attempt to mitigate quantization problems
model preprocessing techniques have been added to the quantizer that may improve quantization performance on models which exhibit sharp drops in accuracy upon quantization.</p>
<p>The primary technique introduced is CLE (Cross Layer Equalization).</p>
<p>CLE works by scaling the convolution weight ranges in the network by making use of a scale-equivariance property of activation functions. In addition, the process absorbs high biases which may be result from weight scaling from one convolution layer to a subsequent convolution layer.</p>
<div class="section" id="enhanced-quantization-techniques-limitations">
<h3><a class="toc-backref" href="#id16">Enhanced Quantization Techniques: Limitations</a><a class="headerlink" href="#enhanced-quantization-techniques-limitations" title="Permalink to this heading">¶</a></h3>
<p>In many cases CLE may enable quantized models to return to close to their original floating-point accuracy. There are some caveats/limitations to the current algorithms:</p>
<blockquote>
<div><p>CLE operates on specific patterns of operations that all exist in a single branch (outputs cannot be consumed by more than one op). The matched operation patterns (r=required, o=optional) are:</p>
<blockquote>
<div><p>Conv(r)-&gt;Batchnorm(r)-&gt;activation(o)-&gt;Conv(r)-&gt;Batchnorm(r)-&gt;activation(o)
Conv(r)-&gt;Batchnorm(r)-&gt;activation(o)-&gt;DepthwiseConv(r)-&gt;Batchnorm(r)-&gt;activation(o)-&gt;Conv(r)-&gt;Batchnorm(r)-&gt;activation(o)</p>
</div></blockquote>
<p>The CLE algorithm currently only supports Relu activations. Any Relu6 activations will be automatically changed to Relu and any activations other than
these will cause the algorithm to ignore the preceding convolution. Typically the switch from Relu6-&gt;Relu is harmless and does not cause any degradation in accuracy, however some
models may exhibit a slight degradation of accuracy. In this case, CLE can only recover accuracy to that degraded level, and not to the original float
accuracy.
CLE requires batchnorms (specifically detectable batchnorm beta/gamma data) be present in the original model before conversion to DLC for the complete
algorithm to be run and to regain maximum accuracy. For Tensorflow, the beta and gamma can sometimes still be found even with folded batchnorms, so long as
the folding didn’t fold the parameters into the convolution’s static weights and bias. If it does not detect the required information you may see a message
that looks like: “Invalid model for HBA quantization algorithm.” This indicates the algorithm will only partially run and accuracy issues may likely be present.</p>
</div></blockquote>
<p>To run CLE simply add the option –algorithms cle to the converter command line.</p>
<p>More information about the algorithms can be found here: [<a class="reference external" href="https://arxiv.org/abs/1906.04721">https://arxiv.org/abs/1906.04721</a>]</p>
</div>
</div>
<div class="section" id="quantization-impacts">
<h2><a class="toc-backref" href="#id17">Quantization Impacts</a><a class="headerlink" href="#quantization-impacts" title="Permalink to this heading">¶</a></h2>
<p>Quantizing a model and/or running it in a quantized runtime (like the HTP) can affect accuracy.  Some models may not work well when quantized, and may yield incorrect results.
The metrics for measuring impact of quantization on a model that does classification are typically “Mean Average Precision”, “Top-1 Error” and “Top-5 Error”.</p>
</div>
<div class="section" id="quantization-overrides">
<h2><a class="toc-backref" href="#id18">Quantization Overrides</a><a class="headerlink" href="#quantization-overrides" title="Permalink to this heading">¶</a></h2>
<p>If the option –quantization_overrides is provided the user may provide a json file with parameters to use for quantization. These will override any
quantization data carried from conversion (eg TF fake quantization) or calculated during the normal quantization process. Format defined as per AIMET specification.</p>
<p>There are two sections in the json, a section for overriding operator output encodings called “activation_encodings” and a section for overriding parameter (weight and bias) encodings
called “param_encodings”. Both must be present in the file, but can be empty if no overrides are desired. An example with all of the currently supported options:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">   </span><span class="s">&quot;activation_encodings&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">       </span><span class="s">&quot;Conv1:0&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">           </span><span class="p">{</span>
<span class="w">               </span><span class="s">&quot;bitwidth&quot;</span><span class="o">:</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span>
<span class="w">               </span><span class="s">&quot;max&quot;</span><span class="o">:</span><span class="w"> </span><span class="mf">12.82344407824954</span><span class="p">,</span>
<span class="w">               </span><span class="s">&quot;min&quot;</span><span class="o">:</span><span class="w"> </span><span class="mf">0.0</span><span class="p">,</span>
<span class="w">               </span><span class="s">&quot;offset&quot;</span><span class="o">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span>
<span class="w">               </span><span class="s">&quot;scale&quot;</span><span class="o">:</span><span class="w"> </span><span class="mf">0.050288015993135454</span>
<span class="w">           </span><span class="p">}</span>
<span class="w">       </span><span class="p">],</span>
<span class="w">       </span><span class="s">&quot;input:0&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">           </span><span class="p">{</span>
<span class="w">               </span><span class="s">&quot;bitwidth&quot;</span><span class="o">:</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span>
<span class="w">               </span><span class="s">&quot;max&quot;</span><span class="o">:</span><span class="w"> </span><span class="mf">0.9960872825108046</span><span class="p">,</span>
<span class="w">               </span><span class="s">&quot;min&quot;</span><span class="o">:</span><span class="w"> </span><span class="mf">-1.0039304197656937</span><span class="p">,</span>
<span class="w">               </span><span class="s">&quot;offset&quot;</span><span class="o">:</span><span class="w"> </span><span class="mi">127</span><span class="p">,</span>
<span class="w">               </span><span class="s">&quot;scale&quot;</span><span class="o">:</span><span class="w"> </span><span class="mf">0.007843206675594112</span>
<span class="w">           </span><span class="p">}</span>
<span class="w">       </span><span class="p">]</span>
<span class="w">   </span><span class="p">},</span>
<span class="w">   </span><span class="s">&quot;param_encodings&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">       </span><span class="s">&quot;Conv2d/weights&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">           </span><span class="p">{</span>
<span class="w">               </span><span class="s">&quot;bitwidth&quot;</span><span class="o">:</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span>
<span class="w">               </span><span class="s">&quot;max&quot;</span><span class="o">:</span><span class="w"> </span><span class="mf">1.700559472933134</span><span class="p">,</span>
<span class="w">               </span><span class="s">&quot;min&quot;</span><span class="o">:</span><span class="w"> </span><span class="mf">-2.1006477158567995</span><span class="p">,</span>
<span class="w">               </span><span class="s">&quot;offset&quot;</span><span class="o">:</span><span class="w"> </span><span class="mi">140</span><span class="p">,</span>
<span class="w">               </span><span class="s">&quot;scale&quot;</span><span class="o">:</span><span class="w"> </span><span class="mf">0.01490669485799974</span>
<span class="w">           </span><span class="p">}</span>
<span class="w">       </span><span class="p">]</span>
<span class="w">   </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Note that it is not required to provide scale and offset. If they are provided they will be used, otherwise they will be calulated from the provided bw, min, and
max parameters.</p>
</div>
<div class="section" id="per-channel-quantization-overrides">
<h2><a class="toc-backref" href="#id19">Per-channel Quantization Overrides</a><a class="headerlink" href="#per-channel-quantization-overrides" title="Permalink to this heading">¶</a></h2>
<p>Per-channel quantization should be used for tensors that are weight inputs to Conv consumers (Conv2d, Conv3d, TransposeConv2d, DepthwiseConv2d). This section provides examples to manually override per-channel encodings for these Conv-based op weight tensors.
Per-channel quantization will be used when we provide multiple encodings (equal to the number of channels) for the given tensor.
We see an example for convolution weight for the following cases.</p>
<ul class="simple">
<li><p>Case 1: Asymmetric encodings without per-channel quantization</p></li>
</ul>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">    </span><span class="s">&quot;features.9.conv.3.weight&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="s">&quot;bitwidth&quot;</span><span class="o">:</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span>
<span class="w">            </span><span class="s">&quot;is_symmetric&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;False&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="s">&quot;max&quot;</span><span class="o">:</span><span class="w"> </span><span class="mf">3.0387749017453665</span><span class="p">,</span>
<span class="w">            </span><span class="s">&quot;min&quot;</span><span class="o">:</span><span class="w"> </span><span class="mf">-2.059169834735364</span><span class="p">,</span>
<span class="w">            </span><span class="s">&quot;offset&quot;</span><span class="o">:</span><span class="w"> </span><span class="mi">-103</span><span class="p">,</span>
<span class="w">            </span><span class="s">&quot;scale&quot;</span><span class="o">:</span><span class="w"> </span><span class="mf">0.019991940143061618</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Case 2: Per-channel quantization encodings with 3 output channels</p></li>
</ul>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">    </span><span class="s">&quot;features.8.conv.3.weight&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="s">&quot;bitwidth&quot;</span><span class="o">:</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span>
<span class="w">            </span><span class="s">&quot;is_symmetric&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;True&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="s">&quot;max&quot;</span><span class="o">:</span><span class="w"> </span><span class="mf">0.7011175155639648</span><span class="p">,</span>
<span class="w">            </span><span class="s">&quot;min&quot;</span><span class="o">:</span><span class="w"> </span><span class="mf">-0.7066381259227362</span><span class="p">,</span>
<span class="w">            </span><span class="s">&quot;offset&quot;</span><span class="o">:</span><span class="w"> </span><span class="mf">-128.0</span><span class="p">,</span>
<span class="w">            </span><span class="s">&quot;scale&quot;</span><span class="o">:</span><span class="w"> </span><span class="mf">0.005520610358771377</span>
<span class="w">        </span><span class="p">},</span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="s">&quot;bitwidth&quot;</span><span class="o">:</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span>
<span class="w">            </span><span class="s">&quot;is_symmetric&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;True&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="s">&quot;max&quot;</span><span class="o">:</span><span class="w"> </span><span class="mf">0.5228064656257629</span><span class="p">,</span>
<span class="w">            </span><span class="s">&quot;min&quot;</span><span class="o">:</span><span class="w"> </span><span class="mf">-0.5269230519692729</span><span class="p">,</span>
<span class="w">            </span><span class="s">&quot;offset&quot;</span><span class="o">:</span><span class="w"> </span><span class="mf">-128.0</span><span class="p">,</span>
<span class="w">            </span><span class="s">&quot;scale&quot;</span><span class="o">:</span><span class="w"> </span><span class="mf">0.004116586343509945</span>
<span class="w">        </span><span class="p">},</span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="s">&quot;bitwidth&quot;</span><span class="o">:</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span>
<span class="w">            </span><span class="s">&quot;is_symmetric&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;True&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="s">&quot;max&quot;</span><span class="o">:</span><span class="w"> </span><span class="mf">0.7368279099464417</span><span class="p">,</span>
<span class="w">            </span><span class="s">&quot;min&quot;</span><span class="o">:</span><span class="w"> </span><span class="mf">-0.7426297045129491</span><span class="p">,</span>
<span class="w">            </span><span class="s">&quot;offset&quot;</span><span class="o">:</span><span class="w"> </span><span class="mf">-128.0</span><span class="p">,</span>
<span class="w">            </span><span class="s">&quot;scale&quot;</span><span class="o">:</span><span class="w"> </span><span class="mf">0.005801794566507415</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
<p><strong>Note:</strong> Per-channel quantization must use symmetric representation with offset == -2^(bitwidth-1). Per-channel always has is_symmetric = True.</p>
</div>
<div class="section" id="quantizing-a-model">
<h2><a class="toc-backref" href="#id20">Quantizing a Model</a><a class="headerlink" href="#quantizing-a-model" title="Permalink to this heading">¶</a></h2>
<p>To enable quantization simply pass the option –input_list along with a text file containing raw data inputs to the network. Note that the inputs specified in this file
should match exactly with the inputs in the .cpp file generated by conversion. In most cases, these inputs can be obtained directly from the source framework model. However,
in rare cases, such as when the inputs are pruned by the converter, these inputs can differ. The format of the file uses a single line for each set of inputs to the network:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="n">inputFile0</span><span class="o">&gt;</span>
<span class="o">&lt;</span><span class="n">inputFile1</span><span class="o">&gt;</span>
<span class="o">&lt;</span><span class="n">inputFile2</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>If a network contains multiple inputs they are all listed on a single line separated by a space
and prefaced with the input name and a “:=”</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="n">inputNameA</span><span class="o">&gt;:=&lt;</span><span class="n">inputFile0a</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&lt;</span><span class="n">inputNameB</span><span class="o">&gt;:=&lt;</span><span class="n">inputFile0b</span><span class="o">&gt;</span>
<span class="o">&lt;</span><span class="n">inputNameA</span><span class="o">&gt;:=&lt;</span><span class="n">inputFile1a</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&lt;</span><span class="n">inputNameB</span><span class="o">&gt;:=&lt;</span><span class="n">inputFile1b</span><span class="o">&gt;</span>
<span class="o">&lt;</span><span class="n">inputNameA</span><span class="o">&gt;:=&lt;</span><span class="n">inputFile2a</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&lt;</span><span class="n">inputNameB</span><span class="o">&gt;:=&lt;</span><span class="n">inputFile2b</span><span class="o">&gt;</span>
</pre></div>
</div>
<div class="section" id="examples">
<h3><a class="toc-backref" href="#id21">Examples</a><a class="headerlink" href="#examples" title="Permalink to this heading">¶</a></h3>
<p>For graph containing a single input the input text file would contain something like:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="o">/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">file</span><span class="o">/</span><span class="n">chair</span><span class="p">.</span><span class="n">raw</span>
<span class="o">/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">file</span><span class="o">/</span><span class="n">mongoose</span><span class="p">.</span><span class="n">raw</span>
<span class="o">/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">file</span><span class="o">/</span><span class="n">honeybadger</span><span class="p">.</span><span class="n">raw</span>
</pre></div>
</div>
<p>For a network containing multiple graph inputs:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="nl">input_left_eye</span><span class="p">:</span><span class="o">=</span><span class="n">left0</span><span class="p">.</span><span class="n">rawtensor</span><span class="w"> </span><span class="n">input_right_eye</span><span class="o">:=</span><span class="n">right0</span><span class="p">.</span><span class="n">rawtensor</span>
<span class="nl">input_left_eye</span><span class="p">:</span><span class="o">=</span><span class="n">left1</span><span class="p">.</span><span class="n">rawtensor</span><span class="w"> </span><span class="n">input_right_eye</span><span class="o">:=</span><span class="n">right1</span><span class="p">.</span><span class="n">rawtensor</span>
<span class="nl">input_left_eye</span><span class="p">:</span><span class="o">=</span><span class="n">left2</span><span class="p">.</span><span class="n">rawtensor</span><span class="w"> </span><span class="n">input_right_eye</span><span class="o">:=</span><span class="n">right2</span><span class="p">.</span><span class="n">rawtensor</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="mixed-precision-and-fp16-support">
<h2><a class="toc-backref" href="#id22">Mixed Precision and FP16 Support</a><a class="headerlink" href="#mixed-precision-and-fp16-support" title="Permalink to this heading">¶</a></h2>
<p>Mixed Precision enables specifying different bit widths (e.g. INT8 or INT16) or datatypes (integer or floating point) for different ops within the same graph.
Data type conversion ops are automatically inserted when activation precision or data type is different between successive ops.
Graphs can have a mix of floating-point and fixed-point data types. Each op can have different precision for weights and activations.
However, for a particular op, either all inputs, outputs and parameters (weights/biases) will be floating-point or all will be integer type.
Please refer to the backend supplements for the supported weight/activation bit widths for a particular op.</p>
<ul class="simple">
<li><p><a class="reference internal" href="../OpDef/CpuOpDefSupplement.html"><span class="doc">CPU</span></a></p></li>
<li><p><a class="reference internal" href="../OpDef/GpuOpDefSupplement.html"><span class="doc">GPU</span></a></p></li>
<li><p><a class="reference internal" href="../OpDef/HtpOpDefSupplement.html"><span class="doc">HTP</span></a></p></li>
<li><p><a class="reference internal" href="../OpDef/Htp_fp16OpDefSupplement.html"><span class="doc">HTP FP16</span></a></p></li>
</ul>
<p>FP16 (half-precision) additionally enables converting the entire models to FP16 or selecting between FP16 and FP32 data-types for the float ops in case of mixed precision graphs with a mix of floating point and integer ops.
The different modes of using mixed precision are described below.</p>
<div class="section" id="non-quantized-mode">
<h3><a class="toc-backref" href="#id23">Non-quantized Mode</a><a class="headerlink" href="#non-quantized-mode" title="Permalink to this heading">¶</a></h3>
<p>In this mode no calibration images are given (–input_list flag is not given) to the converter. The converted QNN model has only float tensors for both activations and weights.</p>
<ul class="simple">
<li><p>Non-quantized FP16: If “–float_bw 16” is added in command line, all activation and weight/bias tensors are converted to FP16.</p></li>
<li><p>Non-quantized FP32: If “–float_bw” is absent from command line or “–float_bw 32” is given, all activation and weight/bias tensors use FP32 format.</p></li>
</ul>
</div>
<div class="section" id="quantized-mode">
<h3><a class="toc-backref" href="#id24">Quantized Mode</a><a class="headerlink" href="#quantized-mode" title="Permalink to this heading">¶</a></h3>
<p>In this mode calibration images are given (–input_list is given) to converter. The converted QNN model has fixed point tensors for activations and weights.</p>
<ul>
<li><p>No override: If no –quantization_overrides flag is given with an encoding file, all activations are quantized as per –act_bw (default 8) and parameters are quantized as per –weight_bw/–bias_bw (default 8/8) respectively.</p></li>
<li><p>Full override: If –quantization_overrides flag is given along with encoding file specifying encodings for all ops in the model. In this case, the bitwidth with be set as per JSON for all ops defined as integer/float as per encoding file (dtype=’int’ or dtype=’float’ in encoding json).</p></li>
<li><p>Partial override: If –quantization_overrides flag is given along with encoding file specifying partial encodings (i.e. encodings are missing for some ops), the following will happen.</p>
<blockquote>
<div><ul class="simple">
<li><p>Layers for which encoding are NOT available in json file are encoded in the same manner as the no override case i.e. defined as integer with bitwidth defined as per –act_bw/–weight_bw/–bias_bw (or their default values 8/8/8).
For some ops (Conv2d, Conv3d, TransposeConv2d, DepthwiseConv2d, FullyConnected, MatMul) even if any of the output/weights/bias are specified as float in the encoding file, all three of them will be overridden to float.
The float bitwidth used will be same as the float bitwidth of the overriding tensor in the encodings file. We can also manually control the bitwidth of bias tensors in such case (if encodings for it are absent in encodings json and present for output/weights) with the use of the –float_bias_bw (16/32) flag.</p></li>
<li><p>Layers for which encoding are available in json are encoded in same manner as full override case.</p></li>
</ul>
</div></blockquote>
</li>
</ul>
<p>We show a sample json for network with 3 Conv2d ops. The first and third Conv2d ops are INT8 while the second Conv2d op is marked as FP32.
The FP32 op (namely conv2_1) is sandwiched between two INT8 ops in “activation_encodings”, hence convert ops will be inserted before and after the FP32 op.
The corresponding weights and biases for conv2_1 are also marked as floating-point in the JSON in “param_encodings”.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">   </span><span class="s">&quot;activation_encodings&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">       </span><span class="s">&quot;data_0&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">           </span><span class="p">{</span>
<span class="w">               </span><span class="s">&quot;bitwidth&quot;</span><span class="o">:</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span>
<span class="w">               </span><span class="s">&quot;dtype&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;int&quot;</span>
<span class="w">           </span><span class="p">}</span>
<span class="w">       </span><span class="p">],</span>
<span class="w">       </span><span class="s">&quot;conv1_1&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">           </span><span class="p">{</span>
<span class="w">               </span><span class="s">&quot;bitwidth&quot;</span><span class="o">:</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span>
<span class="w">               </span><span class="s">&quot;dtype&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;int&quot;</span>
<span class="w">           </span><span class="p">}</span>
<span class="w">       </span><span class="p">],</span>
<span class="w">       </span><span class="s">&quot;conv2_1&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">           </span><span class="p">{</span>
<span class="w">               </span><span class="s">&quot;bitwidth&quot;</span><span class="o">:</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span>
<span class="w">               </span><span class="s">&quot;dtype&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;float&quot;</span>
<span class="w">           </span><span class="p">}</span>
<span class="w">       </span><span class="p">],</span>
<span class="w">       </span><span class="s">&quot;conv3_1&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">           </span><span class="p">{</span>
<span class="w">               </span><span class="s">&quot;bitwidth&quot;</span><span class="o">:</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span>
<span class="w">               </span><span class="s">&quot;dtype&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;int&quot;</span>
<span class="w">           </span><span class="p">}</span>
<span class="w">       </span><span class="p">]</span>
<span class="w">   </span><span class="p">},</span>
<span class="w">   </span><span class="s">&quot;param_encodings&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">       </span><span class="s">&quot;conv1_w_0&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">           </span><span class="p">{</span>
<span class="w">               </span><span class="s">&quot;bitwidth&quot;</span><span class="o">:</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span>
<span class="w">               </span><span class="s">&quot;dtype&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;int&quot;</span>
<span class="w">           </span><span class="p">}</span>
<span class="w">       </span><span class="p">],</span>
<span class="w">       </span><span class="s">&quot;conv1_b_0&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">           </span><span class="p">{</span>
<span class="w">               </span><span class="s">&quot;bitwidth&quot;</span><span class="o">:</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span>
<span class="w">               </span><span class="s">&quot;dtype&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;int&quot;</span>
<span class="w">           </span><span class="p">}</span>
<span class="w">       </span><span class="p">],</span>
<span class="w">       </span><span class="s">&quot;conv2_w_0&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">           </span><span class="p">{</span>
<span class="w">               </span><span class="s">&quot;bitwidth&quot;</span><span class="o">:</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span>
<span class="w">               </span><span class="s">&quot;dtype&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;float&quot;</span>
<span class="w">           </span><span class="p">}</span>
<span class="w">       </span><span class="p">],</span>
<span class="w">       </span><span class="s">&quot;conv2_b_0&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">           </span><span class="p">{</span>
<span class="w">               </span><span class="s">&quot;bitwidth&quot;</span><span class="o">:</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span>
<span class="w">               </span><span class="s">&quot;dtype&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;float&quot;</span>
<span class="w">           </span><span class="p">}</span>
<span class="w">       </span><span class="p">],</span>
<span class="w">       </span><span class="s">&quot;conv3_w_0&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">           </span><span class="p">{</span>
<span class="w">               </span><span class="s">&quot;bitwidth&quot;</span><span class="o">:</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span>
<span class="w">               </span><span class="s">&quot;dtype&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;int&quot;</span>
<span class="w">           </span><span class="p">}</span>
<span class="w">       </span><span class="p">],</span>
<span class="w">       </span><span class="s">&quot;conv3_b_0&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">           </span><span class="p">{</span>
<span class="w">               </span><span class="s">&quot;bitwidth&quot;</span><span class="o">:</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span>
<span class="w">               </span><span class="s">&quot;dtype&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;int&quot;</span>
<span class="w">           </span><span class="p">}</span>
<span class="w">       </span><span class="p">]</span>
<span class="w">   </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The ops that are not present in json will be assumed to be fixed-point and the bit widths will be selected according to –act_bw/–weight_bw/–bias_bw respectively.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">   </span><span class="s">&quot;activation_encodings&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">       </span><span class="s">&quot;conv2_1&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">           </span><span class="p">{</span>
<span class="w">               </span><span class="s">&quot;bitwidth&quot;</span><span class="o">:</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span>
<span class="w">               </span><span class="s">&quot;dtype&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;float&quot;</span>
<span class="w">           </span><span class="p">}</span>
<span class="w">       </span><span class="p">]</span>
<span class="w">   </span><span class="p">},</span>
<span class="w">   </span><span class="s">&quot;param_encodings&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">       </span><span class="s">&quot;conv2_w_0&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">           </span><span class="p">{</span>
<span class="w">               </span><span class="s">&quot;bitwidth&quot;</span><span class="o">:</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span>
<span class="w">               </span><span class="s">&quot;dtype&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;float&quot;</span>
<span class="w">           </span><span class="p">}</span>
<span class="w">       </span><span class="p">],</span>
<span class="w">       </span><span class="s">&quot;conv2_b_0&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">           </span><span class="p">{</span>
<span class="w">               </span><span class="s">&quot;bitwidth&quot;</span><span class="o">:</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span>
<span class="w">               </span><span class="s">&quot;dtype&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;float&quot;</span>
<span class="w">           </span><span class="p">}</span>
<span class="w">       </span><span class="p">]</span>
<span class="w">   </span><span class="p">},</span>
<span class="w">   </span><span class="s">&quot;version&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;0.5.0&quot;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The following quantized mixed-precision graph will be generated based on the JSON shown above. Please note that the convert operations are added appropriately to convert between float and int types and vice-versa.</p>
<div class="figure align-default">
<img alt="../_static/resources/qnn_quantization_mp_graph.png" src="../_static/resources/qnn_quantization_mp_graph.png" />
</div>
</div>
</div>
<div class="section" id="qairt-quantizer">
<h2><a class="toc-backref" href="#id25">qairt-quantizer</a><a class="headerlink" href="#qairt-quantizer" title="Permalink to this heading">¶</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This tool is still in a Beta release status.</p>
</div>
<p>The <a class="reference external" href="tools.html#qairt-converter">qairt-converter</a> tool converts non-quantized models into a non-quantized DLC file.
Quantizing requires another step. The <a class="reference external" href="tools.html#qairt-quantizer">qairt-quantizer</a> tool is used to quantize the model
to one of supported fixed point formats.</p>
<p>For example, the following command will convert an Inception v3 DLC file into a quantized Inception v3 DLC file.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="n">qairt</span><span class="o">-</span><span class="n">quantizer</span><span class="w"> </span><span class="o">--</span><span class="n">input_dlc</span><span class="w"> </span><span class="n">inception_v3</span><span class="p">.</span><span class="n">dlc</span><span class="w"> </span><span class="o">--</span><span class="n">input_list</span><span class="w"> </span><span class="n">image_file_list</span><span class="p">.</span><span class="n">txt</span><span class="w"> </span>\
<span class="w">                  </span><span class="o">--</span><span class="n">output_dlc</span><span class="w"> </span><span class="n">inception_v3_quantized</span><span class="p">.</span><span class="n">dlc</span>
</pre></div>
</div>
<p>To properly calculate the ranges for the quantization parameters, a representative set of input data needs to be used as
input into <a class="reference external" href="tools.html#qairt-quantizer">qairt-quantizer</a> using the <code class="docutils literal notranslate"><span class="pre">--input_list</span></code> parameter.
The input list specifies paths to raw image files used for quantization.
For specifying <code class="docutils literal notranslate"><span class="pre">--input_list</span></code>, refer to input_list argument in <a class="reference external" href="tools.html#qnn-net-run">qnn-net-run</a> for supported
input formats (in order to calculate output activation encoding information for all layers, <strong>do not</strong> include the line
which specifies desired outputs).</p>
<p>The tool requires the batch dimension of the DLC input file to be set to 1 during model conversion. The batch dimension
can be changed to a different value for inference, by resizing the network during initialization.</p>
<div class="section" id="additional-details">
<h3><a class="toc-backref" href="#id26">Additional details</a><a class="headerlink" href="#additional-details" title="Permalink to this heading">¶</a></h3>
<ul>
<li><p>qairt-quantizer is majorly similar to snpe-dlc-quant with the following differences:</p>
<ul>
<li><p>External Overrides and Source Model Encodings (QAT) cached in Float DLC during
Conversion stage are applied by default. Use the commandline argument “–ignore_encodings”
to ignore Overrides and Source Model Encodings and use Quantizer Runtime to
generate encodings using calibration dataset provided through “–input_list”.</p></li>
<li><p>Float_Fallback feature: A commandline option “–float_fallback” is added to enable
this feature. When the commandline option is specified, Qairt quantizer produces a
fully quantized or mixed precision graph by applying encoding overrides or
Source model encodings, propagate encodings accross Data invariant Ops and fallback
the missing tensors in float datatype.</p>
<p>Note: float_fallback and input_list are mutually exclusive options. One of them is
mandatory for quantizer</p>
</li>
</ul>
</li>
<li><p>Outputs can be specified for qairt-quantizer by modifying the input_list in the following ways:</p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>#&lt;output_layer_name&gt;[&lt;space&gt;&lt;output_layer_name&gt;]
%&lt;output_tensor_name&gt;[&lt;space&gt;&lt;output_tensor_name&gt;]
&lt;input_layer_name&gt;:=&lt;input_layer_path&gt;[&lt;space&gt;&lt;input_layer_name&gt;:=&lt;input_layer_path&gt;]
</pre></div>
</div>
<p><strong>Note:</strong> Output tensors and layers can be specified individually, but when specifying both, the order shown must
be used to specify each.</p>
</li>
<li><p>qairt-quantizer also supports quantization using AIMET, inplace of default Quantizer,
when “–use_aimet_quantizer” command line option is provided. To use AIMET Quantizer,
run the setup script to create AIMET specific environment, by executing the following command</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="n">source</span><span class="w"> </span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">bin</span><span class="o">/</span><span class="n">aimet_env_setup</span><span class="p">.</span><span class="n">sh</span><span class="w"> </span><span class="o">--</span><span class="n">env_path</span><span class="w"> </span><span class="o">&lt;</span><span class="n">path</span><span class="w"> </span><span class="n">where</span><span class="w"> </span><span class="n">AIMET</span><span class="w"> </span><span class="n">venv</span><span class="w"> </span><span class="n">needs</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">created</span><span class="o">&gt;</span><span class="w"> </span>\
<span class="w">                                               </span><span class="o">--</span><span class="n">aimet_sdk_tar</span><span class="w"> </span><span class="o">&lt;</span><span class="n">AIMET</span><span class="w"> </span><span class="n">Torch</span><span class="w"> </span><span class="n">SDK</span><span class="w"> </span><span class="n">tarball</span><span class="o">&gt;</span>
</pre></div>
</div>
<dl class="simple">
<dt><strong>Note:</strong></dt><dd><ol class="arabic simple">
<li><p>AIMET Torch Tarball naming convention should be as follows -
aimetpro-release-&lt;VERSION (optionally with build ID)&gt;.torch-&lt;cpu/gpu&gt;-.*.tar.gz.
For example, aimetpro-release-x.xx.x.torch-xxx-release.tar.gz.</p></li>
<li><p>Once the setup script is run, ensure that AIMET_ENV_PYTHON environment variable is set to
&lt;AIMET virtual environment path&gt;/bin/python</p></li>
<li><p>Minimum AIMET version supported is, <strong>AIMET-1.32.0</strong></p></li>
</ol>
</dd>
</dl>
</li>
</ul>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="tutorials.html" class="btn btn-neutral float-right" title="Tutorials" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="converters.html" class="btn btn-neutral float-left" title="Converters" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020-2024, Qualcomm Technologies, Inc..

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>