

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Tools &mdash; Qualcomm® AI Engine Direct</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/custom_css.css" type="text/css" />
  <link rel="stylesheet" href="../_static/collapsible-lists/css/tree_view.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/collapsible-lists/js/CollapsibleLists.compressed.js"></script>
        <script src="../_static/collapsible-lists/js/apply-collapsible-lists.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Converters" href="converters.html" />
    <link rel="prev" title="Create a QNN converter Op Package shared library" href="converter_op_package_gen_example.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> Qualcomm® AI Engine Direct
          

          
          </a>

          
            
            
              <div class="version">
                v2.22.6
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="setup.html">Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="backend.html">Backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="op_packages.html">Op Packages</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Tools</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#model-conversion">Model Conversion</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#qnn-tensorflow-converter">qnn-tensorflow-converter</a></li>
<li class="toctree-l3"><a class="reference internal" href="#qnn-tflite-converter">qnn-tflite-converter</a></li>
<li class="toctree-l3"><a class="reference internal" href="#qnn-pytorch-converter">qnn-pytorch-converter</a></li>
<li class="toctree-l3"><a class="reference internal" href="#qnn-onnx-converter">qnn-onnx-converter</a></li>
<li class="toctree-l3"><a class="reference internal" href="#qairt-converter">qairt-converter</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#model-preparation">Model Preparation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#quantization-support">Quantization Support</a></li>
<li class="toctree-l3"><a class="reference internal" href="#qairt-quantizer">qairt-quantizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="#qnn-model-lib-generator">qnn-model-lib-generator</a></li>
<li class="toctree-l3"><a class="reference internal" href="#qnn-op-package-generator">qnn-op-package-generator</a></li>
<li class="toctree-l3"><a class="reference internal" href="#qnn-context-binary-generator">qnn-context-binary-generator</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#execution">Execution</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#qnn-net-run">qnn-net-run</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#running-quantized-model-on-htp-backend-with-qnn-net-run">Running Quantized Model on HTP backend with qnn-net-run</a></li>
<li class="toctree-l4"><a class="reference internal" href="#running-float-model-on-htp-backend-with-qnn-net-run">Running Float Model on HTP backend with qnn-net-run</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#qnn-throughput-net-run">qnn-throughput-net-run</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#analysis">Analysis</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#qnn-quantization-checker-beta">qnn-quantization-checker (<span class="xref std std-ref">Beta</span>)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#viewing-the-results-html-csv-or-log-files">Viewing the results (html, csv or log files)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#qnn-accuracy-evaluator-beta">qnn-accuracy-evaluator (<span class="xref std std-ref">Beta</span>)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#usage">Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="#supported-models">Supported models</a></li>
<li class="toctree-l4"><a class="reference internal" href="#minimal-mode">Minimal Mode</a></li>
<li class="toctree-l4"><a class="reference internal" href="#config-mode">Config Mode</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#qnn-architecture-checker-beta">qnn-architecture-checker (<span class="xref std std-ref">Beta</span>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#qnn-accuracy-debugger-beta">qnn-accuracy-debugger (<span class="xref std std-ref">Beta</span>)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#framework-diagnosis">Framework Diagnosis</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id1">Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="#inference-engine">Inference Engine</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id2">Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="#verification">Verification</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id3">Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="#compare-encodings">Compare Encodings</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id4">Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="#tensor-inspection">Tensor inspection</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id5">Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="#run-qnn-accuracy-debugger-e2e">Run QNN Accuracy Debugger E2E</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id6">Usage</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#qnn-platform-validator">qnn-platform-validator</a></li>
<li class="toctree-l3"><a class="reference internal" href="#qnn-profile-viewer">qnn-profile-viewer</a></li>
<li class="toctree-l3"><a class="reference internal" href="#qnn-netron-beta">qnn-netron (<span class="xref std std-ref">Beta</span>)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="#launching-tool">Launching Tool</a></li>
<li class="toctree-l4"><a class="reference internal" href="#qnn-netron-visualize-deep-dive">QNN Netron Visualize Deep Dive</a></li>
<li class="toctree-l4"><a class="reference internal" href="#netron-diff-customization-deep-dive">Netron Diff Customization Deep Dive</a></li>
<li class="toctree-l4"><a class="reference internal" href="#inference-vs-inference">Inference vs Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="#results-and-outputs">Results and Outputs:</a></li>
<li class="toctree-l4"><a class="reference internal" href="#performance-and-accuracy-diff-visualizations">Performance and Accuracy Diff Visualizations:</a></li>
<li class="toctree-l4"><a class="reference internal" href="#qnn-netron-diff-navigation">QNN Netron Diff Navigation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#qnn-context-binary-utility">qnn-context-binary-utility</a></li>
<li class="toctree-l3"><a class="reference internal" href="#accuracy-evaluator-plugins">Accuracy Evaluator plugins</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#file-based-plugins">File-based plugins</a></li>
<li class="toctree-l4"><a class="reference internal" href="#memory-based-plugins">Memory-based plugins</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="converters.html">Converters</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmarking.html">Benchmarking</a></li>
<li class="toctree-l1"><a class="reference internal" href="operations.html">Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="api.html">API</a></li>
<li class="toctree-l1"><a class="reference internal" href="glossary.html">Glossary</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Qualcomm® AI Engine Direct</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Tools</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="tools">
<h1>Tools<a class="headerlink" href="#tools" title="Permalink to this heading">¶</a></h1>
<p>This page describes the various SDK tools and feature for Linux/Android and Windows developers.
For the integration flow of different developers, please refer to <a class="reference internal" href="overview.html"><span class="doc">Overview</span></a> page for further information.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 16%" />
<col style="width: 29%" />
<col style="width: 7%" />
<col style="width: 7%" />
<col style="width: 7%" />
<col style="width: 7%" />
<col style="width: 12%" />
<col style="width: 16%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head" rowspan="3"><p>Category</p></th>
<th class="head" rowspan="3"><p>Tool</p></th>
<th class="head" colspan="6"><p>Developer</p></th>
</tr>
<tr class="row-even"><th class="head" colspan="3"><p>Linux/Android</p></th>
<th class="head" colspan="3"><p>Windows</p></th>
</tr>
<tr class="row-odd"><th class="head"><p>Ubuntu</p></th>
<th class="head"><p>WSL x86</p></th>
<th class="head"><p>Device</p></th>
<th class="head"><p>WSL x86</p></th>
<th class="head"><p>Windows x86_64</p></th>
<th class="head"><p>Windows on Snapdragon</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td rowspan="4"><p><a class="reference internal" href="#model-conversion">Model Conversion</a></p></td>
<td><p><a class="reference internal" href="#qnn-tensorflow-converter">qnn-tensorflow-converter</a></p></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
<td></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
<td><p>YES**</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#qnn-tflite-converter">qnn-tflite-converter</a></p></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
<td></td>
<td><p>YES</p></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#qnn-pytorch-converter">qnn-pytorch-converter</a></p></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
<td></td>
<td><p>YES</p></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#qnn-onnx-converter">qnn-onnx-converter</a></p></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
<td></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
<td><p>YES**</p></td>
</tr>
<tr class="row-even"><td rowspan="4"><p><a class="reference internal" href="#model-preparation">Model Preparation</a></p></td>
<td><p><a class="reference internal" href="#quantization-support">Quantization Support</a></p></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
<td></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#qnn-model-lib-generator">qnn-model-lib-generator</a></p></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
<td></td>
<td></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#qnn-op-package-generator">qnn-op-package-generator</a></p></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
<td></td>
<td><p>YES</p></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#qnn-context-binary-generator">qnn-context-binary-generator</a></p></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
</tr>
<tr class="row-even"><td rowspan="2"><p><a class="reference internal" href="#execution">Execution</a></p></td>
<td><p><a class="reference internal" href="#qnn-net-run">qnn-net-run</a></p></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
<td></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#qnn-throughput-net-run">qnn-throughput-net-run</a></p></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
<td></td>
<td></td>
<td><p>YES</p></td>
</tr>
<tr class="row-even"><td rowspan="9"><p><a class="reference internal" href="#analysis">Analysis</a></p></td>
<td><p><a class="reference internal" href="#qnn-quantization-checker-beta">qnn-quantization-checker (Beta)</a></p></td>
<td><p>YES</p></td>
<td></td>
<td></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#qnn-accuracy-evaluator-beta">qnn-accuracy-evaluator (Beta)</a></p></td>
<td><p>YES</p></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#qnn-architecture-checker-beta">qnn-architecture-checker (Beta)</a></p></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
<td></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
<td><p>YES**</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#qnn-accuracy-debugger-beta">qnn-accuracy-debugger (Beta)</a></p></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
<td></td>
<td></td>
<td><p>YES</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#qnn-platform-validator">qnn-platform-validator</a></p></td>
<td><p>YES</p></td>
<td></td>
<td><p>YES</p></td>
<td></td>
<td></td>
<td><p>YES</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#qnn-profile-viewer">qnn-profile-viewer</a></p></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
<td></td>
<td><p>YES*</p></td>
<td><p>YES*</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="benchmarking.html"><span class="doc">Benchmarking</span></a></p></td>
<td><p>YES</p></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#qnn-netron-beta">qnn-netron (Beta)</a></p></td>
<td><p>YES</p></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#qnn-context-binary-utility">qnn-context-binary-utility</a></p></td>
<td><p>YES</p></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<div class="admonition note" id="qnn-ai-tools-beta-note">
<p class="admonition-title">Note</p>
<div class="line-block">
<div class="line">The <strong>Beta designation</strong> indicates <strong>pre-production quality</strong>. This means that the component is currently undergoing more rigorous testing and may not fully satisfy compatibility requirements as expected in the production version. In other words, <strong>incompatible changes</strong> (such as alterations in behavior or interface) between releases are allowed without prior notice, although every effort is made to minimize such changes.</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<div class="line-block">
<div class="line">* When using converter tools in Windows PowerShell, make sure a virtual environment
with the required python packages (see <a class="reference internal" href="setup.html"><span class="doc">Setup</span></a> for more details) is activated and converters are executed via <strong>python</strong>,
as shown in the following example.</div>
<div class="line-block">
<div class="line">(venv-3.10) &gt; python qnn-onnx-converter &lt;options&gt;</div>
</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>Extension naming of library: For Windows developers, please replace all ‘.so’ files with the analogous ‘.dll’ file in the following sections.
Please refer to <a class="reference internal" href="introduction.html#platform-differences"><span class="std std-ref">Platform Differences</span></a> for more details.</p></li>
<li><p>For more detailed information on converters please refer to <a class="reference internal" href="converters.html"><span class="doc">Converters</span></a>.</p></li>
<li><p>[*] libQnnGpuProfilingReader.dll is not supported on Windows platform for qnn-profile-viewer.</p></li>
<li><p>[**] Requires the python scripts and the executables from the Windows x86_64 binary folder(bin\x86_64-windows-msvc).</p></li>
</ul>
</div>
<div class="section" id="model-conversion">
<h2>Model Conversion<a class="headerlink" href="#model-conversion" title="Permalink to this heading">¶</a></h2>
<div class="section" id="qnn-tensorflow-converter">
<h3>qnn-tensorflow-converter<a class="headerlink" href="#qnn-tensorflow-converter" title="Permalink to this heading">¶</a></h3>
<p>The <strong>qnn-tensorflow-converter</strong> tool converts a model from the TensorFlow framework to
a CPP file representing the model as a series of QNN API calls. Additionally,
a binary file containing static weights of the model is produced.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>usage: qnn-tensorflow-converter -d INPUT_NAME INPUT_DIM --out_node OUT_NAMES
                                [--input_type INPUT_NAME INPUT_TYPE]
                                [--input_dtype INPUT_NAME INPUT_DTYPE] [--input_encoding INPUT_ENCODING [INPUT_ENCODING ...]]
                                [--input_layout INPUT_NAME INPUT_LAYOUT] [--custom_io CUSTOM_IO]
                                [--show_unconsumed_nodes] [--saved_model_tag SAVED_MODEL_TAG]
                                [--saved_model_signature_key SAVED_MODEL_SIGNATURE_KEY]
                                [--quantization_overrides QUANTIZATION_OVERRIDES]
                                [--keep_quant_nodes] [--disable_batchnorm_folding]
                                [--keep_disconnected_nodes] [--input_list INPUT_LIST]
                                [--param_quantizer PARAM_QUANTIZER] [--act_quantizer ACT_QUANTIZER]
                                [--algorithms ALGORITHMS [ALGORITHMS ...]] [--bias_bw BIAS_BW]
                                [--act_bw ACT_BW] [--weight_bw WEIGHT_BW] [--ignore_encodings]
                                [--use_per_row_quantization]
                                [--use_per_channel_quantization [USE_PER_CHANNEL_QUANTIZATION [USE_PER_CHANNEL_QUANTIZATION ...]]]
                                [--use_native_input_files] [--use_native_dtype]
                                [--use_native_output_files] --input_network INPUT_NETWORK
                                [--debug [DEBUG]] [-o OUTPUT_PATH] [--copyright_file COPYRIGHT_FILE]
                                [--float_bw FLOAT_BW] [--float_bias_bw FLOAT_BIAS_BW] [--overwrite_model_prefix]
                                [--exclude_named_tensors] [--op_package_lib OP_PACKAGE_LIB]
                                [--restrict_quantization_steps ENCODING_MIN, ENCODING_MAX]
                                [--converter_op_package_lib CONVERTER_OP_PACKAGE_LIB]
                                [-p PACKAGE_NAME | --op_package_config CUSTOM_OP_CONFIG_PATHS [CUSTOM_OP_CONFIG_PATHS ...]]
                                [-h] [--arch_checker]

Script to convert TF model into QNN

required arguments:
  -d INPUT_NAME INPUT_DIM, --input_dim INPUT_NAME INPUT_DIM
                        The names and dimensions of the network input layers specified in the format
                        [input_name comma-separated-dimensions], for example:
                            &#39;data&#39; 1,224,224,3
                        Note that the quotes should always be included in order to
                        handlespecial characters, spaces, etc.
                        For multiple inputs specify multiple --input_dim on the command line like:
                            --input_dim &#39;data1&#39; 1,224,224,3 --input_dim &#39;data2&#39; 1,50,100,3
  --out_node OUT_NODE,  --out_name OUT_NAMES
                        Name of the graph&#39;s output nodes. Multiple output nodes should be
                        provided separately like:
                            --out_node out_1 --out_node out_2
  --input_network INPUT_NETWORK, -i INPUT_NETWORK
                        Path to the source framework model.

optional arguments:
  --input_type INPUT_NAME INPUT_TYPE, -t INPUT_NAME INPUT_TYPE
                        Type of data expected by each input op/layer. Type for each input is
                        |default| if not specified. For example: &quot;data&quot; image.Note that the quotes
                        should always be included in order to handle special characters, spaces,etc.
                        For multiple inputs specify multiple --input_type on the command line.
                        Eg:
                            --input_type &quot;data1&quot; image --input_type &quot;data2&quot; opaque
                        These options get used by DSP runtime and following descriptions state how
                        input will be handled for each option.
                        Image:
                        Input is float between 0-255 and the input&#39;s mean is 0.0f and the input&#39;s
                        max is 255.0f. We will cast the float to uint8ts and pass the uint8ts to the
                        DSP.
                        Default:
                        Pass the input as floats to the dsp directly and the DSP will quantize it.
                        Opaque:
                        Assumes input is float because the consumer layer(i.e next layer) requires
                        it as float, therefore it won&#39;t be quantized.
                        Choices supported:
                            image
                            default
                            opaque
  --input_dtype INPUT_NAME INPUT_DTYPE
                        The names and datatype of the network input layers specified in the format
                        [input_name datatype], for example:
                            &#39;data&#39; &#39;float32&#39;.
                        Default is float32 if not specified.
                        Note that the quotes should always be included in order to handle special
                        characters, spaces, etc.
                        For multiple inputs specify multiple --input_dtype on the command line like:
                            --input_dtype &#39;data1&#39; &#39;float32&#39; --input_dtype &#39;data2&#39; &#39;float32&#39;
  --input_encoding INPUT_ENCODING [INPUT_ENCODING ...], -e INPUT_ENCODING [INPUT_ENCODING ...]
                        Usage:     --input_encoding &quot;INPUT_NAME&quot; INPUT_ENCODING_IN
                        [INPUT_ENCODING_OUT]
                        Input encoding of the network inputs. Default is bgr.
                        e.g.
                            --input_encoding &quot;data&quot; rgba
                        Quotes must wrap the input node name to handle special characters,
                        spaces, etc. To specify encodings for multiple inputs, invoke
                        --input_encoding for each one.
                        e.g.
                            --input_encoding &quot;data1&quot; rgba --input_encoding &quot;data2&quot; other
                        Optionally, an output encoding may be specified for an input node by
                        providing a second encoding. The default output encoding is bgr.
                        e.g.
                            --input_encoding &quot;data3&quot; rgba rgb
                        Input encoding types:
                            image color encodings: bgr,rgb, nv21, nv12, ...
                            time_series: for inputs of rnn models;
                            other: not available above or is unknown.
                        Supported encodings:
                            bgr
                            rgb
                            rgba
                            argb32
                            nv21
                            nv12
                            time_series
                            other
  --input_layout INPUT_NAME INPUT_LAYOUT, -l INPUT_NAME INPUT_LAYOUT
                        Layout of each input tensor. If not specified, it will use the default
                        based on the Source Framework, shape of input and input encoding.
                        Accepted values are-
                            NCDHW, NDHWC, NCHW, NHWC, NFC, NCF, NTF, TNF, NF, NC, F, NONTRIVIAL
                        N = Batch, C = Channels, D = Depth, H = Height, W = Width, F = Feature, T = Time
                        NDHWC/NCDHW used for 5d inputs
                        NHWC/NCHW used for 4d image-like inputs
                        NFC/NCF used for inputs to Conv1D or other 1D ops
                        NTF/TNF used for inputs with time steps like the ones used for LSTM op
                        NF used for 2D inputs, like the inputs to Dense/FullyConnected layers
                        NC used for 2D inputs with 1 for batch and other for Channels (rarely used)
                        F used for 1D inputs, e.g. Bias tensor
                        NONTRIVIAL for everything elseFor multiple inputs specify multiple
                        --input_layout on the command line.
                        Eg:
                           --input_layout &quot;data1&quot; NCHW --input_layout &quot;data2&quot; NCHW
  --custom_io CUSTOM_IO
                        Use this option to specify a yaml file for custom IO
  --show_unconsumed_nodes
                        Displays a list of unconsumed nodes, if there any are found. Nodes which are
                        unconsumed do not violate the structural fidelity of thegenerated graph.
  --saved_model_tag SAVED_MODEL_TAG
                        Specify the tag to seletet a MetaGraph from savedmodel. ex:
                        --saved_model_tag serve. Default value will be &#39;serve&#39; when it is not
                        assigned.
  --saved_model_signature_key SAVED_MODEL_SIGNATURE_KEY
                        Specify signature key to select input and output of the model. ex:
                        --saved_model_signature_key serving_default. Default value will be
                        &#39;serving_default&#39; when it is not assigned
  --disable_batchnorm_folding
  --keep_disconnected_nodes
                        Disable Optimization that removes Ops not connected to the main graph.
                        This optimization uses output names provided over commandline OR
                        inputs/outputs extracted from the Source model to determine the main graph
  --debug [DEBUG]       Run the converter in debug mode.
  -o OUTPUT_PATH, --output_path OUTPUT_PATH
                        Path where the converted Output model should be saved.If not specified, the
                        converter model will be written to a file with same name as the input model
  --copyright_file COPYRIGHT_FILE
                        Path to copyright file. If provided, the content of the file will be added
                        to the output model.
  --float_bw FLOAT_BW   Option to select the bitwidth to use when using float for parameters (weights/
                        bias) and activations for all ops or a specific op (via encodings) selected
                        through encoding; values are 32 (default) or 16.
  --overwrite_model_prefix
                        If option passed, model generator will use the output path name to use as
                        model prefix to name functions in &lt;qnn_model_name&gt;.cpp. (Useful for running
                        multiple models at once) eg: ModelName_composeGraphs. Default is to use
                        generic &quot;QnnModel_&quot;.
  --exclude_named_tensors
                        Remove using source framework tensorNames; instead use a counter for naming
                        tensors. Note: This can potentially help to reduce the final model library
                        that will be generated(Recommended for deploying model). Default is False.
  -h, --help            show this help message and exit

Quantizer Options:
  --quantization_overrides QUANTIZATION_OVERRIDES
                        Use this option to specify a json file with parameters to use for
                        quantization. These will override any quantization data carried from
                        conversion (eg TF fake quantization) or calculated during the normal
                        quantization process. Format defined as per AIMET specification.
  --keep_quant_nodes    Use this option to keep activation quantization nodes in the graph rather
                        than stripping them.
  --input_list INPUT_LIST
                        Path to a file specifying the input data. This file should be a plain text
                        file, containing one or more absolute file paths per line. Each path is
                        expected to point to a binary file containing one input in the &quot;raw&quot; format,
                        ready to be consumed by the quantizer without any further preprocessing.
                        Multiple files per line separated by spaces indicate multiple inputs to the
                        network. See documentation for more details. Must be specified for
                        quantization. All subsequent quantization options are ignored when this is
                        not provided.
  --param_quantizer PARAM_QUANTIZER
                        Optional parameter to indicate the weight/bias quantizer to use. Must be
                        followed by one of the following options: &quot;tf&quot;: Uses the real min/max of the
                        data and specified bitwidth (default) &quot;enhanced&quot;: Uses an algorithm useful
                        for quantizing models with long tails present in the weight distribution
                        &quot;adjusted&quot;: Uses an adjusted min/max for computing the range, particularly
                        good for denoise models &quot;symmetric&quot;: Ensures min and max have the same
                        absolute values about zero. Data will be stored as int#_t data such that the
                        offset is always 0.
  --act_quantizer ACT_QUANTIZER
                        Optional parameter to indicate the activation quantizer to use. Must be
                        followed by one of the following options: &quot;tf&quot;: Uses the real min/max of the
                        data and specified bitwidth (default) &quot;enhanced&quot;: Uses an algorithm useful
                        for quantizing models with long tails present in the weight distribution
                        &quot;adjusted&quot;: Uses an adjusted min/max for computing the range, particularly
                        good for denoise models &quot;symmetric&quot;: Ensures min and max have the same
                        absolute values about zero. Data will be stored as int#_t data such that the
                        offset is always 0.
  --algorithms ALGORITHMS [ALGORITHMS ...]
                        Use this option to enable new optimization algorithms. Usage is:
                        --algorithms &lt;algo_name1&gt; ... The available optimization algorithms are:
                        &quot;cle&quot; - Cross layer equalization includes a number of methods for equalizing
                        weights and biases across layers in order to rectify imbalances that cause
                        quantization errors.
  --bias_bw BIAS_BW     Use the --bias_bw option to select the bitwidth to use when quantizing the
                        biases, either 8 (default) or 32.
  --act_bw ACT_BW       Use the --act_bw option to select the bitwidth to use when quantizing the
                        activations, either 8 (default) or 16.
  --weight_bw WEIGHT_BW
                        Use the --weight_bw option to select the bitwidth to use when quantizing the
                        weights, currently only 8 bit (default) supported.
 --float_bias_bw FLOAT_BIAS_BW
                        Use the --float_bias_bw option to select the bitwidth to use when biases are
                        in float, either 32 or 16.
  --ignore_encodings    Use only quantizer generated encodings, ignoring any user or model provided
                        encodings.
                        Note: Cannot use --ignore_encodings with --quantization_overrides
  --use_per_row_quantization
                        Use this option to enable rowwise quantization of Matmul and FullyConnected
                        ops.
  --use_per_channel_quantization [USE_PER_CHANNEL_QUANTIZATION [USE_PER_CHANNEL_QUANTIZATION ...]]
                        Use per-channel quantization for convolution-based op weights.
                        Note: This will replace built-in model QAT encodings when used for a given
                        weight.Usage &quot;--use_per_channel_quantization&quot; to enable or &quot;--
                        use_per_channel_quantization false&quot; (default) to disable
  --use_native_input_files
                        Boolean flag to indicate how to read input files:
                        1. float (default): reads inputs as floats and quantizes if necessary based
                        on quantization parameters in the model.
                        2. native: reads inputs assuming the data type to be native to the
                        model. For ex., uint8_t.
  --use_native_dtype    Note: This option is deprecated, use --use_native_input_files option in
                        future.
                        Boolean flag to indicate how to read input files:
                        1. float (default): reads inputs as floats and quantizes if necessary based
                        on quantization parameters in the model.
                        2. native: reads inputs assuming the data type to be native to the
                        model. For ex., uint8_t.
  --use_native_output_files
                        Use this option to indicate the data type of the output files
                        1. float (default): output the file as floats.
                        2. native: outputs the file that is native to the model. For ex.,
                        uint8_t.
  --restrict_quantization_steps ENCODING_MIN, ENCODING_MAX
                        Specifies the number of steps to use for computing quantization encodings
                        such that scale = (max - min) / number of quantization steps.
                        The option should be passed as a space separated pair of hexadecimal string
                        minimum and maximum values. i.e. --restrict_quantization_steps &quot;MIN MAX&quot;.
                        Please note that this is a hexadecimal string literal and not a signed
                        integer, to supply a negative value an explicit minus sign is required.
                        E.g.--restrict_quantization_steps &quot;-0x80 0x7F&quot; indicates an example 8 bit range,
                        --restrict_quantization_steps &quot;-0x8000 0x7F7F&quot; indicates an example 16
                        bit range. This argument is required for 16-bit Matmul operations.


 Custom Op Package Options:
  --op_package_lib OP_PACKAGE_LIB, -opl OP_PACKAGE_LIB
                        Use this argument to pass an op package library for quantization. Must be in
                        the form
                        &lt;op_package_lib_path:interfaceProviderName&gt; and be separated by a
                        comma for multiple package libs
  -p PACKAGE_NAME, --package_name PACKAGE_NAME
                        A global package name to be used for each node in the Model.cpp file.
                        Defaults to Qnn header defined package name
  --converter_op_package_lib CONVERTER_OP_PACKAGE_LIB, -cpl CONVERTER_OP_PACKAGE_LIB
                        Absolute path to converter op package library compiled by the OpPackage
                        generator. Must be separated by a comma for multiple package libraries.
                        Note: Libraries must follow the same order as the xml files.
                        E.g.1: --converter_op_package_lib absolute_path_to/libExample.so
                        E.g.2: -cpl absolute_path_to/libExample1.so,absolute_path_to/libExample2.so
  --op_package_config OP_PACKAGE_CONFIG [OP_PACKAGE_CONFIG ...], -opc OP_PACKAGE_CONFIG [OP_PACKAGE_CONFIG ...]
                        Path to a Qnn Op Package XML configuration file that contains user defined
                        custom operations.

Architecture Checker Options(Experimental):
  --arch_checker        Note: This option will be soon deprecated. Use the qnn-architecture-checker tool to achieve the same result.

Note: Only one of: {&#39;package_name&#39;, &#39;op_package_config&#39;} can be specified
</pre></div>
</div>
<p>Basic command line usage looks like:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>$ qnn-tensorflow-converter -i &lt;path&gt;/frozen_graph.pb
                    -d &lt;network_input_name&gt; &lt;dims&gt;
                    --out_node &lt;network_output_name&gt;
                    -o &lt;optional_output_path&gt;
                    --allow_unconsumed_nodes  # optional, but most likely will be need for larger models
                    -p &lt;optional_package_name&gt; # Defaults to &quot;qti.aisw&quot;
</pre></div>
</div>
</div>
<div class="section" id="qnn-tflite-converter">
<h3>qnn-tflite-converter<a class="headerlink" href="#qnn-tflite-converter" title="Permalink to this heading">¶</a></h3>
<p>The <strong>qnn-tflite-converter</strong> tool converts a TFLite model to
a CPP file representing the model as a series of QNN API calls. Additionally,
a binary file containing static weights of the model is produced.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>usage: qnn-tflite-converter -d INPUT_NAME INPUT_DIM [--out_node OUT_NAMES]
                            [--input_type INPUT_NAME INPUT_TYPE]
                            [--input_dtype INPUT_NAME INPUT_DTYPE] [--input_encoding INPUT_ENCODING [INPUT_ENCODING ...]]
                            [--input_layout INPUT_NAME INPUT_LAYOUT] [--custom_io CUSTOM_IO]
                            [--dump_relay DUMP_RELAY]
                            [--quantization_overrides QUANTIZATION_OVERRIDES] [--keep_quant_nodes]
                            [--disable_batchnorm_folding] [--keep_disconnected_nodes]
                            [--input_list INPUT_LIST] [--param_quantizer PARAM_QUANTIZER]
                            [--act_quantizer ACT_QUANTIZER]
                            [--algorithms ALGORITHMS [ALGORITHMS ...]] [--bias_bw BIAS_BW]
                            [--act_bw ACT_BW] [--weight_bw WEIGHT_BW] [--ignore_encodings]
                            [--use_per_row_quantization]
                            [--use_per_channel_quantization [USE_PER_CHANNEL_QUANTIZATION [USE_PER_CHANNEL_QUANTIZATION ...]]]
                            [--use_native_input_files] [--use_native_dtype]
                            [--use_native_output_files] --input_network INPUT_NETWORK
                            [--debug [DEBUG]] [-o OUTPUT_PATH] [--copyright_file COPYRIGHT_FILE]
                            [--float_bw FLOAT_BW] [--float_bias_bw FLOAT_BIAS_BW] [--overwrite_model_prefix]
                            [--exclude_named_tensors] [--op_package_lib OP_PACKAGE_LIB]
                            [--restrict_quantization_steps ENCODING_MIN, ENCODING_MAX]
                            [--converter_op_package_lib CONVERTER_OP_PACKAGE_LIB]
                            [-p PACKAGE_NAME | --op_package_config CUSTOM_OP_CONFIG_PATHS [CUSTOM_OP_CONFIG_PATHS ...]]
                            [-h] [--arch_checker]

Script to convert TFLite model into QNN

required arguments:
  -d INPUT_NAME INPUT_DIM, --input_dim INPUT_NAME INPUT_DIM
                        The names and dimensions of the network input layers specified in the format
                        [input_name comma-separated-dimensions], for example:
                            &#39;data&#39; 1,224,224,3 Note that the quotes should always be included in order to handle special
                        characters, spaces, etc. For multiple inputs specify multiple --input_dim on the command
                        line like:
                            --input_dim &#39;data1&#39; 1,224,224,3 --input_dim &#39;data2&#39; 1,50,100,3
  --input_network INPUT_NETWORK, -i INPUT_NETWORK
                        Path to the source framework model.

optional arguments:
  --out_node OUT_NAMES, --out_name OUT_NAMES
                        Name of the graph&#39;s output Tensor Names. Multiple output names should be
                        provided separately like:
                            --out_name out_1 --out_name out_2
  --input_type INPUT_NAME INPUT_TYPE, -t INPUT_NAME INPUT_TYPE
                        Type of data expected by each input op/layer. Type for each input is
                        |default| if not specified. For example: &quot;data&quot; image.Note that the quotes
                        should always be included in order to handle special characters, spaces,etc.
                        For multiple inputs specify multiple --input_type on the command line.
                        Eg:
                            --input_type &quot;data1&quot; image --input_type &quot;data2&quot; opaque
                        These options get used by DSP runtime and following descriptions state how
                        input will be handled for each option.
                        Image:
                        Input is float between 0-255 and the input&#39;s mean is 0.0f and the input&#39;s
                        max is 255.0f. We will cast the float to uint8ts and pass the uint8ts to the
                        DSP.
                        Default:
                        Pass the input as floats to the dsp directly and the DSP will quantize it.
                        Opaque:
                        Assumes input is float because the consumer layer(i.e next layer) requires
                        it as float, therefore it won&#39;t be quantized.
                        Choices supported:
                            image
                            default
                            opaque
  --input_dtype INPUT_NAME INPUT_DTYPE
                        The names and datatype of the network input layers specified in the format
                        [input_name datatype], for example:
                            &#39;data&#39; &#39;float32&#39;
                        Default is float32 if not specified
                        Note that the quotes should always be included in order to handlespecial
                        characters, spaces, etc.
                        For multiple inputs specify multiple --input_dtype on the command line like:
                            --input_dtype &#39;data1&#39; &#39;float32&#39; --input_dtype &#39;data2&#39; &#39;float32&#39;
  --input_encoding INPUT_ENCODING [INPUT_ENCODING ...], -e INPUT_ENCODING [INPUT_ENCODING ...]
                        Usage:     --input_encoding &quot;INPUT_NAME&quot; INPUT_ENCODING_IN
                        [INPUT_ENCODING_OUT]
                        Input encoding of the network inputs. Default is bgr.
                        e.g.
                            --input_encoding &quot;data&quot; rgba
                        Quotes must wrap the input node name to handle special characters,
                        spaces, etc. To specify encodings for multiple inputs, invoke
                        --input_encoding for each one.
                        e.g.
                            --input_encoding &quot;data1&quot; rgba --input_encoding &quot;data2&quot; other
                        Optionally, an output encoding may be specified for an input node by
                        providing a second encoding. The default output encoding is bgr.
                        e.g.
                            --input_encoding &quot;data3&quot; rgba rgb
                        Input encoding types:
                            image color encodings: bgr,rgb, nv21, nv12, ...
                            time_series: for inputs of rnn models;
                            other: not available above or is unknown.
                        Supported encodings:
                            bgr
                            rgb
                            rgba
                            argb32
                            nv21
                            nv12
                            time_series
                            other
  --input_layout INPUT_NAME INPUT_LAYOUT, -l INPUT_NAME INPUT_LAYOUT
                        Layout of each input tensor. If not specified, it will use the default
                        based on the Source Framework, shape of input and input encoding.
                        Accepted values are-
                            NCDHW, NDHWC, NCHW, NHWC, NFC, NCF, NTF, TNF, NF, NC, F, NONTRIVIAL
                        N = Batch, C = Channels, D = Depth, H = Height, W = Width, F = Feature, T = Time
                        NDHWC/NCDHW used for 5d inputs
                        NHWC/NCHW used for 4d image-like inputs
                        NFC/NCF used for inputs to Conv1D or other 1D ops
                        NTF/TNF used for inputs with time steps like the ones used for LSTM op
                        NF used for 2D inputs, like the inputs to Dense/FullyConnected layers
                        NC used for 2D inputs with 1 for batch and other for Channels (rarely used)
                        F used for 1D inputs, e.g. Bias tensor
                        NONTRIVIAL for everything elseFor multiple inputs specify multiple
                        --input_layout on the command line.
                        Eg:
                           --input_layout &quot;data1&quot; NCHW --input_layout &quot;data2&quot; NCHW
  --custom_io CUSTOM_IO
                        Use this option to specify a yaml file for custom IO.
  --dump_relay DUMP_RELAY
                        Dump Relay ASM and Params at the path provided with the argument
                        Usage: --dump_relay &lt;path_to_dump&gt;
  --show_unconsumed_nodes
                        Displays a list of unconsumed nodes, if there any are
                        found. Nodeswhich are unconsumed do not violate the
                        structural fidelity of thegenerated graph.
  --disable_batchnorm_folding
  --keep_disconnected_nodes
                        Disable Optimization that removes Ops not connected to the main graph.
                        This optimization uses output names provided over commandline OR
                        inputs/outputs extracted from the Source model to determine the main graph
  -o OUTPUT_PATH, --output_path OUTPUT_PATH
                        Path where the converted Output model should be saved.If not specified, the
                        converter model will be written to a file with same name as the input model
  --copyright_file COPYRIGHT_FILE
                        Path to copyright file. If provided, the content of the file will be added
                        to the output model.
  --float_bw FLOAT_BW   Option to select the bitwidth to use when using float for parameters (weights/
                        bias) and activations for all ops or a specific op (via encodings) selected
                        through encoding; values are 32 (default) or 16.
  --overwrite_model_prefix
                        If option passed, model generator will use the output path name to use as
                        model prefix to name functions in &lt;qnn_model_name&gt;.cpp. (Useful for running
                        multiple models at once) eg: ModelName_composeGraphs. Default is to use
                        generic &quot;QnnModel_&quot;.
  --exclude_named_tensors
                        Remove using source framework tensorNames; instead use a counter for naming
                        tensors. Note: This can potentially help to reduce the final model library
                        that will be generated(Recommended for deploying model). Default is False.
  -h, --help            show this help message and exit

Quantizer Options:
  --quantization_overrides QUANTIZATION_OVERRIDES
                        Use this option to specify a json file with parameters to use for
                        quantization. These will override any quantization data carried from
                        conversion (eg TF fake quantization) or calculated during the normal
                        quantization process. Format defined as per AIMET specification.
  --keep_quant_nodes    Use this option to keep activation quantization nodes in the graph rather
                        than stripping them.
  --input_list INPUT_LIST
                        Path to a file specifying the input data. This file should be a plain text
                        file, containing one or more absolute file paths per line. Each path is
                        expected to point to a binary file containing one input in the &quot;raw&quot; format,
                        ready to be consumed by the quantizer without any further preprocessing.
                        Multiple files per line separated by spaces indicate multiple inputs to the
                        network. See documentation for more details. Must be specified for
                        quantization. All subsequent quantization options are ignored when this is
                        not provided.
  --param_quantizer PARAM_QUANTIZER
                        Optional parameter to indicate the weight/bias quantizer to use. Must be
                        followed by one of the following options: &quot;tf&quot;: Uses the real min/max of the
                        data and specified bitwidth (default) &quot;enhanced&quot;: Uses an algorithm useful
                        for quantizing models with long tails present in the weight distribution
                        &quot;adjusted&quot;: Uses an adjusted min/max for computing the range, particularly
                        good for denoise models &quot;symmetric&quot;: Ensures min and max have the same
                        absolute values about zero. Data will be stored as int#_t data such that the
                        offset is always 0.
  --act_quantizer ACT_QUANTIZER
                        Optional parameter to indicate the activation quantizer to use. Must be
                        followed by one of the following options: &quot;tf&quot;: Uses the real min/max of the
                        data and specified bitwidth (default) &quot;enhanced&quot;: Uses an algorithm useful
                        for quantizing models with long tails present in the weight distribution
                        &quot;adjusted&quot;: Uses an adjusted min/max for computing the range, particularly
                        good for denoise models &quot;symmetric&quot;: Ensures min and max have the same
                        absolute values about zero. Data will be stored as int#_t data such that the
                        offset is always 0.
  --algorithms ALGORITHMS [ALGORITHMS ...]
                        Use this option to enable new optimization algorithms. Usage is:
                        --algorithms &lt;algo_name1&gt; ... The available optimization algorithms are:
                        &quot;cle&quot; - Cross layer equalization includes a number of methods for equalizing
                        weights and biases across layers in order to rectify imbalances that cause
                        quantization errors.
  --bias_bw BIAS_BW     Use the --bias_bw option to select the bitwidth to use when quantizing the
                        biases, either 8 (default) or 32.
  --act_bw ACT_BW       Use the --act_bw option to select the bitwidth to use when quantizing the
                        activations, either 8 (default) or 16.
  --weight_bw WEIGHT_BW
                        Use the --weight_bw option to select the bitwidth to use when quantizing the
                        weights, currently only 8 bit (default) supported.
  --float_bias_bw FLOAT_BIAS_BW
                        Use the --float_bias_bw option to select the bitwidth to use when biases are
                        in float, either 32 or 16.
  --ignore_encodings    Use only quantizer generated encodings, ignoring any user or model provided
                        encodings.
                        Note: Cannot use --ignore_encodings with --quantization_overrides
  --use_per_row_quantization
                        Use this option to enable rowwise quantization of Matmul and FullyConnected
                        ops.
  --use_per_channel_quantization [USE_PER_CHANNEL_QUANTIZATION [USE_PER_CHANNEL_QUANTIZATION ...]]
                        Use per-channel quantization for convolution-based op weights.
                        Note: This will replace built-in model QAT encodings when used for a given
                        weight.Usage &quot;--use_per_channel_quantization&quot; to enable or &quot;--
                        use_per_channel_quantization false&quot; (default) to disable
  --use_native_input_files
                        Boolean flag to indicate how to read input files:
                        1. float (default): reads inputs as floats and quantizes if necessary based
                        on quantization parameters in the model.
                        2. native: reads inputs assuming the data type to be native to the
                        model. For ex., uint8_t.
  --use_native_dtype    Note: This option is deprecated, use --use_native_input_files option in
                        future.
                        Boolean flag to indicate how to read input files:
                        1. float (default): reads inputs as floats and quantizes if necessary based
                        on quantization parameters in the model.
                        2. native: reads inputs assuming the data type to be native to the
                        model. For ex., uint8_t.
  --use_native_output_files
                        Use this option to indicate the data type of the output files
                        1. float (default): output the file as floats.
                        2. native: outputs the file that is native to the model. For ex.,
                        uint8_t.
  --restrict_quantization_steps ENCODING_MIN, ENCODING_MAX
                        Specifies the number of steps to use for computing quantization encodings
                        such that scale = (max - min) / number of quantization steps.
                        The option should be passed as a space separated pair of hexadecimal string
                        minimum and maximum values. i.e. --restrict_quantization_steps &quot;MIN MAX&quot;.
                        Please note that this is a hexadecimal string literal and not a signed
                        integer, to supply a negative value an explicit minus sign is required.
                        E.g.--restrict_quantization_steps &quot;-0x80 0x7F&quot; indicates an example 8 bit range,
                        --restrict_quantization_steps &quot;-0x8000 0x7F7F&quot; indicates an example 16
                        bit range.

Custom Op Package Options:
  --op_package_lib OP_PACKAGE_LIB, -opl OP_PACKAGE_LIB
                        Use this argument to pass an op package library for quantization. Must be in
                        the form &lt;op_package_lib_path:interfaceProviderName&gt; and be separated by a
                        comma for multiple package libs
  --converter_op_package_lib CONVERTER_OP_PACKAGE_LIB, -cpl CONVERTER_OP_PACKAGE_LIB
                        Absolute path to converter op package library compiled by the OpPackage
                        generator. Must be separated by a comma for multiple package libraries.
                        Note: Libraries must follow the same order as the xml files.
                        E.g.1: --converter_op_package_lib absolute_path_to/libExample.so
                        E.g.2: -cpl absolute_path_to/libExample1.so,absolute_path_to/libExample2.so
  -p PACKAGE_NAME, --package_name PACKAGE_NAME
                        A global package name to be used for each node in the Model.cpp file.
                        Defaults to Qnn header defined package name
  --op_package_config OP_PACKAGE_CONFIG [OP_PACKAGE_CONFIG ...], -opc OP_PACKAGE_CONFIG [OP_PACKAGE_CONFIG ...]
                        Path to a Qnn Op Package XML configuration file that contains user defined
                        custom operations.

Architecture Checker Options(Experimental):
  --arch_checker        Note: This option will be soon deprecated. Use the qnn-architecture-checker tool to achieve the same result.

Note: Only one of: {&#39;package_name&#39;, &#39;op_package_config&#39;} can be specified
</pre></div>
</div>
<p>Basic command line usage looks like:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>$ qnn-tflite-converter -i &lt;path&gt;/model.tflite
                       -d &lt;network_input_name&gt; &lt;dims&gt;
                       -o &lt;optional_output_path&gt;
                       -p &lt;optional_package_name&gt; # Defaults to &quot;qti.aisw&quot;
</pre></div>
</div>
</div>
<div class="section" id="qnn-pytorch-converter">
<h3>qnn-pytorch-converter<a class="headerlink" href="#qnn-pytorch-converter" title="Permalink to this heading">¶</a></h3>
<p>The <strong>qnn-pytorch-converter</strong> tool converts a PyTorch model to
a CPP file representing the model as a series of QNN API calls. Additionally,
a binary file containing static weights of the model is produced.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>usage: qnn-pytorch-converter -d INPUT_NAME INPUT_DIM [--out_node OUT_NAMES]
                          [--input_type INPUT_NAME INPUT_TYPE]
                          [--input_dtype INPUT_NAME INPUT_DTYPE] [--input_encoding INPUT_ENCODING [INPUT_ENCODING ...]]
                          [--input_layout INPUT_NAME INPUT_LAYOUT] [--custom_io CUSTOM_IO]
                          [--preserve_io [PRESERVE_IO [PRESERVE_IO ...]]]
                          [--dump_relay DUMP_RELAY] [--dry_run] [--dump_out_names]
                          [--pytorch_custom_op_lib PYTORCH_CUSTOM_OP_LIB]
                          [--quantization_overrides QUANTIZATION_OVERRIDES] [--keep_quant_nodes]
                          [--disable_batchnorm_folding] [--keep_disconnected_nodes]
                          [--input_list INPUT_LIST] [--param_quantizer PARAM_QUANTIZER]
                          [--act_quantizer ACT_QUANTIZER]
                          [--algorithms ALGORITHMS [ALGORITHMS ...]] [--bias_bw BIAS_BW]
                          [--act_bw ACT_BW] [--weight_bw WEIGHT_BW] [--ignore_encodings]
                          [--use_per_row_quantization]
                          [--use_per_channel_quantization [USE_PER_CHANNEL_QUANTIZATION [USE_PER_CHANNEL_QUANTIZATION ...]]]
                          [--use_native_input_files] [--use_native_dtype]
                          [--use_native_output_files] --input_network INPUT_NETWORK
                          [--debug [DEBUG]] [-o OUTPUT_PATH] [--copyright_file COPYRIGHT_FILE]
                          [--float_bw FLOAT_BW] [--float_bias_bw FLOAT_BIAS_BW] [--overwrite_model_prefix]
                          [--exclude_named_tensors] [--op_package_lib OP_PACKAGE_LIB]
                          [--restrict_quantization_steps ENCODING_MIN, ENCODING_MAX]
                          [--converter_op_package_lib CONVERTER_OP_PACKAGE_LIB]
                          [-p PACKAGE_NAME | --op_package_config CUSTOM_OP_CONFIG_PATHS [CUSTOM_OP_CONFIG_PATHS ...]]
                          [-h] [--arch_checker]

Script to convert PyTorch model into QNN

required arguments:
  -d INPUT_NAME INPUT_DIM, --input_dim INPUT_NAME INPUT_DIM
                        The names and dimensions of the network input layers specified in the format
                        [input_name comma-separated-
                        dimensions], for example:
                            &#39;data&#39; 1,3,224,224
                        Note that the quotes should always be included in order to handle special
                        characters, spaces, etc.
                        For multiple inputs specify multiple --input_dim on the command line like:
                            --input_dim &#39;data1&#39; 1,3,224,224 --input_dim &#39;data2&#39; 1,50,100,3
  --input_network INPUT_NETWORK, -i INPUT_NETWORK
                        Path to the source framework model.

optional arguments:
  --out_node OUT_NAMES, --out_name OUT_NAMES
                        Name of the graph&#39;s output Tensor Names. Multiple output names should be
                        provided separately like:
                            --out_name out_1 --out_name out_2
  --input_type INPUT_NAME INPUT_TYPE, -t INPUT_NAME INPUT_TYPE
                        Type of data expected by each input op/layer. Type for each input is
                        |default| if not specified. For example: &quot;data&quot; image.Note that the quotes
                        should always be included in order to handle special characters, spaces, etc.
                        For multiple inputs specify multiple --input_type on the command line.
                        Eg:
                            --input_type &quot;data1&quot; image --input_type &quot;data2&quot; opaque
                        These options get used by DSP runtime and following descriptions state how
                        input will be handled for each option.
                        Image:
                        Input is float between 0-255 and the input&#39;s mean is 0.0f and the input&#39;s
                        max is 255.0f. We will cast the float to uint8ts and pass the uint8ts to the
                        DSP.
                        Default:
                        Pass the input as floats to the dsp directly and the DSP will quantize it.
                        Opaque:
                        Assumes
                        input is float because the consumer layer(i.e next layer) requires
                        it as float, therefore it won&#39;t be quantized.
                        Choices supported:
                            image
                            default
                            opaque
  --input_dtype INPUT_NAME INPUT_DTYPE
                        The names and datatype of the network input layers specified in the format
                        [input_name datatype], for example:
                            &#39;data&#39; &#39;float32&#39;
                        Default is float32 if not specified
                        Note that the quotes should always be included in order to handlespecial
                        characters, spaces, etc.
                        For multiple inputs specify multiple --input_dtype on the command line like:
                            --input_dtype &#39;data1&#39; &#39;float32&#39; --input_dtype &#39;data2&#39; &#39;float32&#39;
  --input_encoding INPUT_ENCODING [INPUT_ENCODING ...], -e INPUT_ENCODING [INPUT_ENCODING ...]
                        Usage:     --input_encoding &quot;INPUT_NAME&quot; INPUT_ENCODING_IN
                        [INPUT_ENCODING_OUT]
                        Input encoding of the network inputs. Default is bgr.
                        e.g.
                            --input_encoding &quot;data&quot; rgba
                        Quotes must wrap the input node name to handle special characters,
                        spaces, etc. To specify encodings for multiple inputs, invoke
                        --input_encoding for each one.
                        e.g.
                            --input_encoding &quot;data1&quot; rgba --input_encoding &quot;data2&quot; other
                        Optionally, an output encoding may be specified for an input node by
                        providing a second encoding. The default output encoding is bgr.
                        e.g.
                            --input_encoding &quot;data3&quot; rgba rgb
                        Input encoding types:
                            image color encodings: bgr,rgb, nv21, nv12, ...
                            time_series: for inputs of rnn models;
                            other: not available above or is unknown.
                        Supported encodings:
                            bgr
                            rgb
                            rgba
                            argb32
                            nv21
                            nv12
                            time_series
                            other
  --input_layout INPUT_NAME INPUT_LAYOUT, -l INPUT_NAME INPUT_LAYOUT
                        Layout of each input tensor. If not specified, it will use the default
                        based on the Source Framework, shape of input and input encoding.
                        Accepted values are-
                            NCDHW, NDHWC, NCHW, NHWC, NFC, NCF, NTF, TNF, NF, NC, F, NONTRIVIAL
                        N = Batch, C = Channels, D = Depth, H = Height, W = Width, F = Feature, T = Time
                        NDHWC/NCDHW used for 5d inputs
                        NHWC/NCHW used for 4d image-like inputs
                        NFC/NCF used for inputs to Conv1D or other 1D ops
                        NTF/TNF used for inputs with time steps like the ones used for LSTM op
                        NF used for 2D inputs, like the inputs to Dense/FullyConnected layers
                        NC used for 2D inputs with 1 for batch and other for Channels (rarely used)
                        F used for 1D inputs, e.g. Bias tensor
                        NONTRIVIAL for everything elseFor multiple inputs specify multiple
                        --input_layout on the command line.
                        Eg:
                           --input_layout &quot;data1&quot; NCHW --input_layout &quot;data2&quot; NCHW
  --custom_io CUSTOM_IO
                        Use this option to specify a yaml file for custom IO.
  --preserve_io [PRESERVE_IO [PRESERVE_IO ...]]
                        Use this option to preserve IO layout and datatype. The different ways of
                        using this option are as follows:
                            --preserve_io layout &lt;space separated list of names of inputs and
                        outputs of the graph&gt;
                            --preserve_io datatype &lt;space separated list of names of inputs and
                        outputs of the graph&gt;
                        In this case, user should also specify the string - layout or datatype in
                        the command to indicate that converter needs to
                        preserve the layout or datatype. e.g.
                        --preserve_io layout input1 input2 output1
                        --preserve_io datatype input1 input2 output1
                        Optionally, the user may choose to preserve the layout and/or datatype for
                        all the inputs and outputs of the graph.
                        This can be done in the following two ways:
                            --preserve_io layout
                            --preserve_io datatype
                        Additionally, the user may choose to preserve both layout and datatypes for
                        all IO tensors by just passing the option as follows:
                            --preserve_io
                        Note: Only one of the above usages are allowed at a time.
                        Note: --custom_io gets higher precedence than --preserve_io.
  --dump_relay DUMP_RELAY
                        Dump Relay ASM and Params at the path provided with the argument
                        Usage: --dump_relay &lt;path_to_dump&gt;
  --dry_run             Evaluates the model without actually converting any ops, and
                         returns unsupported ops if any.
  --dump_out_names      Dump output names mapped from QNN CPP stored names to converter used
                        names and save to file &#39;model_output_names.json&#39;.
  --pytorch_custom_op_lib PYTORCH_CUSTOM_OP_LIB, -pcl PYTORCH_CUSTOM_OP_LIB
                        Absolute path to the PyTorch library containing the custom op definition.
                        Multiple custom op libraries must be comma-separated.
                        For PyTorch custom op details, refer to:
                             https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html
                        For custom C++ extension details, refer to:
                             https://pytorch.org/tutorials/advanced/cpp_extension.html
                        Eg. 1: --pytorch_custom_op_lib absolute_path_to/Example.so
                        Eg. 2: -pcl absolute_path_to/Example1.so,absolute_path_to/Example2.so
  --disable_batchnorm_folding
  --keep_disconnected_nodes
                        Disable Optimization that removes Ops not connected to the main graph.
                        This optimization uses output names provided over commandline OR
                        inputs/outputs extracted from the Source model to determine the main graph
  --debug [DEBUG]       Run the converter in debug mode.
  -o OUTPUT_PATH, --output_path OUTPUT_PATH
                        Path where the converted Output model should be saved.If not specified, the
                        converter model will be written to a file with same name as the input model
  --copyright_file COPYRIGHT_FILE
                        Path to copyright file. If provided, the content of the file will be added
                        to the output model.
  --float_bw FLOAT_BW   Option to select the bitwidth to use when using float for parameters (weights/
                        bias) and activations for all ops or a specific op (via encodings) selected
                        through encoding; values are 32 (default) or 16.
  --overwrite_model_prefix
                        If option passed, model generator will use the output path name to use as
                        model prefix to name functions in &lt;qnn_model_name&gt;.cpp. (Useful for running
                        multiple models at once) eg: ModelName_composeGraphs. Default is to use
                        generic &quot;QnnModel_&quot;.
  --exclude_named_tensors
                        Remove using source framework tensorNames; instead use a counter for naming
                        tensors. Note: This can potentially help to reduce the final model library
                        that will be generated(Recommended for deploying model). Default is False.
  -h, --help            show this help message and exit

Quantizer Options:
  --quantization_overrides QUANTIZATION_OVERRIDES
                        Use this option to specify a json file with parameters to use for
                        quantization. These will override any quantization data carried from
                        conversion (eg TF fake quantization) or calculated during the normal
                        quantization process. Format defined as per AIMET specification.
  --keep_quant_nodes    Use this option to keep activation quantization nodes in the graph rather
                        than stripping them.
  --input_list INPUT_LIST
                        Path to a file specifying the input data. This file should be a plain text
                        file, containing one or more absolute file paths per line. Each path is
                        expected to point to a binary file containing one input in the &quot;raw&quot; format,
                        ready to be consumed by the quantizer without any further preprocessing.
                        Multiple files per line separated by spaces indicate multiple inputs to the
                        network. See documentation for more details. Must be specified for
                        quantization. All subsequent quantization options are ignored when this is
                        not provided.
  --param_quantizer PARAM_QUANTIZER
                        Optional parameter to indicate the weight/bias quantizer to use. Must be
                        followed by one of the following options: &quot;tf&quot;: Uses the real min/max of the
                        data and specified bitwidth (default) &quot;enhanced&quot;: Uses an algorithm useful
                        for quantizing models with long tails present in the weight distribution
                        &quot;adjusted&quot;: Uses an adjusted min/max for computing the range, particularly
                        good for denoise models &quot;symmetric&quot;: Ensures min and max have the same
                        absolute values about zero. Data will be stored as int#_t data such that the
                        offset is always 0.
  --act_quantizer ACT_QUANTIZER
                        Optional parameter to indicate the activation quantizer to use. Must be
                        followed by one of the following options: &quot;tf&quot;: Uses the real min/max of the
                        data and specified bitwidth (default) &quot;enhanced&quot;: Uses an algorithm useful
                        for quantizing models with long tails present in the weight distribution
                        &quot;adjusted&quot;: Uses an adjusted min/max for computing the range, particularly
                        good for denoise models &quot;symmetric&quot;: Ensures min and max have the same
                        absolute values about zero. Data will be stored as int#_t data such that the
                        offset is always 0.
  --algorithms ALGORITHMS [ALGORITHMS ...]
                        Use this option to enable new optimization algorithms. Usage is:
                        --algorithms &lt;algo_name1&gt; ... The available optimization algorithms are:
                        &quot;cle&quot; - Cross layer equalization includes a number of methods for equalizing
                        weights and biases across layers in order to rectify imbalances that cause
                        quantization errors.
  --bias_bw BIAS_BW     Use the --bias_bw option to select the bitwidth to use when quantizing the
                        biases, either 8 (default) or 32.
  --act_bw ACT_BW       Use the --act_bw option to select the bitwidth to use when quantizing the
                        activations, either 8 (default) or 16.
  --weight_bw WEIGHT_BW
                        Use the --weight_bw option to select the bitwidth to use when quantizing the
                        weights, currently only 8 bit (default) supported.
  --float_bias_bw FLOAT_BIAS_BW
                        Use the --float_bias_bw option to select the bitwidth to use when biases are
                        in float, either 32 or 16.
  --ignore_encodings    Use only quantizer generated encodings, ignoring any user or model provided
                        encodings.
                        Note: Cannot use --ignore_encodings with --quantization_overrides
  --use_per_row_quantization
                        Use this option to enable rowwise quantization of Matmul and FullyConnected
                        ops.
  --use_per_channel_quantization [USE_PER_CHANNEL_QUANTIZATION [USE_PER_CHANNEL_QUANTIZATION ...]]
                        Use per-channel quantization for convolution-based op weights.
                        Note: This will replace built-in model QAT encodings when used for a given
                        weight.Usage &quot;--use_per_channel_quantization&quot; to enable or &quot;--
                        use_per_channel_quantization false&quot; (default) to disable
  --use_native_input_files
                        Boolean flag to indicate how to read input files:
                        1. float (default): reads inputs as floats and quantizes if necessary based
                        on quantization parameters in the model.
                        2. native: reads inputs assuming the data type to be native to the
                        model. For ex., uint8_t.
  --use_native_dtype    Note: This option is deprecated, use --use_native_input_files option in
                        future.
                        Boolean flag to indicate how to read input files:
                        1. float (default): reads inputs as floats and quantizes if necessary based
                        on quantization parameters in the model.
                        2. native: reads inputs assuming the data type to be native to the
                        model. For ex., uint8_t.
  --use_native_output_files
                        Use this option to indicate the data type of the output files
                        1. float (default): output the file as floats.
                        2. native: outputs the file that is native to the model. For ex.,
                        uint8_t.
  --restrict_quantization_steps ENCODING_MIN, ENCODING_MAX
                        Specifies the number of steps to use for computing quantization encodings
                        such that scale = (max - min) / number of quantization steps.
                        The option should be passed as a space separated pair of hexadecimal string
                        minimum and maximum values. i.e. --restrict_quantization_steps &quot;MIN MAX&quot;.
                        Please note that this is a hexadecimal string literal and not a signed
                        integer, to supply a negative value an explicit minus sign is required.
                        E.g.--restrict_quantization_steps &quot;-0x80 0x7F&quot; indicates an example 8 bit range,
                        --restrict_quantization_steps &quot;-0x8000 0x7F7F&quot; indicates an example 16
                        bit range.

Custom Op Package Options:
  --op_package_lib OP_PACKAGE_LIB, -opl OP_PACKAGE_LIB
                        Use this argument to pass an op package library for quantization. Must be in
                        the form &lt;op_package_lib_path:interfaceProviderName&gt; and be separated by a
                        comma for multiple package libs
  --converter_op_package_lib CONVERTER_OP_PACKAGE_LIB, -cpl CONVERTER_OP_PACKAGE_LIB
                        Absolute path to converter op package library compiled by the OpPackage
                        generator. Must be separated by a comma for multiple package libraries.
                        Note: Libraries must follow the same order as the xml files.
                        E.g.1: --converter_op_package_lib absolute_path_to/libExample.so
                        E.g.2: -cpl absolute_path_to/libExample1.so,absolute_path_to/libExample2.so
  -p PACKAGE_NAME, --package_name PACKAGE_NAME
                        A global package name to be used for each node in the Model.cpp file.
                        Defaults to Qnn header defined package name
  --op_package_config CUSTOM_OP_CONFIG_PATHS [CUSTOM_OP_CONFIG_PATHS ...], -opc CUSTOM_OP_CONFIG_PATHS [CUSTOM_OP_CONFIG_PATHS ...]
                        Path to a Qnn Op Package XML configuration file that contains user defined
                        custom operations.

Architecture Checker Options(Experimental):
  --arch_checker        Note: This option will be soon deprecated. Use the qnn-architecture-checker tool to achieve the same result.
</pre></div>
</div>
<p>Note: Only one of: {‘package_name’, ‘op_package_config’} can be specified</p>
<p>Basic command line usage looks like:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>$ qnn-pytorch-converter -i &lt;path&gt;/model.pt
                       -d &lt;network_input_name&gt; &lt;dims&gt;
                       -o &lt;optional_output_path&gt;
                       -p &lt;optional_package_name&gt; # Defaults to &quot;qti.aisw&quot;
</pre></div>
</div>
</div>
<div class="section" id="qnn-onnx-converter">
<h3>qnn-onnx-converter<a class="headerlink" href="#qnn-onnx-converter" title="Permalink to this heading">¶</a></h3>
<p>The <strong>qnn-onnx-converter</strong> tool converts a model from the ONNX framework to
a CPP file representing the model as a series of QNN API calls. Additionally,
a binary file containing static weights of the model is produced.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>usage: qnn-onnx-converter [--out_node OUT_NAMES] [--input_type INPUT_NAME INPUT_TYPE]
            [--input_dtype INPUT_NAME INPUT_DTYPE] [--input_encoding INPUT_ENCODING [INPUT_ENCODING ...]]
            [--input_layout INPUT_NAME INPUT_LAYOUT] [--custom_io CUSTOM_IO]
            [--dry_run [DRY_RUN]] [-d INPUT_NAME INPUT_DIM] [-n] [-b BATCH]
            [-s SYMBOL_NAME VALUE] [--preserve_io PRESERVE_IO]
            [--dump_custom_io_config_template DUMP_CUSTOM_IO_CONFIG_TEMPLATE]
            [--quantization_overrides QUANTIZATION_OVERRIDES] [--keep_quant_nodes]
            [--disable_batchnorm_folding] [--keep_disconnected_nodes]
            [--input_list INPUT_LIST] [--param_quantizer PARAM_QUANTIZER]
            [--act_quantizer ACT_QUANTIZER] [--algorithms ALGORITHMS [ALGORITHMS ...]]
            [--bias_bw BIAS_BW] [--act_bw ACT_BW] [--weight_bw WEIGHT_BW]
            [--ignore_encodings] [--use_per_row_quantization]
            [--use_per_channel_quantization [USE_PER_CHANNEL_QUANTIZATION [USE_PER_CHANNEL_QUANTIZATION ...]]]
            [--use_native_input_files] [--use_native_dtype]
            [--use_native_output_files] --input_network INPUT_NETWORK
            [--debug [DEBUG]] [-o OUTPUT_PATH] [--copyright_file COPYRIGHT_FILE]
            [--float_bw FLOAT_BW] [--float_bias_bw FLOAT_BIAS_BW] [--overwrite_model_prefix] [--exclude_named_tensors]
            [--restrict_quantization_steps ENCODING_MIN, ENCODING_MAX]
            [--op_package_lib OP_PACKAGE_LIB]
            [--converter_op_package_lib CONVERTER_OP_PACKAGE_LIB]
            [-p PACKAGE_NAME | --op_package_config CUSTOM_OP_CONFIG_PATHS [CUSTOM_OP_CONFIG_PATHS ...]]
            [-h] [--arch_checker]

Script to convert ONNX model into QNN

required arguments:
  --input_network INPUT_NETWORK, -i INPUT_NETWORK
                        Path to the source framework model.

optional arguments:
  --out_node OUT_NAMES, --out_name OUT_NAMES
                        Name of the graph&#39;s output tensor names. Multiple output
                        nodes should be provided separately like:
                            --out_name out_1 --out_name out_2
  --input_type INPUT_NAME INPUT_TYPE, -t INPUT_NAME INPUT_TYPE
                        Type of data expected by each input op/layer. Type for
                        each input is |default| if not specified. For example:
                        &quot;data&quot; image.Note that the quotes should always be
                        included in order to handle special characters,
                        spaces,etc. For multiple inputs specify multiple
                        --input_type on the command line. Eg:
                            --input_type &quot;data1&quot; image --input_type &quot;data2&quot; opaque
                        These options get used by DSP runtime and following
                        descriptions state how input will be handled for each
                        option.
                        Image:
                        Input is float between 0-255 and the input&#39;s mean is 0.0f and the input&#39;s
                        max is 255.0f. We will cast the float to uint8ts and pass the uint8ts to
                        the DSP.
                        Default:
                        Pass the input as floats to the dsp
                        directly and the DSP will quantize it.
                        Opaque:
                        Assumes input is float because the consumer layer(i.e next
                        layer) requires it as float, therefore it won&#39;t be
                        quantized.
                        Choices supported:
                            image
                            default
                            opaque
  --input_dtype INPUT_NAME INPUT_DTYPE
                        The names and datatype of the network input layers
                        specified in the format [input_name datatype], for
                        example:
                            &#39;data&#39; &#39;float32&#39;.
                        Default is float32 if not specified.
                        Note that the quotes should always be included in order to handle special
                        characters, spaces, etc.
                        For multiple inputs specify multiple --input_dtype on the command line like:
                            --input_dtype &#39;data1&#39; &#39;float32&#39; --input_dtype &#39;data2&#39; &#39;float32&#39;
  --input_encoding INPUT_ENCODING [INPUT_ENCODING ...], -e INPUT_ENCODING [INPUT_ENCODING ...]
                        Usage:     --input_encoding &quot;INPUT_NAME&quot; INPUT_ENCODING_IN
                        [INPUT_ENCODING_OUT]
                        e.g.
                            --input_encoding &quot;data&quot; rgba
                        Quotes must wrap the input node name to handle special characters,
                        spaces, etc. To specify encodings for multiple inputs, invoke
                        --input_encoding for each one.
                        e.g.
                            --input_encoding &quot;data1&quot; rgba --input_encoding &quot;data2&quot; other
                        Optionally, an output encoding may be specified for an input node by
                        providing a second encoding. The default output encoding is bgr.
                        e.g.
                            --input_encoding &quot;data3&quot; rgba rgb
                        Input encoding types:
                            image color encodings: bgr,rgb, nv21, nv12, ...
                            time_series: for inputs of rnn models;
                            other: not available above or is unknown.
                        Supported encodings:
                            bgr
                            rgb
                            rgba
                            argb32
                            nv21
                            nv12
                            time_series
                            other
  --input_layout INPUT_NAME INPUT_LAYOUT, -l INPUT_NAME INPUT_LAYOUT
                        Layout of each input tensor. If not specified, it will use the default
                        based on the Source Framework, shape of input and input encoding.
                        Accepted values are-
                            NCDHW, NDHWC, NCHW, NHWC, NFC, NCF, NTF, TNF, NF, NC, F, NONTRIVIAL
                        N = Batch, C = Channels, D = Depth, H = Height, W = Width, F = Feature, T = Time
                        NDHWC/NCDHW used for 5d inputs
                        NHWC/NCHW used for 4d image-like inputs
                        NFC/NCF used for inputs to Conv1D or other 1D ops
                        NTF/TNF used for inputs with time steps like the ones used for LSTM op
                        NF used for 2D inputs, like the inputs to Dense/FullyConnected layers
                        NC used for 2D inputs with 1 for batch and other for Channels (rarely used)
                        F used for 1D inputs, e.g. Bias tensor
                        NONTRIVIAL for everything elseFor multiple inputs specify multiple
                        --input_layout on the command line.
                        Eg:
                            --input_layout &quot;data1&quot; NCHW --input_layout &quot;data2&quot; NCHW
                        Note: This flag does not set the layout of the input tensor in the converted DLC.
                            Please use --custom_io for that.
  --custom_io CUSTOM_IO
                        Use this option to specify a yaml file for custom IO.
  --preserve_io PRESERVE_IO
                        Use this option to preserve IO layout and datatype. The different ways of using
                        this option are as follows:
                            --preserve_io layout &lt;space separated list of names of inputs and outputs of the graph&gt;
                            --preserve_io datatype &lt;space separated list of names of inputs and outputs of the graph&gt;
                        In this case, user should also specify the string - layout or datatype in the command
                        to indicate that converter needs to preserve the layout or datatype. e.g.
                            --preserve_io layout input1 input2 output1
                            --preserve_io datatype input1 input2 output1
                        Optionally, the user may choose to preserve the layout and/or datatype for all
                        the inputs and outputs of the graph. This can be done in the following two ways:
                            --preserve_io layout
                            --preserve_io datatype
                        Additionally, the user may choose to preserve both layout and datatypes for all
                        IO tensors by just passing the option as follows:
                            --preserve_io
                        Note: Only one of the above usages are allowed at a time.
                        Note: --custom_io gets higher precedence than --preserve_io.
  --dry_run [DRY_RUN]   Evaluates the model without actually converting any ops, and returns
                        unsupported ops/attributes as well as unused inputs and/or outputs if any.
                        Leave empty or specify &quot;info&quot; to see dry run as a table, or specify &quot;debug&quot;
                        to show more detailed messages only&quot;
  -d INPUT_NAME INPUT_DIM, --input_dim INPUT_NAME INPUT_DIM
                        The name and dimension of all the input buffers to the network specified in
                        the format [input_name comma-separated-dimensions],
                        for example: &#39;data&#39; 1,224,224,3.
                        Note that the quotes should always be included in order to handle special
                        characters, spaces, etc.
                        NOTE: This feature works only with Onnx 1.6.0 and above
  -n, --no_simplification
                        Do not attempt to simplify the model automatically. This may prevent some
                        models from properly converting
  -b BATCH, --batch BATCH
                        The batch dimension override. This will take the first dimension of all
                        inputs and treat it as a batch dim, overriding it with the value provided
                        here. For example:
                        --batch 6
                        will result in a shape change from [1,3,224,224] to [6,3,224,224].
                        If there are inputs without batch dim this should not be used and each input
                        should be overridden independently using -d option for input dimension
                        overrides.
  -s SYMBOL_NAME VALUE, --define_symbol SYMBOL_NAME VALUE
                        This option allows overriding specific input dimension symbols. For instance
                        you might see input shapes specified with variables such as :
                        data: [1,3,height,width]
                        To override these simply pass the option as:
                        --define_symbol height 224 --define_symbol width 448
                        which results in dimensions that look like:
                        data: [1,3,224,448]
  --dump_custom_io_config_template
                        Dumps the yaml template for Custom I/O configuration. This file can be edited
                        as per the custom requirements and passed using the option --custom_ioUse
                        this option to specify a yaml file to which the custom IO config template is
                        dumped.
  --disable_batchnorm_folding
  --keep_disconnected_nodes
                        Disable Optimization that removes Ops not connected to the main graph.
                        This optimization uses output names provided over commandline OR
                        inputs/outputs extracted from the Source model to determine the main graph
  --debug [DEBUG]       Run the converter in debug mode.
  -o OUTPUT_PATH, --output_path OUTPUT_PATH
                        Path where the converted Output model should be saved.If not specified, the
                        converter model will be written to a file with same name as the input model
  --copyright_file COPYRIGHT_FILE
                        Path to copyright file. If provided, the content of the file will be added
                        to the output model.
  --float_bw FLOAT_BW   Option to select the bitwidth to use when using float for parameters (weights/
                        bias) and activations for all ops or a specific op (via encodings) selected
                        through encoding; values are 32 (default) or 16.
  --overwrite_model_prefix
                        If option passed, model generator will use the output path name to use as
                        model prefix to name functions in &lt;qnn_model_name&gt;.cpp. (Useful for running
                        multiple models at once) eg: ModelName_composeGraphs. Default is to use
                        generic &quot;QnnModel_&quot;.
  --exclude_named_tensors
                        Remove using source framework tensorNames; instead use a counter for naming
                        tensors. Note: This can potentially help to reduce  the final model library
                        that will be generated(Recommended for deploying model). Default is False.
  -h, --help            show this help message and exit

Quantizer Options:
  --quantization_overrides QUANTIZATION_OVERRIDES
                        Use this option to specify a json file with parameters to use for
                        quantization. These will override any quantization data carried from
                        conversion (eg TF fake quantization) or calculated during the normal
                        quantization process. Format defined as per AIMET specification.

  --keep_quant_nodes    Use this option to keep activation quantization nodes in the graph rather
                        than stripping them.
  --input_list INPUT_LIST
                        Path to a file specifying the input data. This file should be a plain text
                        file, containing one or more absolute file paths per line. Each path is
                        expected to point to a binary file containing one input in the &quot;raw&quot; format,
                        ready to be consumed by the quantizer without any further preprocessing.
                        Multiple files per line separated by spaces indicate multiple inputs to the
                        network. See documentation for more details. Must be specified for
                        quantization. All subsequent quantization options are ignored when this is
                        not provided.
  --param_quantizer PARAM_QUANTIZER
                        Optional parameter to indicate the weight/bias quantizer to use. Must be
                        followed by one of the following options: &quot;tf&quot;: Uses the real min/max of the
                        data and specified bitwidth (default) &quot;enhanced&quot;: Uses an algorithm useful
                        for quantizing models with long tails present in the weight distribution
                        &quot;adjusted&quot;: Uses an adjusted min/max for computing the range, particularly
                        good for denoise models &quot;symmetric&quot;: Ensures min and max have the same
                        absolute values about zero. Data will be stored as int#_t data such that the
                        offset is always 0.
  --act_quantizer ACT_QUANTIZER
                        Optional parameter to indicate the activation quantizer to use. Must be
                        followed by one of the following options: &quot;tf&quot;: Uses the real min/max of the
                        data and specified bitwidth (default) &quot;enhanced&quot;: Uses an algorithm useful
                        for quantizing models with long tails present in the weight distribution
                        &quot;adjusted&quot;: Uses an adjusted min/max for computing the range, particularly
                        good for denoise models &quot;symmetric&quot;: Ensures min and max have the same
                        absolute values about zero. Data will be stored as int#_t data such that the
                        offset is always 0.
  --algorithms ALGORITHMS [ALGORITHMS ...]
                        Use this option to enable new optimization algorithms. Usage is:
                        --algorithms &lt;algo_name1&gt; ... The available optimization algorithms are:
                        &quot;cle&quot; - Cross layer equalization includes a number of methods for equalizing
                        weights and biases across layers in order to rectify imbalances that cause
                        quantization errors.

  --bias_bw BIAS_BW     Use the --bias_bw option to select the bitwidth to use when quantizing the
                        biases, either 8 (default) or 32.
  --act_bw ACT_BW       Use the --act_bw option to select the bitwidth to use when quantizing the
                        activations, either 8 (default) or 16.
  --weight_bw WEIGHT_BW
                        Use the --weight_bw option to select the bitwidth to use when quantizing the
                        weights, currently only 8 bit (default) supported.
  --float_bias_bw FLOAT_BIAS_BW
                        Use the --float_bias_bw option to select the bitwidth to use when biases are
                        in float, either 32 or 16.
  --ignore_encodings    Use only quantizer generated encodings, ignoring any user or model provided
                        encodings.
                        Note: Cannot use --ignore_encodings with --quantization_overrides
  --use_per_row_quantization
                        Use this option to enable rowwise quantization of Matmul and FullyConnected
                        ops.
  --use_per_channel_quantization [USE_PER_CHANNEL_QUANTIZATION [USE_PER_CHANNEL_QUANTIZATION ...]]
                        Use per-channel quantization for convolution-based op weights.
                        Note: This will replace built-in model QAT encodings when used for a given
                        weight.Usage &quot;--use_per_channel_quantization&quot; to enable or &quot;--
                        use_per_channel_quantization false&quot; (default) to disable
  --use_native_input_files
                        Boolean flag to indicate how to read input files:
                        1. float (default): reads inputs as floats and quantizes if necessary based
                        on quantization parameters in the model.
                        2. native: reads inputs assuming the data type to be native to the
                        model. For ex., uint8_t.
  --use_native_dtype    Note: This option is deprecated, use --use_native_input_files option in
                        future.
                        Boolean flag to indicate how to read input files:
                        1. float (default): reads inputs as floats and quantizes if necessary based
                        on quantization parameters in the model.
                        2. native: reads inputs assuming the data type to be native to the
                        model. For ex., uint8_t.
  --use_native_output_files
                        Use this option to indicate the data type of the output files
                        1. float (default): output the file as floats.
                        2. native:          outputs the file that is native to the model. For ex.,
                        uint8_t.
  --restrict_quantization_steps ENCODING_MIN, ENCODING_MAX
                        Specifies the number of steps to use for computing quantization encodings
                        such that scale = (max - min) / number of quantization steps.
                        The option should be passed as a space separated pair of hexadecimal string
                        minimum and maximum values. i.e. --restrict_quantization_steps &quot;MIN MAX&quot;.
                        Please note that this is a hexadecimal string literal and not a signed
                        integer, to supply a negative value an explicit minus sign is required.
                        E.g.--restrict_quantization_steps &quot;-0x80 0x7F&quot; indicates an example 8 bit range,
                        --restrict_quantization_steps &quot;-0x8000 0x7F7F&quot; indicates an example 16
                        bit range. This argument is required for 16-bit Matmul operations.

Custom Op Package Options:
  --op_package_lib OP_PACKAGE_LIB, -opl OP_PACKAGE_LIB
                        Use this argument to pass an op package library for quantization. Must be in
                        the form &lt;op_package_lib_path:interfaceProviderName&gt; and be separated by a
                        comma for multiple package libs
  --converter_op_package_lib CONVERTER_OP_PACKAGE_LIB, -cpl CONVERTER_OP_PACKAGE_LIB
                        Absolute path to converter op package library compiled by the OpPackage
                        generator. Must be separated by a comma for multiple package libraries.
                        Note: Libraries must follow the same order as the xml files.
                        E.g.1: --converter_op_package_lib absolute_path_to/libExample.so
                        E.g.2: -cpl absolute_path_to/libExample1.so,absolute_path_to/libExample2.so
  -p PACKAGE_NAME, --package_name PACKAGE_NAME
                        A global package name to be used for each node in the Model.cpp file.
                        Defaults to Qnn header defined package name
  --op_package_config OP_PACKAGE_CONFIG [OP_PACKAGE_CONFIG ...], -opc OP_PACKAGE_CONFIG [OP_PACKAGE_CONFIG ...]
                        Path to a Qnn Op Package XML configuration file that contains user defined
                        custom operations.

Architecture Checker Options(Experimental):
  --arch_checker        Note: This option will be soon deprecated. Use the qnn-architecture-checker tool to achieve the same result.

Note: Only one of: {&#39;package_name&#39;, &#39;op_package_config&#39;} can be specified
</pre></div>
</div>
</div>
<div class="section" id="qairt-converter">
<h3>qairt-converter<a class="headerlink" href="#qairt-converter" title="Permalink to this heading">¶</a></h3>
<p>The <strong>qairt-converter</strong> tool converts a model from the one of Onnx/TensorFlow/TFLite/PyTorch framework to
a DLC file representing the QNN graph format that can enable inference on Qualcomm AI IP/HW. The converter auto detects
the framework based on the source model extension.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>Basic command line usage looks like:

usage: qairt-converter [--desired_input_shape INPUT_NAME INPUT_DIM] [--out_tensor_node OUT_NAMES]
                       [--source_model_input_datatype INPUT_NAME INPUT_DTYPE]
                       [--source_model_input_layout INPUT_NAME INPUT_LAYOUT]
                       [--desired_input_color_encoding  [ ...]]
                       [--dump_io_config_template DUMP_IO_CONFIG_TEMPLATE] [--io_config IO_CONFIG]
                       [--dry_run [DRY_RUN]] [--quantization_overrides QUANTIZATION_OVERRIDES]
                       [--onnx_no_simplification] [--onnx_batch BATCH]
                       [--onnx_define_symbol SYMBOL_NAME VALUE] [--tf_no_optimization]
                       [--tf_show_unconsumed_nodes] [--tf_saved_model_tag SAVED_MODEL_TAG]
                       [--tf_saved_model_signature_key SAVED_MODEL_SIGNATURE_KEY]
                       [--tf_validate_models] [--tflite_signature_name SIGNATURE_NAME]
                       --input_network INPUT_NETWORK [-h] [--debug [DEBUG]]
                       [--output_path OUTPUT_PATH] [--copyright_file COPYRIGHT_FILE]
                       [--float_bitwidth FLOAT_BITWIDTH] [--float_bias_bitwidth FLOAT_BIAS_BITWIDTH]
                       [--model_version MODEL_VERSION] [--converter_op_package_lib CONVERTER_OP_PACKAGE_LIB]
                       [--package_name PACKAGE_NAME | --op_package_config CUSTOM_OP_CONFIG_PATHS [CUSTOM_OP_CONFIG_PATHS ...]]

required arguments:
  --input_network INPUT_NETWORK, -i INPUT_NETWORK
                        Path to the source framework model.

optional arguments:
  --desired_input_shape INPUT_NAME INPUT_DIM, -d INPUT_NAME INPUT_DIM
                        The name and dimension of all the input buffers to the network specified in
                        the format [input_name comma-separated-dimensions],
                        for example: &#39;data&#39; 1,224,224,3.
                        Note that the quotes should always be included in order to handle special
                        characters, spaces, etc.
                        NOTE: Required for TensorFlow and PyTorch. Optional for Onnx and Tflite
                        In case of Onnx, this feature works only with Onnx 1.6.0 and above
  --out_tensor_node OUT_NAMES, --out_tensor_name OUT_NAMES
                        Name of the graph&#39;s output Tensor Names. Multiple output names should be
                        provided separately like:
                            --out_name out_1 --out_name out_2
                        NOTE: Required for TensorFlow. Optional for Onnx, Tflite and PyTorch
  --source_model_input_datatype INPUT_NAME INPUT_DTYPE
                        The names and datatype of the network input layers specified in the format
                        [input_name datatype], for example:
                            &#39;data&#39; &#39;float32&#39;
                        Default is float32 if not specified
                        Note that the quotes should always be included in order to handlespecial
                        characters, spaces, etc.
                        For multiple inputs specify multiple --input_dtype on the command line like:
                            --input_dtype &#39;data1&#39; &#39;float32&#39; --input_dtype &#39;data2&#39; &#39;float32&#39;
  --source_model_input_layout INPUT_NAME INPUT_LAYOUT, -l INPUT_NAME INPUT_LAYOUT
                        Layout of each input tensor. If not specified, it will use the default
                        based on the Source Framework, shape of input and input encoding.
                        Accepted values are-
                            NCDHW, NDHWC, NCHW, NHWC, NFC, NCF, NTF, TNF, NF, NC, F, NONTRIVIAL
                        N = Batch, C = Channels, D = Depth, H = Height, W = Width, F = Feature, T =
                        Time
                        NDHWC/NCDHW used for 5d inputs
                        NHWC/NCHW used for 4d image-like inputs
                        NFC/NCF used for inputs to Conv1D or other 1D ops
                        NTF/TNF used for inputs with time steps like the ones used for LSTM op
                        NF used for 2D inputs, like the inputs to Dense/FullyConnected layers
                        NC used for 2D inputs with 1 for batch and other for Channels (rarely used)
                        F used for 1D inputs, e.g. Bias tensor
                        NONTRIVIAL for everything elseFor multiple inputs specify multiple
                        --input_layout on the command line.
                        Eg:
                            --input_layout &quot;data1&quot; NCHW --input_layout &quot;data2&quot; NCHW
  --desired_input_color_encoding  [ ...], -e  [ ...]
                        Usage:     --input_color_encoding &quot;INPUT_NAME&quot; INPUT_ENCODING_IN
                        [INPUT_ENCODING_OUT]
                        Input encoding of the network inputs. Default is bgr.
                        e.g.
                           --input_color_encoding &quot;data&quot; rgba
                        Quotes must wrap the input node name to handle special characters,
                        spaces, etc. To specify encodings for multiple inputs, invoke
                        --input_color_encoding for each one.
                        e.g.
                            --input_color_encoding &quot;data1&quot; rgba --input_color_encoding &quot;data2&quot; other
                        Optionally, an output encoding may be specified for an input node by
                        providing a second encoding. The default output encoding is bgr.
                        e.g.
                            --input_color_encoding &quot;data3&quot; rgba rgb
                        Input encoding types:
                             image color encodings: bgr,rgb, nv21, nv12, ...
                            time_series: for inputs of rnn models;
                            other: not available above or is unknown.
                        Supported encodings:
                           bgr
                           rgb
                           rgba
                           argb32
                           nv21
                           nv12
  --dump_io_config_template DUMP_IO_CONFIG_TEMPLATE
                        Dumps the yaml template for I/O configuration. This file can be edited as
                        per the custom requirements and passed using the option --io_configUse this
                        option to specify a yaml file to which the IO config template is dumped.
  --io_config IO_CONFIG
                        Use this option to specify a yaml file for input and output options.
  --dry_run [DRY_RUN]   Evaluates the model without actually converting any ops, and returns
                        unsupported ops/attributes as well as unused inputs and/or outputs if any.
  -h, --help            show this help message and exit
  --debug [DEBUG]       Run the converter in debug mode.
  --output_path OUTPUT_PATH, -o OUTPUT_PATH
                        Path where the converted Output model should be saved.If not specified, the
                        converter model will be written to a file with same name as the input model
  --copyright_file COPYRIGHT_FILE
                        Path to copyright file. If provided, the content of the file will be added
                        to the output model.
  --float_bitwidth FLOAT_BITWIDTH
                        Use the --float_bitwidth option to convert the graph to the specified float
                        bitwidth, either 32 (default) or 16.
  --float_bias_bitwidth FLOAT_BIAS_BITWIDTH
                        Use the --float_bias_bitwidth option to select the bitwidth to use for float
                        bias tensor
  --model_version MODEL_VERSION
                        User-defined ASCII string to identify the model, only first 64 bytes will be
                        stored

Custom Op Package Options:
  --converter_op_package_lib CONVERTER_OP_PACKAGE_LIB, -cpl CONVERTER_OP_PACKAGE_LIB
                        Absolute path to converter op package library compiled by the OpPackage
                        generator. Must be separated by a comma for multiple package libraries.
                        Note: Order of converter op package libraries must follow the order of xmls.
                        Ex1: --converter_op_package_lib absolute_path_to/libExample.so
                        Ex2: -cpl absolute_path_to/libExample1.so,absolute_path_to/libExample2.so
  --package_name PACKAGE_NAME, -p PACKAGE_NAME
                        A global package name to be used for each node in the Model.cpp file.
                        Defaults to Qnn header defined package name
  --op_package_config CUSTOM_OP_CONFIG_PATHS [CUSTOM_OP_CONFIG_PATHS ...], -opc CUSTOM_OP_CONFIG_PATHS [CUSTOM_OP_CONFIG_PATHS ...]
                        Path to a Qnn Op Package XML configuration file that contains user defined
                        custom operations.

Quantizer Options:
  --quantization_overrides QUANTIZATION_OVERRIDES
                        Use this option to specify a json file with parameters to use for
                        quantization. These will override any quantization data carried from
                        conversion (eg TF fake quantization) or calculated during the normal
                        quantization process. Format defined as per AIMET specification.

Onnx Converter Options:
  --onnx_no_simplification
                        Do not attempt to simplify the model automatically. This may prevent some
                        models from properly converting
                        when sequences of unsupported static operations are present.
  --onnx_batch BATCH    The batch dimension override. This will take the first dimension of all
                        inputs and treat it as a batch dim, overriding it with the value provided
                        here. For example:
                        --batch 6
                        will result in a shape change from [1,3,224,224] to [6,3,224,224].
                        If there are inputs without batch dim this should not be used and each input
                        should be overridden independently using -d option for input dimension
                        overrides.
  --onnx_define_symbol SYMBOL_NAME VALUE
                        This option allows overriding specific input dimension symbols. For instance
                        you might see input shapes specified with variables such as :
                        data: [1,3,height,width]
                        To override these simply pass the option as:
                        --define_symbol height 224 --define_symbol width 448
                        which results in dimensions that look like:
                        data: [1,3,224,448]

TensorFlow Converter Options:
  --tf_no_optimization  Do not attempt to optimize the model automatically.
  --tf_show_unconsumed_nodes
                        Displays a list of unconsumed nodes, if there any are found. Nodeswhich are
                        unconsumed do not violate the structural fidelity of thegenerated graph.
  --tf_saved_model_tag SAVED_MODEL_TAG
                        Specify the tag to seletet a MetaGraph from savedmodel. ex:
                        --saved_model_tag serve. Default value will be &#39;serve&#39; when it is not
                        assigned.
  --tf_saved_model_signature_key SAVED_MODEL_SIGNATURE_KEY
                        Specify signature key to select input and output of the model. ex:
                        --saved_model_signature_key serving_default. Default value will be
                        &#39;serving_default&#39; when it is not assigned
  --tf_validate_models  Validate the original TF model against optimized TF model.
                        Constant inputs with all value 1s will be generated and will be used
                        by both models and their outputs are checked against each other.
                        The % average error and 90th percentile of output differences will be
                        calculated for this.
                        Note: Usage of this flag will incur extra time due to inference of the
                        models.

Tflite Converter Options:
  --tflite_signature_name SIGNATURE_NAME
                        Use this option to specify a specific Subgraph signature to convert
</pre></div>
</div>
</div>
</div>
<div class="section" id="model-preparation">
<h2>Model Preparation<a class="headerlink" href="#model-preparation" title="Permalink to this heading">¶</a></h2>
<div class="section" id="quantization-support">
<h3>Quantization Support<a class="headerlink" href="#quantization-support" title="Permalink to this heading">¶</a></h3>
<p>Quantization is supported through the converter interface and is performed at
conversion time. The only required option to enable quantization along with
conversion is the –input_list option, which provides the quantizer with the
required input data for the given model. The following options are available
in each converter listed above to enable and configure quantization:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>Quantizer Options:
--quantization_overrides QUANTIZATION_OVERRIDES
                        Use this option to specify a json file with parameters
                        to use for quantization. These will override any
                        quantization data carried from conversion (eg TF fake
                        quantization) or calculated during the normal
                        quantization process. Format defined as per AIMET
                        specification.
--input_list INPUT_LIST
                      Path to a file specifying the input data. This file
                      should be a plain text file, containing one or more
                      absolute file paths per line. Each path is expected to
                      point to a binary file containing one input in the
                      &quot;raw&quot; format, ready to be consumed by the quantizer
                      without any further preprocessing. Multiple files per
                      line separated by spaces indicate multiple inputs to
                      the network. See documentation for more details. Must
                      be specified for quantization. All subsequent
                      quantization options are ignored when this is not
                      provided.
--param_quantizer PARAM_QUANTIZER
                      Optional parameter to indicate the weight/bias
                      quantizer to use. Must be followed by one of the
                      following options: &quot;tf&quot;: Uses the real min/max of the
                      data and specified bitwidth (default) &quot;enhanced&quot;: Uses
                      an algorithm useful for quantizing models with long
                      tails present in the weight distribution &quot;adjusted&quot;:
                      Uses an adjusted min/max for computing the range,
                      particularly good for denoise models &quot;symmetric&quot;:
                      Ensures min and max have the same absolute values
                      about zero. Data will be stored as int#_t data such
                      that the offset is always 0.
--act_quantizer ACT_QUANTIZER
                      Optional parameter to indicate the activation
                      quantizer to use. Must be followed by one of the
                      following options: &quot;tf&quot;: Uses the real min/max of the
                      data and specified bitwidth (default) &quot;enhanced&quot;: Uses
                      an algorithm useful for quantizing models with long
                      tails present in the weight distribution &quot;adjusted&quot;:
                      Uses an adjusted min/max for computing the range,
                      particularly good for denoise models &quot;symmetric&quot;:
                      Ensures min and max have the same absolute values
                      about zero. Data will be stored as int#_t data such
                      that the offset is always 0.
--algorithms ALGORITHMS [ALGORITHMS ...]
                      Use this option to enable new optimization algorithms.
                      Usage is: --algorithms &lt;algo_name1&gt; ... The
                      available optimization algorithms are: &quot;cle&quot; - Cross
                      layer equalization includes a number of methods for
                      equalizing weights and biases across layers in order
                      to rectify imbalances that cause quantization errors.
--bias_bw BIAS_BW     Use the --bias_bw option to select the bitwidth to use
                      when quantizing the biases, either 8 (default) or 32.
--act_bw ACT_BW       Use the --act_bw option to select the bitwidth to use
                      when quantizing the activations, either 8 (default) or
                      16.
--weight_bw WEIGHT_BW
                      Use the --weight_bw option to select the bitwidth to
                      use when quantizing the weights, currently only 8 bit
                      (default) supported.
--float_bias_bw FLOAT_BIAS_BW
                      Use the --float_bias_bw option to select the bitwidth to
                      use when biases are in float, either 32 or 16.
--ignore_encodings    Use only quantizer generated encodings, ignoring any
                      user or model provided encodings. Note: Cannot use
                      --ignore_encodings with --quantization_overrides
--use_per_channel_quantization [USE_PER_CHANNEL_QUANTIZATION [USE_PER_CHANNEL_QUANTIZATION ...]]
                      Use per-channel quantization for
                      convolution-based op weights. Note: This will replace
                      built-in model QAT encodings when used for a given
                      weight.Usage &quot;--use_per_channel_quantization&quot; to
                      enable or &quot;--use_per_channel_quantization false&quot;
                      (default) to disable
--use_per_row_quantization [USE_PER_ROW_QUANTIZATION [USE_PER_ROW_QUANTIZATION ...]]
                      Use this option to enable rowwise quantization of Matmul and
                      FullyConnected op. Usage &quot;--use_per_row_quantization&quot; to enable
                      or &quot;--use_per_row_quantization false&quot; (default) to
                      disable. This option may not be supported by all backends.
</pre></div>
</div>
<p>Basic command line usage to convert and quantize a model using the TF converter would look like:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>$ qnn-tensorflow-converter -i &lt;path&gt;/frozen_graph.pb
                    -d &lt;network_input_name&gt; &lt;dims&gt;
                    --out_node &lt;network_output_name&gt;
                    -o &lt;optional_output_path&gt;
                    --allow_unconsumed_nodes  # optional, but most likely will be need for larger models
                    -p &lt;optional_package_name&gt; # Defaults to &quot;qti.aisw&quot;
                    --input_list input_list.txt
</pre></div>
</div>
<p>This will quantize the network using the default quantizer and bitwidths (8 bits for activations, weights, and biases).</p>
<p>For more detailed information on quantization, options, and algorithms please refer to <a class="reference internal" href="quantization.html"><span class="doc">Quantization</span></a>.</p>
</div>
<div class="section" id="qairt-quantizer">
<h3>qairt-quantizer<a class="headerlink" href="#qairt-quantizer" title="Permalink to this heading">¶</a></h3>
<p>The <strong>qairt-quantizer</strong> tool converts non-quantized DLC models into quantized DLC models.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">Basic</span><span class="w"> </span><span class="n">command</span><span class="w"> </span><span class="n">line</span><span class="w"> </span><span class="n">usage</span><span class="w"> </span><span class="n">looks</span><span class="w"> </span><span class="n">like</span><span class="o">:</span>

<span class="nl">usage</span><span class="p">:</span><span class="w"> </span><span class="n">qairt</span><span class="o">-</span><span class="n">quantizer</span><span class="w"> </span><span class="o">--</span><span class="n">input_dlc</span><span class="w"> </span><span class="n">INPUT_DLC</span><span class="w"> </span><span class="p">[</span><span class="o">-</span><span class="n">h</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">output_dlc</span><span class="w"> </span><span class="n">OUTPUT_DLC</span><span class="p">]</span>
<span class="w">                   </span><span class="p">[</span><span class="o">--</span><span class="n">input_list</span><span class="w"> </span><span class="n">INPUT_LIST</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">float_fallback</span><span class="p">]</span>
<span class="w">                   </span><span class="p">[</span><span class="o">--</span><span class="n">algorithms</span><span class="w"> </span><span class="n">ALGORITHMS</span><span class="w"> </span><span class="p">[</span><span class="n">ALGORITHMS</span><span class="w"> </span><span class="p">...]]</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">bias_bitwidth</span><span class="w"> </span><span class="n">BIAS_BITWIDTH</span><span class="p">]</span>
<span class="w">                   </span><span class="p">[</span><span class="o">--</span><span class="n">act_bitwidth</span><span class="w"> </span><span class="n">ACT_BITWIDTH</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">weights_bitwidth</span><span class="w"> </span><span class="n">WEIGHTS_BITWIDTH</span><span class="p">]</span>
<span class="w">                   </span><span class="p">[</span><span class="o">--</span><span class="n">float_bitwidth</span><span class="w"> </span><span class="n">FLOAT_BITWIDTH</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">float_bias_bitwidth</span><span class="w"> </span><span class="n">FLOAT_BIAS_BITWIDTH</span><span class="p">]</span>
<span class="w">                   </span><span class="p">[</span><span class="o">--</span><span class="n">ignore_encodings</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">use_per_channel_quantization</span><span class="p">]</span>
<span class="w">                   </span><span class="p">[</span><span class="o">--</span><span class="n">use_per_row_quantization</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">use_native_input_files</span><span class="p">]</span>
<span class="w">                   </span><span class="p">[</span><span class="o">--</span><span class="n">use_native_output_files</span><span class="p">]</span>
<span class="w">                   </span><span class="p">[</span><span class="o">--</span><span class="n">restrict_quantization_steps</span><span class="w"> </span><span class="n">ENCODING_MIN</span><span class="p">,</span><span class="w"> </span><span class="n">ENCODING_MAX</span><span class="p">]</span>
<span class="w">                   </span><span class="p">[</span><span class="o">--</span><span class="n">act_quantizer_calibration</span><span class="w"> </span><span class="n">ACT_QUANTIZER_CALIBRATION</span><span class="p">]</span>
<span class="w">                   </span><span class="p">[</span><span class="o">--</span><span class="n">param_quantizer_calibration</span><span class="w"> </span><span class="n">PARAM_QUANTIZER_CALIBRATION</span><span class="p">]</span>
<span class="w">                   </span><span class="p">[</span><span class="o">--</span><span class="n">act_quantizer_schema</span><span class="w"> </span><span class="n">ACT_QUANTIZER_SCHEMA</span><span class="p">]</span>
<span class="w">                   </span><span class="p">[</span><span class="o">--</span><span class="n">param_quantizer_schema</span><span class="w"> </span><span class="n">PARAM_QUANTIZER_SCHEMA</span><span class="p">]</span>
<span class="w">                   </span><span class="p">[</span><span class="o">--</span><span class="n">percentile_calibration_value</span><span class="w"> </span><span class="n">PERCENTILE_CALIBRATION_VALUE</span><span class="p">]</span>
<span class="w">                   </span><span class="p">[</span><span class="o">--</span><span class="n">use_aimet_quantizer</span><span class="p">]</span>
<span class="w">                   </span><span class="p">[</span><span class="o">--</span><span class="n">op_package_lib</span><span class="w"> </span><span class="n">OP_PACKAGE_LIB</span><span class="p">]</span>
<span class="w">                   </span><span class="p">[</span><span class="o">--</span><span class="n">dump_encoding_json</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">debug</span><span class="w"> </span><span class="p">[</span><span class="n">DEBUG</span><span class="p">]]</span>

<span class="n">required</span><span class="w"> </span><span class="n">arguments</span><span class="o">:</span>
<span class="w">  </span><span class="o">--</span><span class="n">input_dlc</span><span class="w"> </span><span class="n">INPUT_DLC</span>
<span class="w">                        </span><span class="n">Path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">dlc</span><span class="w"> </span><span class="n">container</span><span class="w"> </span><span class="n">containing</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">which</span><span class="w"> </span><span class="n">fixed</span><span class="o">-</span><span class="n">point</span>
<span class="w">                        </span><span class="n">encoding</span><span class="w"> </span><span class="n">metadata</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">generated</span><span class="p">.</span><span class="w"> </span><span class="n">This</span><span class="w"> </span><span class="n">argument</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">required</span>

<span class="n">optional</span><span class="w"> </span><span class="n">arguments</span><span class="o">:</span>
<span class="w">  </span><span class="o">-</span><span class="n">h</span><span class="p">,</span><span class="w"> </span><span class="o">--</span><span class="n">help</span><span class="w">            </span><span class="n">show</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">help</span><span class="w"> </span><span class="n">message</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">exit</span>
<span class="w">  </span><span class="o">--</span><span class="n">output_dlc</span><span class="w"> </span><span class="n">OUTPUT_DLC</span>
<span class="w">                        </span><span class="n">Path</span><span class="w"> </span><span class="n">at</span><span class="w"> </span><span class="n">which</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">metadata</span><span class="o">-</span><span class="n">included</span><span class="w"> </span><span class="n">quantized</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="n">container</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">be</span>
<span class="w">                        </span><span class="n">written</span><span class="p">.</span><span class="n">If</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">argument</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">omitted</span><span class="p">,</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">quantized</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">written</span><span class="w"> </span><span class="n">at</span>
<span class="w">                        </span><span class="o">&lt;</span><span class="n">unquantized_model_name</span><span class="o">&gt;</span><span class="n">_quantized</span><span class="p">.</span><span class="n">dlc</span>
<span class="w">  </span><span class="o">--</span><span class="n">input_list</span><span class="w"> </span><span class="n">INPUT_LIST</span>
<span class="w">                        </span><span class="n">Path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">specifying</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">input</span><span class="w"> </span><span class="n">data</span><span class="p">.</span><span class="w"> </span><span class="n">This</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">plain</span><span class="w"> </span><span class="n">text</span>
<span class="w">                        </span><span class="n">file</span><span class="p">,</span><span class="w"> </span><span class="n">containing</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="n">or</span><span class="w"> </span><span class="n">more</span><span class="w"> </span><span class="n">absolute</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">paths</span><span class="w"> </span><span class="n">per</span><span class="w"> </span><span class="n">line</span><span class="p">.</span><span class="w"> </span><span class="n">Each</span><span class="w"> </span><span class="n">path</span><span class="w"> </span><span class="n">is</span>
<span class="w">                        </span><span class="n">expected</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">point</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">binary</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">containing</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="n">input</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="s">&quot;raw&quot;</span><span class="w"> </span><span class="n">format</span><span class="p">,</span>
<span class="w">                        </span><span class="n">ready</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">consumed</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">quantizer</span><span class="w"> </span><span class="n">without</span><span class="w"> </span><span class="n">any</span><span class="w"> </span><span class="n">further</span><span class="w"> </span><span class="n">preprocessing</span><span class="p">.</span>
<span class="w">                        </span><span class="n">Multiple</span><span class="w"> </span><span class="n">files</span><span class="w"> </span><span class="n">per</span><span class="w"> </span><span class="n">line</span><span class="w"> </span><span class="n">separated</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">spaces</span><span class="w"> </span><span class="n">indicate</span><span class="w"> </span><span class="n">multiple</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span>
<span class="w">                        </span><span class="n">network</span><span class="p">.</span><span class="w"> </span><span class="n">See</span><span class="w"> </span><span class="n">documentation</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">more</span><span class="w"> </span><span class="n">details</span><span class="p">.</span><span class="w"> </span><span class="n">Must</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">specified</span><span class="w"> </span><span class="k">for</span>
<span class="w">                        </span><span class="n">quantization</span><span class="p">.</span><span class="w"> </span><span class="n">All</span><span class="w"> </span><span class="n">subsequent</span><span class="w"> </span><span class="n">quantization</span><span class="w"> </span><span class="n">options</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">ignored</span><span class="w"> </span><span class="n">when</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">is</span>
<span class="w">                        </span><span class="n">not</span><span class="w"> </span><span class="n">provided</span><span class="p">.</span>
<span class="w">  </span><span class="o">--</span><span class="n">float_fallback</span><span class="w">      </span><span class="n">Use</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">option</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">enable</span><span class="w"> </span><span class="n">fallback</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">floating</span><span class="w"> </span><span class="n">point</span><span class="w"> </span><span class="p">(</span><span class="n">FP</span><span class="p">)</span><span class="w"> </span><span class="n">instead</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">fixed</span>
<span class="w">                        </span><span class="n">point</span><span class="p">.</span>
<span class="w">                        </span><span class="n">This</span><span class="w"> </span><span class="n">option</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">paired</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="o">--</span><span class="n">float_bitwidth</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">indicate</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">bitwidth</span><span class="w"> </span><span class="k">for</span>
<span class="w">                        </span><span class="n">FP</span><span class="w"> </span><span class="p">(</span><span class="n">by</span><span class="w"> </span><span class="k">default</span><span class="w"> </span><span class="mi">32</span><span class="p">).</span>
<span class="w">                        </span><span class="n">If</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">option</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">enabled</span><span class="p">,</span><span class="w"> </span><span class="n">then</span><span class="w"> </span><span class="n">input</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">must</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">provided</span><span class="w"> </span><span class="n">and</span>
<span class="w">                        </span><span class="o">--</span><span class="n">ignore_encodings</span><span class="w"> </span><span class="n">must</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">provided</span><span class="p">.</span>
<span class="w">                        </span><span class="n">The</span><span class="w"> </span><span class="n">external</span><span class="w"> </span><span class="n">quantization</span><span class="w"> </span><span class="n">encodings</span><span class="w"> </span><span class="p">(</span><span class="n">encoding</span><span class="w"> </span><span class="n">file</span><span class="o">/</span><span class="n">FakeQuant</span><span class="w"> </span><span class="n">encodings</span><span class="p">)</span>
<span class="w">                        </span><span class="n">might</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">missing</span><span class="w"> </span><span class="n">quantization</span><span class="w"> </span><span class="n">parameters</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">some</span><span class="w"> </span><span class="n">interim</span><span class="w"> </span><span class="n">tensors</span><span class="p">.</span>
<span class="w">                        </span><span class="n">First</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">try</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">fill</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">gaps</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">propagating</span><span class="w"> </span><span class="n">across</span><span class="w"> </span><span class="n">math</span><span class="o">-</span><span class="n">invariant</span>
<span class="w">                        </span><span class="n">functions</span><span class="p">.</span><span class="w"> </span><span class="n">If</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">quantization</span><span class="w"> </span><span class="n">params</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">still</span><span class="w"> </span><span class="n">missing</span><span class="p">,</span>
<span class="w">                        </span><span class="n">then</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">apply</span><span class="w"> </span><span class="n">fallback</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">nodes</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">floating</span><span class="w"> </span><span class="n">point</span><span class="p">.</span>
<span class="w">  </span><span class="o">--</span><span class="n">algorithms</span><span class="w"> </span><span class="n">ALGORITHMS</span><span class="w"> </span><span class="p">[</span><span class="n">ALGORITHMS</span><span class="w"> </span><span class="p">...]</span>
<span class="w">                        </span><span class="n">Use</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">option</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">enable</span><span class="w"> </span><span class="n">new</span><span class="w"> </span><span class="n">optimization</span><span class="w"> </span><span class="n">algorithms</span><span class="p">.</span><span class="w"> </span><span class="n">Usage</span><span class="w"> </span><span class="n">is</span><span class="o">:</span>
<span class="w">                        </span><span class="o">--</span><span class="n">algorithms</span><span class="w"> </span><span class="o">&lt;</span><span class="n">algo_name1</span><span class="o">&gt;</span><span class="w"> </span><span class="p">...</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">available</span><span class="w"> </span><span class="n">optimization</span><span class="w"> </span><span class="n">algorithms</span><span class="w"> </span><span class="n">are</span><span class="o">:</span>
<span class="w">                        </span><span class="s">&quot;cle&quot;</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">Cross</span><span class="w"> </span><span class="n">layer</span><span class="w"> </span><span class="n">equalization</span><span class="w"> </span><span class="n">includes</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">methods</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">equalizing</span>
<span class="w">                        </span><span class="n">weights</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">biases</span><span class="w"> </span><span class="n">across</span><span class="w"> </span><span class="n">layers</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">order</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">rectify</span><span class="w"> </span><span class="n">imbalances</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">cause</span>
<span class="w">                        </span><span class="n">quantization</span><span class="w"> </span><span class="n">errors</span><span class="p">.</span>
<span class="w">  </span><span class="o">--</span><span class="n">bias_bitwidth</span><span class="w"> </span><span class="n">BIAS_BITWIDTH</span>
<span class="w">                        </span><span class="n">Use</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="o">--</span><span class="n">bias_bitwidth</span><span class="w"> </span><span class="n">option</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">select</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">bitwidth</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="n">when</span><span class="w"> </span><span class="n">quantizing</span>
<span class="w">                        </span><span class="n">the</span><span class="w"> </span><span class="n">biases</span><span class="p">,</span><span class="w"> </span><span class="n">either</span><span class="w"> </span><span class="mi">8</span><span class="w"> </span><span class="p">(</span><span class="k">default</span><span class="p">)</span><span class="w"> </span><span class="n">or</span><span class="w"> </span><span class="mf">32.</span>
<span class="w">  </span><span class="o">--</span><span class="n">act_bitwidth</span><span class="w"> </span><span class="n">ACT_BITWIDTH</span>
<span class="w">                        </span><span class="n">Use</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="o">--</span><span class="n">act_bitwidth</span><span class="w"> </span><span class="n">option</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">select</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">bitwidth</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="n">when</span><span class="w"> </span><span class="n">quantizing</span>
<span class="w">                        </span><span class="n">the</span><span class="w"> </span><span class="n">activations</span><span class="p">,</span><span class="w"> </span><span class="n">either</span><span class="w"> </span><span class="mi">8</span><span class="w"> </span><span class="p">(</span><span class="k">default</span><span class="p">)</span><span class="w"> </span><span class="n">or</span><span class="w"> </span><span class="mf">16.</span>
<span class="w">  </span><span class="o">--</span><span class="n">weights_bitwidth</span><span class="w"> </span><span class="n">WEIGHTS_BITWIDTH</span>
<span class="w">                        </span><span class="n">Use</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="o">--</span><span class="n">weights_bitwidth</span><span class="w"> </span><span class="n">option</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">select</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">bitwidth</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="n">when</span>
<span class="w">                        </span><span class="n">quantizing</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">weights</span><span class="p">,</span><span class="w"> </span><span class="n">either</span><span class="w"> </span><span class="mi">4</span><span class="w"> </span><span class="n">or</span><span class="w"> </span><span class="mi">8</span><span class="w"> </span><span class="p">(</span><span class="k">default</span><span class="p">).</span>
<span class="w">  </span><span class="o">--</span><span class="n">float_bitwidth</span><span class="w"> </span><span class="n">FLOAT_BITWIDTH</span>
<span class="w">                        </span><span class="n">Use</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="o">--</span><span class="n">float_bitwidth</span><span class="w"> </span><span class="n">option</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">select</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">bitwidth</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="kt">float</span>
<span class="w">                        </span><span class="n">tensors</span><span class="p">,</span><span class="n">either</span><span class="w"> </span><span class="mi">32</span><span class="w"> </span><span class="p">(</span><span class="k">default</span><span class="p">)</span><span class="w"> </span><span class="n">or</span><span class="w"> </span><span class="mf">16.</span>
<span class="w">  </span><span class="o">--</span><span class="n">float_bias_bitwidth</span><span class="w"> </span><span class="n">FLOAT_BIAS_BITWIDTH</span>
<span class="w">                        </span><span class="n">Use</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="o">--</span><span class="n">float_bias_bitwidth</span><span class="w"> </span><span class="n">option</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">select</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">bitwidth</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="n">when</span>
<span class="w">                        </span><span class="n">biases</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="kt">float</span><span class="p">,</span><span class="w"> </span><span class="n">either</span><span class="w"> </span><span class="mi">32</span><span class="w"> </span><span class="n">or</span><span class="w"> </span><span class="mf">16.</span>
<span class="w">  </span><span class="o">--</span><span class="n">ignore_encodings</span><span class="w">    </span><span class="n">Use</span><span class="w"> </span><span class="n">only</span><span class="w"> </span><span class="n">quantizer</span><span class="w"> </span><span class="n">generated</span><span class="w"> </span><span class="n">encodings</span><span class="p">,</span><span class="w"> </span><span class="n">ignoring</span><span class="w"> </span><span class="n">any</span><span class="w"> </span><span class="n">user</span><span class="w"> </span><span class="n">or</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="n">provided</span>
<span class="w">                        </span><span class="n">encodings</span><span class="p">.</span>
<span class="w">                        </span><span class="nl">Note</span><span class="p">:</span><span class="w"> </span><span class="n">Cannot</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="o">--</span><span class="n">ignore_encodings</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="o">--</span><span class="n">quantization_overrides</span>
<span class="w">  </span><span class="o">--</span><span class="n">use_per_channel_quantization</span>
<span class="w">                        </span><span class="n">Use</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">option</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">enable</span><span class="w"> </span><span class="n">per</span><span class="o">-</span><span class="n">channel</span><span class="w"> </span><span class="n">quantization</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">convolution</span><span class="o">-</span><span class="n">based</span><span class="w"> </span><span class="n">op</span>
<span class="w">                        </span><span class="n">weights</span><span class="p">.</span>
<span class="w">                        </span><span class="nl">Note</span><span class="p">:</span><span class="w"> </span><span class="n">This</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">replace</span><span class="w"> </span><span class="n">built</span><span class="o">-</span><span class="n">in</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="n">QAT</span><span class="w"> </span><span class="n">encodings</span><span class="w"> </span><span class="n">when</span><span class="w"> </span><span class="n">used</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">given</span>
<span class="w">                        </span><span class="n">weight</span><span class="p">.</span>
<span class="w">  </span><span class="o">--</span><span class="n">use_per_row_quantization</span>
<span class="w">                        </span><span class="n">Use</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">option</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">enable</span><span class="w"> </span><span class="n">rowwise</span><span class="w"> </span><span class="n">quantization</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">Matmul</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">FullyConnected</span>
<span class="w">                        </span><span class="n">ops</span><span class="p">.</span>
<span class="w">  </span><span class="o">--</span><span class="n">use_native_input_files</span>
<span class="w">                        </span><span class="n">Boolean</span><span class="w"> </span><span class="n">flag</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">indicate</span><span class="w"> </span><span class="n">how</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">read</span><span class="w"> </span><span class="n">input</span><span class="w"> </span><span class="n">files</span><span class="o">:</span>
<span class="w">                        </span><span class="mf">1.</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="p">(</span><span class="k">default</span><span class="p">)</span><span class="o">:</span><span class="w"> </span><span class="n">reads</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="n">as</span><span class="w"> </span><span class="n">floats</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">quantizes</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">necessary</span><span class="w"> </span><span class="n">based</span>
<span class="w">                        </span><span class="n">on</span><span class="w"> </span><span class="n">quantization</span><span class="w"> </span><span class="n">parameters</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">model</span><span class="p">.</span>
<span class="w">                        </span><span class="mf">2.</span><span class="w"> </span><span class="n">native</span><span class="o">:</span><span class="w">          </span><span class="n">reads</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="n">assuming</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="n">type</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">native</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span>
<span class="w">                        </span><span class="n">model</span><span class="p">.</span><span class="w"> </span><span class="n">For</span><span class="w"> </span><span class="n">ex</span><span class="p">.,</span><span class="w"> </span><span class="kt">uint8_t</span><span class="p">.</span>
<span class="w">  </span><span class="o">--</span><span class="n">use_native_output_files</span>
<span class="w">                        </span><span class="n">Use</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">option</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">indicate</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="n">type</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="n">files</span>
<span class="w">                        </span><span class="mf">1.</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="p">(</span><span class="k">default</span><span class="p">)</span><span class="o">:</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">as</span><span class="w"> </span><span class="n">floats</span><span class="p">.</span>
<span class="w">                        </span><span class="mf">2.</span><span class="w"> </span><span class="n">native</span><span class="o">:</span><span class="w">          </span><span class="n">outputs</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">native</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="w"> </span><span class="n">For</span><span class="w"> </span><span class="n">ex</span><span class="p">.,</span>
<span class="w">                        </span><span class="kt">uint8_t</span><span class="p">.</span>
<span class="w">  </span><span class="o">--</span><span class="n">restrict_quantization_steps</span><span class="w"> </span><span class="n">ENCODING_MIN</span><span class="p">,</span><span class="w"> </span><span class="n">ENCODING_MAX</span>
<span class="w">                        </span><span class="n">Specifies</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">steps</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">computing</span><span class="w"> </span><span class="n">quantization</span><span class="w"> </span><span class="n">encodings</span>
<span class="w">                        </span><span class="n">such</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">scale</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">max</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">min</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">quantization</span><span class="w"> </span><span class="n">steps</span><span class="p">.</span>
<span class="w">                        </span><span class="n">The</span><span class="w"> </span><span class="n">option</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">passed</span><span class="w"> </span><span class="n">as</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">space</span><span class="w"> </span><span class="n">separated</span><span class="w"> </span><span class="n">pair</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">hexadecimal</span><span class="w"> </span><span class="n">string</span>
<span class="w">                        </span><span class="n">minimum</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">maximum</span><span class="w"> </span><span class="n">valuesi</span><span class="p">.</span><span class="n">e</span><span class="p">.</span><span class="w"> </span><span class="o">--</span><span class="n">restrict_quantization_steps</span><span class="w"> </span><span class="s">&quot;MIN MAX&quot;</span><span class="p">.</span>
<span class="w">                         </span><span class="n">Please</span><span class="w"> </span><span class="n">note</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">hexadecimal</span><span class="w"> </span><span class="n">string</span><span class="w"> </span><span class="n">literal</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="kt">signed</span>
<span class="w">                        </span><span class="n">integer</span><span class="p">,</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">supply</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">negative</span><span class="w"> </span><span class="n">value</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">explicit</span><span class="w"> </span><span class="n">minus</span><span class="w"> </span><span class="n">sign</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">required</span><span class="p">.</span>
<span class="w">                        </span><span class="n">E</span><span class="p">.</span><span class="n">g</span><span class="p">.</span><span class="o">--</span><span class="n">restrict_quantization_steps</span><span class="w"> </span><span class="s">&quot;-0x80 0x7F&quot;</span><span class="w"> </span><span class="n">indicates</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">example</span><span class="w"> </span><span class="mi">8</span><span class="w"> </span><span class="n">bit</span>
<span class="w">                        </span><span class="n">range</span><span class="p">,</span>
<span class="w">                            </span><span class="o">--</span><span class="n">restrict_quantization_steps</span><span class="w"> </span><span class="s">&quot;-0x8000 0x7F7F&quot;</span><span class="w"> </span><span class="n">indicates</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">example</span><span class="w"> </span><span class="mi">16</span>
<span class="w">                        </span><span class="n">bit</span><span class="w"> </span><span class="n">range</span><span class="p">.</span>
<span class="w">                        </span><span class="n">This</span><span class="w"> </span><span class="n">argument</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">required</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="mi">16</span><span class="o">-</span><span class="n">bit</span><span class="w"> </span><span class="n">Matmul</span><span class="w"> </span><span class="n">operations</span><span class="p">.</span>
<span class="w">  </span><span class="o">--</span><span class="n">act_quantizer_calibration</span><span class="w"> </span><span class="n">ACT_QUANTIZER_CALIBRATION</span>
<span class="w">                        </span><span class="n">Specify</span><span class="w"> </span><span class="n">which</span><span class="w"> </span><span class="n">quantization</span><span class="w"> </span><span class="n">calibration</span><span class="w"> </span><span class="n">method</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">activations</span>
<span class="w">                        </span><span class="n">supported</span><span class="w"> </span><span class="n">values</span><span class="o">:</span><span class="w"> </span><span class="n">min</span><span class="o">-</span><span class="n">max</span><span class="w"> </span><span class="p">(</span><span class="k">default</span><span class="p">),</span><span class="w"> </span><span class="n">sqnr</span><span class="p">,</span><span class="w"> </span><span class="n">entropy</span><span class="p">,</span><span class="w"> </span><span class="n">mse</span><span class="p">,</span><span class="w"> </span><span class="n">percentile</span>
<span class="w">                        </span><span class="n">This</span><span class="w"> </span><span class="n">option</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">paired</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="o">--</span><span class="n">act_quantizer_schema</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">override</span><span class="w"> </span><span class="n">the</span>
<span class="w">                        </span><span class="n">quantization</span>
<span class="w">                        </span><span class="n">schema</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">activations</span><span class="w"> </span><span class="n">otherwise</span><span class="w"> </span><span class="k">default</span><span class="w"> </span><span class="n">schema</span><span class="p">(</span><span class="n">asymmetric</span><span class="p">)</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">be</span>
<span class="w">                        </span><span class="n">used</span>
<span class="w">  </span><span class="o">--</span><span class="n">param_quantizer_calibration</span><span class="w"> </span><span class="n">PARAM_QUANTIZER_CALIBRATION</span>
<span class="w">                        </span><span class="n">Specify</span><span class="w"> </span><span class="n">which</span><span class="w"> </span><span class="n">quantization</span><span class="w"> </span><span class="n">calibration</span><span class="w"> </span><span class="n">method</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">parameters</span>
<span class="w">                        </span><span class="n">supported</span><span class="w"> </span><span class="n">values</span><span class="o">:</span><span class="w"> </span><span class="n">min</span><span class="o">-</span><span class="n">max</span><span class="w"> </span><span class="p">(</span><span class="k">default</span><span class="p">),</span><span class="w"> </span><span class="n">sqnr</span><span class="p">,</span><span class="w"> </span><span class="n">entropy</span><span class="p">,</span><span class="w"> </span><span class="n">mse</span><span class="p">,</span><span class="w"> </span><span class="n">percentile</span>
<span class="w">                        </span><span class="n">This</span><span class="w"> </span><span class="n">option</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">paired</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="o">--</span><span class="n">param_quantizer_schema</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">override</span><span class="w"> </span><span class="n">the</span>
<span class="w">                        </span><span class="n">quantization</span>
<span class="w">                        </span><span class="n">schema</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">parameters</span><span class="w"> </span><span class="n">otherwise</span><span class="w"> </span><span class="k">default</span><span class="w"> </span><span class="n">schema</span><span class="p">(</span><span class="n">asymmetric</span><span class="p">)</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">be</span>
<span class="w">                        </span><span class="n">used</span>
<span class="w">  </span><span class="o">--</span><span class="n">act_quantizer_schema</span><span class="w"> </span><span class="n">ACT_QUANTIZER_SCHEMA</span>
<span class="w">                        </span><span class="n">Specify</span><span class="w"> </span><span class="n">which</span><span class="w"> </span><span class="n">quantization</span><span class="w"> </span><span class="n">schema</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">activations</span>
<span class="w">                        </span><span class="n">supported</span><span class="w"> </span><span class="n">values</span><span class="o">:</span><span class="w"> </span><span class="n">asymmetric</span><span class="w"> </span><span class="p">(</span><span class="k">default</span><span class="p">),</span><span class="w"> </span><span class="n">symmetric</span>
<span class="w">  </span><span class="o">--</span><span class="n">param_quantizer_schema</span><span class="w"> </span><span class="n">PARAM_QUANTIZER_SCHEMA</span>
<span class="w">                        </span><span class="n">Specify</span><span class="w"> </span><span class="n">which</span><span class="w"> </span><span class="n">quantization</span><span class="w"> </span><span class="n">schema</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">parameters</span>
<span class="w">                        </span><span class="n">supported</span><span class="w"> </span><span class="n">values</span><span class="o">:</span><span class="w"> </span><span class="n">asymmetric</span><span class="w"> </span><span class="p">(</span><span class="k">default</span><span class="p">),</span><span class="w"> </span><span class="n">symmetric</span>
<span class="w">  </span><span class="o">--</span><span class="n">percentile_calibration_value</span><span class="w"> </span><span class="n">PERCENTILE_CALIBRATION_VALUE</span>
<span class="w">                        </span><span class="n">Specify</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">percentile</span><span class="w"> </span><span class="n">value</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">used</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">Percentile</span><span class="w"> </span><span class="n">calibration</span><span class="w"> </span><span class="n">method</span>
<span class="w">                        </span><span class="n">The</span><span class="w"> </span><span class="n">specified</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">value</span><span class="w"> </span><span class="n">must</span><span class="w"> </span><span class="n">lie</span><span class="w"> </span><span class="n">within</span><span class="w"> </span><span class="mi">90</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="mi">100</span><span class="p">,</span><span class="w"> </span><span class="k">default</span><span class="o">:</span><span class="w"> </span><span class="mf">99.99</span>
<span class="w">  </span><span class="o">--</span><span class="n">use_aimet_quantizer</span>
<span class="w">                        </span><span class="n">Use</span><span class="w"> </span><span class="n">AIMET</span><span class="w"> </span><span class="n">Quantizer</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">place</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">IR</span><span class="w"> </span><span class="n">Quantizer</span><span class="p">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">following</span><span class="w"> </span><span class="n">arguments</span><span class="w"> </span><span class="n">are</span>
<span class="w">                        </span><span class="n">not</span><span class="w"> </span><span class="n">allowed</span><span class="w"> </span><span class="n">together</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">option</span><span class="p">,</span><span class="w"> </span><span class="o">--</span><span class="n">float_bw</span><span class="p">,</span><span class="w"> </span><span class="o">--</span><span class="n">float_bitwidth</span><span class="p">,</span><span class="w"> </span><span class="o">--</span><span class="n">float_bias_bw</span>
<span class="w">                        </span><span class="o">--</span><span class="n">float_bias_bitwidth</span><span class="p">,</span><span class="w"> </span><span class="o">--</span><span class="n">disable_relu_squashing</span><span class="p">,</span><span class="w"> </span><span class="o">--</span><span class="n">restrict_quantization_steps</span><span class="p">,</span>
<span class="w">                        </span><span class="o">--</span><span class="n">float_fallback</span><span class="p">,</span><span class="w"> </span><span class="o">--</span><span class="n">use_dynamic_16_bit_weights</span><span class="p">,</span><span class="w"> </span><span class="o">--</span><span class="n">pack_4_bit_weights</span><span class="p">,</span><span class="w"> </span><span class="o">--</span><span class="n">op_package_lib</span>
<span class="w">  </span><span class="o">--</span><span class="n">op_package_lib</span><span class="w"> </span><span class="n">OP_PACKAGE_LIB</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="n">opl</span><span class="w"> </span><span class="n">OP_PACKAGE_LIB</span>
<span class="w">                        </span><span class="n">Use</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">argument</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">pass</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">op</span><span class="w"> </span><span class="n">package</span><span class="w"> </span><span class="n">library</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">quantization</span><span class="p">.</span><span class="w"> </span><span class="n">Must</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">in</span>
<span class="w">                        </span><span class="n">the</span><span class="w"> </span><span class="n">form</span><span class="w"> </span><span class="o">&lt;</span><span class="n">op_package_lib_path</span><span class="o">:</span><span class="n">interfaceProviderName</span><span class="o">&gt;</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">separated</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">a</span>
<span class="w">                        </span><span class="n">comma</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">multiple</span><span class="w"> </span><span class="n">package</span><span class="w"> </span><span class="n">libs</span>
<span class="w">  </span><span class="o">--</span><span class="n">dump_encoding_json</span><span class="w">  </span><span class="n">Use</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">argument</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">dump</span><span class="w"> </span><span class="n">encoding</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">all</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">tensors</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">json</span><span class="w"> </span><span class="n">file</span>
<span class="w">  </span><span class="o">--</span><span class="n">debug</span><span class="w"> </span><span class="p">[</span><span class="n">DEBUG</span><span class="p">]</span><span class="w">       </span><span class="n">Run</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">quantizer</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">debug</span><span class="w"> </span><span class="n">mode</span><span class="p">.</span>
</pre></div>
</div>
<p>For more information on usage, please refer to SNPE documentation on the snpe-dlc-quant tool.</p>
</div>
<div class="section" id="qnn-model-lib-generator">
<h3>qnn-model-lib-generator<a class="headerlink" href="#qnn-model-lib-generator" title="Permalink to this heading">¶</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<div class="line-block">
<div class="line">For developers who want to execute the model preparation tools under Windows-PC, or on a Qualcomm device with a Windows operating system.</div>
<div class="line">The qnn-model-lib-generator are located under /bin/x86_64-windows-msvc within the SDK for native Windows-PC usage.</div>
<div class="line">For developers who want to run qnn-model-lib-generator on a device with a Windows OS, it is located under /bin/aarch64-windows-msvc.</div>
<div class="line">qnn-model-lib-generator will try to use the CMake command from your platform to generate libraries.</div>
<div class="line">Please make sure the CMake in Windows-OS is feasible by making sure the compile tools are installed(<a class="reference external" href="setup.html#windows">windows-platform compiling tools</a>).</div>
</div>
</div>
<p>The <strong>qnn-model-lib-generator</strong> tool compiles QNN model source code
into artifacts for a specific target.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="nl">usage</span><span class="p">:</span><span class="w"> </span><span class="n">qnn</span><span class="o">-</span><span class="n">model</span><span class="o">-</span><span class="n">lib</span><span class="o">-</span><span class="n">generator</span><span class="w"> </span><span class="p">[</span><span class="o">-</span><span class="n">h</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">-</span><span class="n">c</span><span class="w"> </span><span class="o">&lt;</span><span class="n">QNN_MODEL</span><span class="o">&gt;</span><span class="p">.</span><span class="n">cpp</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">-</span><span class="n">b</span><span class="w"> </span><span class="o">&lt;</span><span class="n">QNN_MODEL</span><span class="o">&gt;</span><span class="p">.</span><span class="n">bin</span><span class="p">]</span>
<span class="w">       </span><span class="p">[</span><span class="o">-</span><span class="n">t</span><span class="w"> </span><span class="n">LIB_TARGETS</span><span class="w"> </span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">-</span><span class="n">l</span><span class="w"> </span><span class="n">LIB_NAME</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">-</span><span class="n">o</span><span class="w"> </span><span class="n">OUTPUT_DIR</span><span class="p">]</span>
<span class="n">Script</span><span class="w"> </span><span class="n">compiles</span><span class="w"> </span><span class="n">provided</span><span class="w"> </span><span class="n">Qnn</span><span class="w"> </span><span class="n">Model</span><span class="w"> </span><span class="n">artifacts</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">specified</span><span class="w"> </span><span class="n">targets</span><span class="p">.</span>

<span class="n">Required</span><span class="w"> </span><span class="n">argument</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="o">:</span>
<span class="w"> </span><span class="o">-</span><span class="n">c</span><span class="w"> </span><span class="o">&lt;</span><span class="n">QNN_MODEL</span><span class="o">&gt;</span><span class="p">.</span><span class="n">cpp</span><span class="w">                    </span><span class="n">Filepath</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">qnn</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="p">.</span><span class="n">cpp</span><span class="w"> </span><span class="n">file</span>

<span class="n">optional</span><span class="w"> </span><span class="n">argument</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="o">:</span>
<span class="w"> </span><span class="o">-</span><span class="n">b</span><span class="w"> </span><span class="o">&lt;</span><span class="n">QNN_MODEL</span><span class="o">&gt;</span><span class="p">.</span><span class="n">bin</span><span class="w">                    </span><span class="n">Filepath</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">qnn</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="p">.</span><span class="n">bin</span><span class="w"> </span><span class="n">file</span>
<span class="w">                                       </span><span class="p">(</span><span class="n">Note</span><span class="o">:</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">passed</span><span class="p">,</span><span class="w"> </span><span class="n">runtime</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">fail</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="p">.</span><span class="n">cpp</span><span class="w"> </span><span class="n">needs</span><span class="w"> </span><span class="n">any</span><span class="w"> </span><span class="n">items</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="p">.</span><span class="n">bin</span><span class="w"> </span><span class="n">file</span><span class="p">.)</span>

<span class="w"> </span><span class="o">-</span><span class="n">t</span><span class="w"> </span><span class="n">LIB_TARGETS</span><span class="w">                        </span><span class="n">Specifies</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">targets</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">build</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">models</span><span class="w"> </span><span class="k">for</span><span class="p">.</span><span class="w"> </span><span class="n">Default</span><span class="o">:</span><span class="w"> </span><span class="n">aarch64</span><span class="o">-</span><span class="n">android</span><span class="w"> </span><span class="n">x86_64</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">clang</span>
<span class="w"> </span><span class="o">-</span><span class="n">l</span><span class="w"> </span><span class="n">LIB_NAME</span><span class="w">                           </span><span class="n">Specifies</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">libraries</span><span class="p">.</span><span class="w"> </span><span class="n">Default</span><span class="o">:</span><span class="w"> </span><span class="n">uses</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="o">&lt;</span><span class="n">model</span><span class="p">.</span><span class="n">bin</span><span class="o">&gt;</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">provided</span><span class="p">,</span>
<span class="w">                                       </span><span class="k">else</span><span class="w"> </span><span class="n">generic</span><span class="w"> </span><span class="n">qnn_model</span><span class="p">.</span><span class="n">so</span>
<span class="w">  </span><span class="o">-</span><span class="n">o</span><span class="w"> </span><span class="n">OUTPUT_DIR</span><span class="w">                         </span><span class="n">Location</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">saving</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="n">libraries</span><span class="p">.</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For Windows users, please execute this tool with python3.</p>
</div>
</div>
<div class="section" id="qnn-op-package-generator">
<h3>qnn-op-package-generator<a class="headerlink" href="#qnn-op-package-generator" title="Permalink to this heading">¶</a></h3>
<p>The <strong>qnn-op-package-generator</strong> tool is used to generate skeleton code for a QNN op package using
an XML config file that describes the attributes of the package. The tool creates the package as a directory containing
skeleton source code and makefiles that can be compiled to create a shared library object.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="nl">usage</span><span class="p">:</span><span class="w"> </span><span class="n">qnn</span><span class="o">-</span><span class="n">op</span><span class="o">-</span><span class="n">package</span><span class="o">-</span><span class="n">generator</span><span class="w"> </span><span class="p">[</span><span class="o">-</span><span class="n">h</span><span class="p">]</span><span class="w"> </span><span class="o">--</span><span class="n">config_path</span><span class="w"> </span><span class="n">CONFIG_PATH</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">debug</span><span class="p">]</span>
<span class="w">                                </span><span class="p">[</span><span class="o">--</span><span class="n">output_path</span><span class="w"> </span><span class="n">OUTPUT_PATH</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">-</span><span class="n">f</span><span class="p">]</span>

<span class="n">optional</span><span class="w"> </span><span class="n">arguments</span><span class="o">:</span>
<span class="w">  </span><span class="o">-</span><span class="n">h</span><span class="p">,</span><span class="w"> </span><span class="o">--</span><span class="n">help</span><span class="w">            </span><span class="n">show</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">help</span><span class="w"> </span><span class="n">message</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">exit</span>

<span class="n">required</span><span class="w"> </span><span class="n">arguments</span><span class="o">:</span>
<span class="w">  </span><span class="o">--</span><span class="n">config_path</span><span class="w"> </span><span class="n">CONFIG_PATH</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="n">p</span><span class="w"> </span><span class="n">CONFIG_PATH</span>
<span class="w">                        </span><span class="n">The</span><span class="w"> </span><span class="n">path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">config</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">defines</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">QNN</span><span class="w"> </span><span class="n">Op</span>
<span class="w">                        </span><span class="n">package</span><span class="p">(</span><span class="n">s</span><span class="p">).</span>

<span class="n">optional</span><span class="w"> </span><span class="n">arguments</span><span class="o">:</span>
<span class="w">  </span><span class="o">--</span><span class="n">debug</span><span class="w">               </span><span class="n">Returns</span><span class="w"> </span><span class="n">debugging</span><span class="w"> </span><span class="n">information</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">generating</span><span class="w"> </span><span class="n">the</span>
<span class="w">                        </span><span class="n">package</span>
<span class="w">  </span><span class="o">--</span><span class="n">output_path</span><span class="w"> </span><span class="n">OUTPUT_PATH</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="n">o</span><span class="w"> </span><span class="n">OUTPUT_PATH</span>
<span class="w">                        </span><span class="n">Path</span><span class="w"> </span><span class="n">where</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">package</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">saved</span>
<span class="w">  </span><span class="o">-</span><span class="n">f</span><span class="p">,</span><span class="w"> </span><span class="o">--</span><span class="n">force</span><span class="o">-</span><span class="n">generation</span>
<span class="w">                        </span><span class="n">This</span><span class="w"> </span><span class="n">option</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">delete</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">entire</span><span class="w"> </span><span class="n">existing</span><span class="w"> </span><span class="n">package</span>
<span class="w">                        </span><span class="n">Note</span><span class="w"> </span><span class="n">appropriate</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">permissions</span><span class="w"> </span><span class="n">must</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">set</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">use</span>
<span class="w">                        </span><span class="n">this</span><span class="w"> </span><span class="n">option</span><span class="p">.</span>
<span class="w">  </span><span class="o">--</span><span class="n">converter_op_package</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="n">cop</span>
<span class="w">                        </span><span class="n">Generates</span><span class="w"> </span><span class="n">Converter</span><span class="w"> </span><span class="n">Op</span><span class="w"> </span><span class="n">Package</span><span class="w"> </span><span class="n">skeleton</span><span class="w"> </span><span class="n">code</span><span class="w"> </span><span class="n">needed</span>
<span class="w">                        </span><span class="n">by</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="n">shape</span><span class="w"> </span><span class="n">inference</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">converters</span>
</pre></div>
</div>
</div>
<div class="section" id="qnn-context-binary-generator">
<h3>qnn-context-binary-generator<a class="headerlink" href="#qnn-context-binary-generator" title="Permalink to this heading">¶</a></h3>
<p>The <strong>qnn-context-binary-generator</strong> tool is used to create a context binary by using a particular backend and consuming
a model library created by the <a class="reference internal" href="#qnn-model-lib-generator">qnn-model-lib-generator</a>.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="nl">usage</span><span class="p">:</span><span class="w"> </span><span class="n">qnn</span><span class="o">-</span><span class="n">context</span><span class="o">-</span><span class="n">binary</span><span class="o">-</span><span class="n">generator</span><span class="w"> </span><span class="o">--</span><span class="n">model</span><span class="w"> </span><span class="n">QNN_MODEL</span><span class="p">.</span><span class="n">so</span><span class="w"> </span><span class="o">--</span><span class="n">backend</span><span class="w"> </span><span class="n">QNN_BACKEND</span><span class="p">.</span><span class="n">so</span>
<span class="w">                                    </span><span class="o">--</span><span class="n">binary_file</span><span class="w"> </span><span class="n">BINARY_FILE_NAME</span>
<span class="w">                                    </span><span class="p">[</span><span class="o">--</span><span class="n">model_prefix</span><span class="w"> </span><span class="n">MODEL_PREFIX</span><span class="p">]</span>
<span class="w">                                    </span><span class="p">[</span><span class="o">--</span><span class="n">output_dir</span><span class="w"> </span><span class="n">OUTPUT_DIRECTORY</span><span class="p">]</span>
<span class="w">                                    </span><span class="p">[</span><span class="o">--</span><span class="n">op_packages</span><span class="w"> </span><span class="n">ONE_OR_MORE_OP_PACKAGES</span><span class="p">]</span>
<span class="w">                                    </span><span class="p">[</span><span class="o">--</span><span class="n">config_file</span><span class="w"> </span><span class="n">CONFIG_FILE</span><span class="p">.</span><span class="n">json</span><span class="p">]</span>
<span class="w">                                    </span><span class="p">[</span><span class="o">--</span><span class="n">profiling_level</span><span class="w"> </span><span class="n">PROFILING_LEVEL</span><span class="p">]</span>
<span class="w">                                    </span><span class="p">[</span><span class="o">--</span><span class="n">verbose</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">version</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">help</span><span class="p">]</span>

<span class="n">REQUIRED</span><span class="w"> </span><span class="n">ARGUMENTS</span><span class="o">:</span>
<span class="o">-------------------</span>
<span class="w">  </span><span class="o">--</span><span class="n">model</span><span class="w">                         </span><span class="o">&lt;</span><span class="kt">FILE</span><span class="o">&gt;</span><span class="w">      </span><span class="n">Path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="o">&lt;</span><span class="n">qnn_model_name</span><span class="p">.</span><span class="n">so</span><span class="o">&gt;</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">containing</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">QNN</span><span class="w"> </span><span class="n">network</span><span class="p">.</span>
<span class="w">                                              </span><span class="n">To</span><span class="w"> </span><span class="n">create</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="n">binary</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">multiple</span><span class="w"> </span><span class="n">graphs</span><span class="p">,</span><span class="w"> </span><span class="n">use</span>
<span class="w">                                              </span><span class="n">comma</span><span class="o">-</span><span class="n">separated</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="n">so</span><span class="w"> </span><span class="n">files</span><span class="p">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">syntax</span><span class="w"> </span><span class="n">is</span>
<span class="w">                                              </span><span class="o">&lt;</span><span class="n">qnn_model_name_1</span><span class="p">.</span><span class="n">so</span><span class="o">&gt;</span><span class="p">,</span><span class="o">&lt;</span><span class="n">qnn_model_name_2</span><span class="p">.</span><span class="n">so</span><span class="o">&gt;</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">backend</span><span class="w">                       </span><span class="o">&lt;</span><span class="kt">FILE</span><span class="o">&gt;</span><span class="w">      </span><span class="n">Path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">QNN</span><span class="w"> </span><span class="n">backend</span><span class="w"> </span><span class="p">.</span><span class="n">so</span><span class="w"> </span><span class="n">library</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">create</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="n">binary</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">binary_file</span><span class="w">                   </span><span class="o">&lt;</span><span class="n">VAL</span><span class="o">&gt;</span><span class="w">       </span><span class="n">Name</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">binary</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">save</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="n">binary</span><span class="w"> </span><span class="n">to</span><span class="p">.</span>
<span class="w">                                              </span><span class="n">Saved</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">same</span><span class="w"> </span><span class="n">path</span><span class="w"> </span><span class="n">as</span><span class="w"> </span><span class="o">--</span><span class="n">output_dir</span><span class="w"> </span><span class="n">option</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="p">.</span><span class="n">bin</span>
<span class="w">                                              </span><span class="n">as</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">binary</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">extension</span><span class="p">.</span><span class="w"> </span><span class="n">If</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">provided</span><span class="p">,</span><span class="w"> </span><span class="n">no</span><span class="w"> </span><span class="n">backend</span><span class="w"> </span><span class="n">binary</span>
<span class="w">                                              </span><span class="n">is</span><span class="w"> </span><span class="n">created</span><span class="p">.</span>


<span class="n">OPTIONAL</span><span class="w"> </span><span class="n">ARGUMENTS</span><span class="o">:</span>
<span class="o">-------------------</span>
<span class="w">  </span><span class="o">--</span><span class="n">model_prefix</span><span class="w">                              </span><span class="n">Function</span><span class="w"> </span><span class="n">prefix</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="n">when</span><span class="w"> </span><span class="n">loading</span><span class="w"> </span><span class="o">&lt;</span><span class="n">qnn_model_name</span><span class="p">.</span><span class="n">so</span><span class="o">&gt;</span><span class="w"> </span><span class="n">file</span>
<span class="w">                                              </span><span class="n">containing</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">QNN</span><span class="w"> </span><span class="n">network</span><span class="p">.</span><span class="w"> </span><span class="n">Default</span><span class="o">:</span><span class="w"> </span><span class="n">QnnModel</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">output_dir</span><span class="w">                    </span><span class="o">&lt;</span><span class="kt">DIR</span><span class="o">&gt;</span><span class="w">       </span><span class="n">The</span><span class="w"> </span><span class="n">directory</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">save</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="n">to</span><span class="p">.</span><span class="w"> </span><span class="n">Defaults</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="p">.</span><span class="o">/</span><span class="n">output</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">op_packages</span><span class="w">                   </span><span class="o">&lt;</span><span class="n">VAL</span><span class="o">&gt;</span><span class="w">       </span><span class="n">Provide</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">comma</span><span class="w"> </span><span class="n">separated</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">op</span><span class="w"> </span><span class="n">packages</span>
<span class="w">                                              </span><span class="n">and</span><span class="w"> </span><span class="n">interface</span><span class="w"> </span><span class="n">providers</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="k">register</span><span class="p">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">syntax</span><span class="w"> </span><span class="n">is</span><span class="o">:</span>
<span class="w">                                              </span><span class="nl">op_package_path</span><span class="p">:</span><span class="n">interface_provider</span><span class="p">[,</span><span class="n">op_package_path</span><span class="o">:</span><span class="n">interface_provider</span><span class="p">...]</span>

<span class="w">  </span><span class="o">--</span><span class="n">profiling_level</span><span class="w">               </span><span class="o">&lt;</span><span class="n">VAL</span><span class="o">&gt;</span><span class="w">       </span><span class="n">Enable</span><span class="w"> </span><span class="n">profiling</span><span class="p">.</span><span class="w"> </span><span class="n">Valid</span><span class="w"> </span><span class="n">Values</span><span class="o">:</span>
<span class="w">                                              </span><span class="mf">1.</span><span class="w"> </span><span class="n">basic</span><span class="o">:</span><span class="w">    </span><span class="n">captures</span><span class="w"> </span><span class="n">execution</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">init</span><span class="w"> </span><span class="n">time</span><span class="p">.</span>
<span class="w">                                              </span><span class="mf">2.</span><span class="w"> </span><span class="n">detailed</span><span class="o">:</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">addition</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">basic</span><span class="p">,</span><span class="w"> </span><span class="n">captures</span><span class="w"> </span><span class="n">per</span><span class="w"> </span><span class="n">Op</span><span class="w"> </span><span class="n">timing</span>
<span class="w">                                                  </span><span class="k">for</span><span class="w"> </span><span class="n">execution</span><span class="p">.</span>
<span class="w">                                              </span><span class="mf">3.</span><span class="w"> </span><span class="n">backend</span><span class="o">:</span><span class="w">  </span><span class="n">backend</span><span class="o">-</span><span class="n">specific</span><span class="w"> </span><span class="n">profiling</span><span class="w"> </span><span class="n">level</span><span class="w"> </span><span class="n">specified</span>
<span class="w">                                                  </span><span class="n">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">backend</span><span class="w"> </span><span class="n">extension</span><span class="w"> </span><span class="n">related</span><span class="w"> </span><span class="n">JSON</span><span class="w"> </span><span class="n">config</span><span class="w"> </span><span class="n">file</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">profiling_option</span><span class="w">              </span><span class="o">&lt;</span><span class="n">VAL</span><span class="o">&gt;</span><span class="w">       </span><span class="n">Set</span><span class="w"> </span><span class="n">profiling</span><span class="w"> </span><span class="n">options</span><span class="o">:</span>
<span class="w">                                              </span><span class="mf">1.</span><span class="w"> </span><span class="n">optrace</span><span class="o">:</span><span class="w">    </span><span class="n">Generates</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">optrace</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">run</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">config_file</span><span class="w">                   </span><span class="o">&lt;</span><span class="kt">FILE</span><span class="o">&gt;</span><span class="w">      </span><span class="n">Path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">JSON</span><span class="w"> </span><span class="n">config</span><span class="w"> </span><span class="n">file</span><span class="p">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">config</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">currently</span>
<span class="w">                                              </span><span class="n">supports</span><span class="w"> </span><span class="n">options</span><span class="w"> </span><span class="n">related</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">backend</span><span class="w"> </span><span class="n">extensions</span><span class="w"> </span><span class="n">and</span>
<span class="w">                                              </span><span class="n">context</span><span class="w"> </span><span class="n">priority</span><span class="p">.</span><span class="w"> </span><span class="n">Please</span><span class="w"> </span><span class="n">refer</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">SDK</span><span class="w"> </span><span class="n">documentation</span>
<span class="w">                                              </span><span class="k">for</span><span class="w"> </span><span class="n">more</span><span class="w"> </span><span class="n">details</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">enable_intermediate_outputs</span><span class="w">               </span><span class="n">Enable</span><span class="w"> </span><span class="n">all</span><span class="w"> </span><span class="n">intermediate</span><span class="w"> </span><span class="n">nodes</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="n">along</span><span class="w"> </span><span class="n">with</span>
<span class="w">                                              </span><span class="k">default</span><span class="w"> </span><span class="n">outputs</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">saved</span><span class="w"> </span><span class="n">context</span><span class="p">.</span>
<span class="w">                                              </span><span class="n">Note</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">options</span><span class="w"> </span><span class="o">--</span><span class="n">enable_intermediate_outputs</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="o">--</span><span class="n">set_output_tensors</span>
<span class="w">                                              </span><span class="n">are</span><span class="w"> </span><span class="n">mutually</span><span class="w"> </span><span class="n">exclusive</span><span class="p">.</span><span class="w"> </span><span class="n">Only</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">options</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">specified</span><span class="w"> </span><span class="n">at</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">time</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">set_output_tensors</span><span class="w">            </span><span class="o">&lt;</span><span class="n">VAL</span><span class="o">&gt;</span><span class="w">       </span><span class="n">Provide</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">comma</span><span class="o">-</span><span class="n">separated</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">intermediate</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="n">tensor</span><span class="w"> </span><span class="n">names</span><span class="p">,</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">which</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">outputs</span>
<span class="w">                                              </span><span class="n">will</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">written</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">addition</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">final</span><span class="w"> </span><span class="n">graph</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="n">tensors</span><span class="p">.</span>
<span class="w">                                              </span><span class="n">Note</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">options</span><span class="w"> </span><span class="o">--</span><span class="n">enable_intermediate_outputs</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="o">--</span><span class="n">set_output_tensors</span>
<span class="w">                                              </span><span class="n">are</span><span class="w"> </span><span class="n">mutually</span><span class="w"> </span><span class="n">exclusive</span><span class="p">.</span><span class="w"> </span><span class="n">Only</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">options</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">specified</span><span class="w"> </span><span class="n">at</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">time</span><span class="p">.</span>
<span class="w">                                              </span><span class="n">The</span><span class="w"> </span><span class="n">syntax</span><span class="w"> </span><span class="n">is</span><span class="o">:</span><span class="w"> </span><span class="n">graphName0</span><span class="o">:</span><span class="n">tensorName0</span><span class="p">,</span><span class="n">tensorName1</span><span class="p">;</span><span class="n">graphName1</span><span class="o">:</span><span class="n">tensorName0</span><span class="p">,</span><span class="n">tensorName1</span>

<span class="w">  </span><span class="o">--</span><span class="n">backend_binary</span><span class="w">                </span><span class="o">&lt;</span><span class="n">VAL</span><span class="o">&gt;</span><span class="w">       </span><span class="n">Name</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">save</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">backend</span><span class="o">-</span><span class="n">specific</span><span class="w"> </span><span class="n">context</span>
<span class="w">                                              </span><span class="n">binary</span><span class="w"> </span><span class="n">to</span><span class="p">.</span>
<span class="w">                                              </span><span class="n">Saved</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">same</span><span class="w"> </span><span class="n">path</span><span class="w"> </span><span class="n">as</span><span class="w"> </span><span class="o">--</span><span class="n">output_dir</span><span class="w"> </span><span class="n">option</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="p">.</span><span class="n">bin</span>
<span class="w">                                              </span><span class="n">as</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">binary</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">extension</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">log_level</span><span class="w">                                 </span><span class="n">Specifies</span><span class="w"> </span><span class="n">max</span><span class="w"> </span><span class="n">logging</span><span class="w"> </span><span class="n">level</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">set</span><span class="p">.</span><span class="w"> </span><span class="n">Valid</span><span class="w"> </span><span class="n">settings</span><span class="o">:</span>
<span class="w">                                              </span><span class="s">&quot;error&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;warn&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;info&quot;</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="s">&quot;verbose&quot;</span>

<span class="w">  </span><span class="o">--</span><span class="n">dlc_path</span><span class="w">                     </span><span class="o">&lt;</span><span class="n">VAL</span><span class="o">&gt;</span><span class="w">        </span><span class="n">Paths</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">comma</span><span class="w"> </span><span class="n">separated</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">Deep</span><span class="w"> </span><span class="n">Learning</span><span class="w"> </span><span class="n">Containers</span><span class="w"> </span><span class="p">(</span><span class="n">DLC</span><span class="p">)</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">which</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">load</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">models</span><span class="p">.</span>
<span class="w">                                              </span><span class="n">Necessitates</span><span class="w"> </span><span class="n">libQnnModelDlc</span><span class="p">.</span><span class="n">so</span><span class="w"> </span><span class="n">as</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="o">--</span><span class="n">model</span><span class="w"> </span><span class="n">argument</span><span class="p">.</span>
<span class="w">                                              </span><span class="n">To</span><span class="w"> </span><span class="n">compose</span><span class="w"> </span><span class="n">multiple</span><span class="w"> </span><span class="n">graphs</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">context</span><span class="p">,</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="n">comma</span><span class="o">-</span><span class="n">separated</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">DLC</span><span class="w"> </span><span class="n">files</span><span class="p">.</span>
<span class="w">                                              </span><span class="n">The</span><span class="w"> </span><span class="n">syntax</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="o">&lt;</span><span class="n">qnn_model_name_1</span><span class="p">.</span><span class="n">dlc</span><span class="o">&gt;</span><span class="p">,</span><span class="o">&lt;</span><span class="n">qnn_model_name_2</span><span class="p">.</span><span class="n">dlc</span><span class="o">&gt;</span>
<span class="w">                                              </span><span class="nl">Default</span><span class="p">:</span><span class="w"> </span><span class="n">None</span>

<span class="w">  </span><span class="o">--</span><span class="n">input_output_tensor_mem_type</span><span class="w">  </span><span class="o">&lt;</span><span class="n">VAL</span><span class="o">&gt;</span><span class="w">       </span><span class="n">Specifies</span><span class="w"> </span><span class="n">mem</span><span class="w"> </span><span class="n">type</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">used</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">input</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="n">tensors</span><span class="w"> </span><span class="n">during</span><span class="w"> </span><span class="n">graph</span><span class="w"> </span><span class="n">creation</span><span class="p">.</span>
<span class="w">                                              </span><span class="n">Valid</span><span class="w"> </span><span class="n">settings</span><span class="o">:</span><span class="s">&quot;raw&quot;</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="s">&quot;memhandle&quot;</span>

<span class="w">  </span><span class="o">--</span><span class="n">platform_options</span><span class="w">              </span><span class="o">&lt;</span><span class="n">VAL</span><span class="o">&gt;</span><span class="w">       </span><span class="n">Specifies</span><span class="w"> </span><span class="n">values</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">pass</span><span class="w"> </span><span class="n">as</span><span class="w"> </span><span class="n">platform</span><span class="w"> </span><span class="n">options</span><span class="p">.</span><span class="w"> </span><span class="n">Multiple</span><span class="w"> </span><span class="n">platform</span><span class="w"> </span><span class="n">options</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">provided</span>
<span class="w">                                              </span><span class="n">using</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">syntax</span><span class="o">:</span><span class="w"> </span><span class="n">key0</span><span class="o">:</span><span class="n">value0</span><span class="p">;</span><span class="n">key1</span><span class="o">:</span><span class="n">value1</span><span class="p">;</span><span class="n">key2</span><span class="o">:</span><span class="n">value2</span>

<span class="w">  </span><span class="o">--</span><span class="n">version</span><span class="w">                                   </span><span class="n">Print</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">QNN</span><span class="w"> </span><span class="n">SDK</span><span class="w"> </span><span class="n">version</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">help</span><span class="w">                                      </span><span class="n">Show</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">help</span><span class="w"> </span><span class="n">message</span><span class="p">.</span>
</pre></div>
</div>
<p>See <a class="reference internal" href="#qnn-net-run">qnn-net-run</a> section for more details about <code class="docutils literal notranslate"><span class="pre">--op_packages</span></code> and <code class="docutils literal notranslate"><span class="pre">--config_file</span></code> options.</p>
</div>
</div>
<div class="section" id="execution">
<h2>Execution<a class="headerlink" href="#execution" title="Permalink to this heading">¶</a></h2>
<div class="section" id="qnn-net-run">
<h3>qnn-net-run<a class="headerlink" href="#qnn-net-run" title="Permalink to this heading">¶</a></h3>
<p>The <strong>qnn-net-run</strong> tool is used to consume a model library compiled from
the output of the QNN converter, and run it on a particular backend.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="nl">DESCRIPTION</span><span class="p">:</span>
<span class="o">------------</span>
<span class="n">Example</span><span class="w"> </span><span class="n">application</span><span class="w"> </span><span class="n">demonstrating</span><span class="w"> </span><span class="n">how</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">load</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">execute</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">neural</span><span class="w"> </span><span class="n">network</span>
<span class="n">using</span><span class="w"> </span><span class="n">QNN</span><span class="w"> </span><span class="n">APIs</span><span class="p">.</span>


<span class="n">REQUIRED</span><span class="w"> </span><span class="n">ARGUMENTS</span><span class="o">:</span>
<span class="o">-------------------</span>
<span class="w">  </span><span class="o">--</span><span class="n">model</span><span class="w">             </span><span class="o">&lt;</span><span class="kt">FILE</span><span class="o">&gt;</span><span class="w">       </span><span class="n">Path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="n">containing</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">QNN</span><span class="w"> </span><span class="n">network</span><span class="p">.</span>
<span class="w">                                   </span><span class="n">To</span><span class="w"> </span><span class="n">compose</span><span class="w"> </span><span class="n">multiple</span><span class="w"> </span><span class="n">graphs</span><span class="p">,</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="n">comma</span><span class="o">-</span><span class="n">separated</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">of</span>
<span class="w">                                   </span><span class="n">model</span><span class="p">.</span><span class="n">so</span><span class="w"> </span><span class="n">files</span><span class="p">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">syntax</span><span class="w"> </span><span class="n">is</span>
<span class="w">                                   </span><span class="o">&lt;</span><span class="n">qnn_model_name_1</span><span class="p">.</span><span class="n">so</span><span class="o">&gt;</span><span class="p">,</span><span class="o">&lt;</span><span class="n">qnn_model_name_2</span><span class="p">.</span><span class="n">so</span><span class="o">&gt;</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">backend</span><span class="w">           </span><span class="o">&lt;</span><span class="kt">FILE</span><span class="o">&gt;</span><span class="w">       </span><span class="n">Path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">QNN</span><span class="w"> </span><span class="n">backend</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">execute</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">model</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">input_list</span><span class="w">        </span><span class="o">&lt;</span><span class="kt">FILE</span><span class="o">&gt;</span><span class="w">       </span><span class="n">Path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">listing</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">network</span><span class="p">.</span>
<span class="w">                                   </span><span class="n">If</span><span class="w"> </span><span class="n">there</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">multiple</span><span class="w"> </span><span class="n">graphs</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="n">so</span><span class="p">,</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">has</span>
<span class="w">                                   </span><span class="n">to</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">comma</span><span class="o">-</span><span class="n">separated</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">input</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">files</span><span class="p">.</span>
<span class="w">                                   </span><span class="n">When</span><span class="w"> </span><span class="n">multiple</span><span class="w"> </span><span class="n">graphs</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">present</span><span class="p">,</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">skip</span><span class="w"> </span><span class="n">execution</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">graph</span><span class="w"> </span><span class="n">use</span>
<span class="w">                                   </span><span class="s">&quot;__&quot;</span><span class="p">(</span><span class="kt">double</span><span class="w"> </span><span class="n">underscore</span><span class="w"> </span><span class="n">without</span><span class="w"> </span><span class="n">quotes</span><span class="p">)</span><span class="w"> </span><span class="n">as</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">the</span>
<span class="w">                                   </span><span class="n">comma</span><span class="o">-</span><span class="n">seperated</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">input</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">files</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">retrieve_context</span><span class="w">  </span><span class="o">&lt;</span><span class="n">VAL</span><span class="o">&gt;</span><span class="w">       </span><span class="n">Path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">cached</span><span class="w"> </span><span class="n">binary</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">which</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">load</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">saved</span>
<span class="w">                                  </span><span class="n">context</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">execute</span><span class="w"> </span><span class="n">graphs</span><span class="p">.</span><span class="w"> </span><span class="o">--</span><span class="n">retrieve_context</span><span class="w"> </span><span class="n">and</span>
<span class="w">                                  </span><span class="o">--</span><span class="n">model</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">mutually</span><span class="w"> </span><span class="n">exclusive</span><span class="p">.</span><span class="w"> </span><span class="n">Only</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">options</span>
<span class="w">                                  </span><span class="n">can</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">specified</span><span class="w"> </span><span class="n">at</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">time</span><span class="p">.</span>


<span class="n">OPTIONAL</span><span class="w"> </span><span class="n">ARGUMENTS</span><span class="o">:</span>
<span class="o">-------------------</span>
<span class="w">  </span><span class="o">--</span><span class="n">model_prefix</span><span class="w">                             </span><span class="n">Function</span><span class="w"> </span><span class="n">prefix</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="n">when</span><span class="w"> </span><span class="n">loading</span><span class="w"> </span><span class="o">&lt;</span><span class="n">qnn_model_name</span><span class="p">.</span><span class="n">so</span><span class="o">&gt;</span><span class="p">.</span>
<span class="w">                                             </span><span class="nl">Default</span><span class="p">:</span><span class="w"> </span><span class="n">QnnModel</span>

<span class="w">  </span><span class="o">--</span><span class="n">debug</span><span class="w">                                    </span><span class="n">Specifies</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">all</span><span class="w"> </span><span class="n">layers</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">network</span>
<span class="w">                                             </span><span class="n">will</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">saved</span><span class="p">.</span><span class="w"> </span><span class="n">This</span><span class="w"> </span><span class="n">option</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">used</span><span class="w"> </span><span class="n">when</span><span class="w"> </span><span class="n">loading</span>
<span class="w">                                             </span><span class="n">a</span><span class="w"> </span><span class="n">saved</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="n">through</span><span class="w"> </span><span class="o">--</span><span class="n">retrieve_context</span><span class="w"> </span><span class="n">option</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">output_dir</span><span class="w">                   </span><span class="o">&lt;</span><span class="kt">DIR</span><span class="o">&gt;</span><span class="w">       </span><span class="n">The</span><span class="w"> </span><span class="n">directory</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">save</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="n">to</span><span class="p">.</span><span class="w"> </span><span class="n">Defaults</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="p">.</span><span class="o">/</span><span class="n">output</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">use_native_output_files</span><span class="w">                  </span><span class="n">Specifies</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="n">files</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">generated</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">data</span>
<span class="w">                                             </span><span class="n">type</span><span class="w"> </span><span class="n">native</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">graph</span><span class="p">.</span><span class="w"> </span><span class="n">If</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">specified</span><span class="p">,</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="n">files</span><span class="w"> </span><span class="n">will</span>
<span class="w">                                             </span><span class="n">be</span><span class="w"> </span><span class="n">generated</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">floating</span><span class="w"> </span><span class="n">point</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">use_native_input_files</span><span class="w">                   </span><span class="n">Specifies</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">input</span><span class="w"> </span><span class="n">files</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">parsed</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">data</span>
<span class="w">                                             </span><span class="n">type</span><span class="w"> </span><span class="n">native</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">graph</span><span class="p">.</span><span class="w"> </span><span class="n">If</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">specified</span><span class="p">,</span><span class="w"> </span><span class="n">input</span><span class="w"> </span><span class="n">files</span><span class="w"> </span><span class="n">will</span>
<span class="w">                                             </span><span class="n">be</span><span class="w"> </span><span class="n">parsed</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">floating</span><span class="w"> </span><span class="n">point</span><span class="p">.</span><span class="w"> </span><span class="n">Note</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">options</span><span class="w"> </span><span class="o">--</span><span class="n">use_native_input_files</span>
<span class="w">                                             </span><span class="n">and</span><span class="w"> </span><span class="o">--</span><span class="n">native_input_tensor_names</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">mutually</span><span class="w"> </span><span class="n">exclusive</span><span class="p">.</span>
<span class="w">                                             </span><span class="n">Only</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">options</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">specified</span><span class="w"> </span><span class="n">at</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">time</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">native_input_tensor_names</span><span class="w">    </span><span class="o">&lt;</span><span class="n">VAL</span><span class="o">&gt;</span><span class="w">       </span><span class="n">Provide</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">comma</span><span class="o">-</span><span class="n">separated</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">input</span><span class="w"> </span><span class="n">tensor</span><span class="w"> </span><span class="n">names</span><span class="p">,</span>
<span class="w">                                             </span><span class="k">for</span><span class="w"> </span><span class="n">which</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">input</span><span class="w"> </span><span class="n">files</span><span class="w"> </span><span class="n">would</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">read</span><span class="o">/</span><span class="n">parsed</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">native</span><span class="w"> </span><span class="n">format</span><span class="p">.</span>
<span class="w">                                             </span><span class="n">Note</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">options</span><span class="w"> </span><span class="o">--</span><span class="n">use_native_input_files</span><span class="w"> </span><span class="n">and</span>
<span class="w">                                             </span><span class="o">--</span><span class="n">native_input_tensor_names</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">mutually</span><span class="w"> </span><span class="n">exclusive</span><span class="p">.</span>
<span class="w">                                             </span><span class="n">Only</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">options</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">specified</span><span class="w"> </span><span class="n">at</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">time</span><span class="p">.</span>
<span class="w">                                             </span><span class="n">The</span><span class="w"> </span><span class="n">syntax</span><span class="w"> </span><span class="n">is</span><span class="o">:</span><span class="w"> </span><span class="n">graphName0</span><span class="o">:</span><span class="n">tensorName0</span><span class="p">,</span><span class="n">tensorName1</span><span class="p">;</span><span class="n">graphName1</span><span class="o">:</span><span class="n">tensorName0</span><span class="p">,</span><span class="n">tensorName1</span>

<span class="w">  </span><span class="o">--</span><span class="n">op_packages</span><span class="w">                  </span><span class="o">&lt;</span><span class="n">VAL</span><span class="o">&gt;</span><span class="w">       </span><span class="n">Provide</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">comma</span><span class="o">-</span><span class="n">separated</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">op</span><span class="w"> </span><span class="n">packages</span><span class="p">,</span><span class="w"> </span><span class="n">interface</span>
<span class="w">                                             </span><span class="n">providers</span><span class="p">,</span><span class="w"> </span><span class="n">and</span><span class="p">,</span><span class="w"> </span><span class="n">optionally</span><span class="p">,</span><span class="w"> </span><span class="n">targets</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="k">register</span><span class="p">.</span><span class="w"> </span><span class="n">Valid</span><span class="w"> </span><span class="n">values</span>
<span class="w">                                             </span><span class="k">for</span><span class="w"> </span><span class="n">target</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">CPU</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">HTP</span><span class="p">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">syntax</span><span class="w"> </span><span class="n">is</span><span class="o">:</span>
<span class="w">                                             </span><span class="nl">op_package_path</span><span class="p">:</span><span class="n">interface_provider</span><span class="o">:</span><span class="n">target</span><span class="p">[,</span><span class="n">op_package_path</span><span class="o">:</span><span class="n">interface_provider</span><span class="o">:</span><span class="n">target</span><span class="p">...]</span>

<span class="w">  </span><span class="o">--</span><span class="n">profiling_level</span><span class="w">              </span><span class="o">&lt;</span><span class="n">VAL</span><span class="o">&gt;</span><span class="w">       </span><span class="n">Enable</span><span class="w"> </span><span class="n">profiling</span><span class="p">.</span><span class="w"> </span><span class="n">Valid</span><span class="w"> </span><span class="n">Values</span><span class="o">:</span>
<span class="w">                                               </span><span class="mf">1.</span><span class="w"> </span><span class="n">basic</span><span class="o">:</span><span class="w">    </span><span class="n">captures</span><span class="w"> </span><span class="n">execution</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">init</span><span class="w"> </span><span class="n">time</span><span class="p">.</span>
<span class="w">                                               </span><span class="mf">2.</span><span class="w"> </span><span class="n">detailed</span><span class="o">:</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">addition</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">basic</span><span class="p">,</span><span class="w"> </span><span class="n">captures</span><span class="w"> </span><span class="n">per</span><span class="w"> </span><span class="n">Op</span><span class="w"> </span><span class="n">timing</span>
<span class="w">                                                            </span><span class="k">for</span><span class="w"> </span><span class="n">execution</span><span class="p">,</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">backend</span><span class="w"> </span><span class="n">supports</span><span class="w"> </span><span class="n">it</span><span class="p">.</span>
<span class="w">                                               </span><span class="mf">3.</span><span class="w"> </span><span class="n">client</span><span class="o">:</span><span class="w">   </span><span class="n">captures</span><span class="w"> </span><span class="n">only</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">performance</span><span class="w"> </span><span class="n">metrics</span>
<span class="w">                                                            </span><span class="n">measured</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">qnn</span><span class="o">-</span><span class="n">net</span><span class="o">-</span><span class="n">run</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">perf_profile</span><span class="w">                 </span><span class="o">&lt;</span><span class="n">VAL</span><span class="o">&gt;</span><span class="w">       </span><span class="n">Specifies</span><span class="w"> </span><span class="n">performance</span><span class="w"> </span><span class="n">profile</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">used</span><span class="p">.</span><span class="w"> </span><span class="n">Valid</span><span class="w"> </span><span class="n">settings</span><span class="w"> </span><span class="n">are</span>
<span class="w">                                             </span><span class="n">low_balanced</span><span class="p">,</span><span class="w"> </span><span class="n">balanced</span><span class="p">,</span><span class="w"> </span><span class="k">default</span><span class="p">,</span><span class="w"> </span><span class="n">high_performance</span><span class="p">,</span>
<span class="w">                                             </span><span class="n">sustained_high_performance</span><span class="p">,</span><span class="w"> </span><span class="n">burst</span><span class="p">,</span><span class="w"> </span><span class="n">low_power_saver</span><span class="p">,</span>
<span class="w">                                             </span><span class="n">power_saver</span><span class="p">,</span><span class="w"> </span><span class="n">high_power_saver</span><span class="p">,</span><span class="w"> </span><span class="n">extreme_power_saver</span>
<span class="w">                                             </span><span class="n">and</span><span class="w"> </span><span class="n">system_settings</span><span class="p">.</span>
<span class="w">                                             </span><span class="nl">Note</span><span class="p">:</span><span class="w"> </span><span class="n">perf_profile</span><span class="w"> </span><span class="n">argument</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">now</span><span class="w"> </span><span class="n">deprecated</span><span class="w"> </span><span class="k">for</span>
<span class="w">                                             </span><span class="n">HTP</span><span class="w"> </span><span class="n">backend</span><span class="p">,</span><span class="w"> </span><span class="n">user</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">specify</span><span class="w"> </span><span class="n">performance</span><span class="w"> </span><span class="n">profile</span>
<span class="w">                                             </span><span class="n">through</span><span class="w"> </span><span class="n">backend</span><span class="w"> </span><span class="n">config</span><span class="w"> </span><span class="n">now</span><span class="p">.</span><span class="w"> </span><span class="n">Please</span><span class="w"> </span><span class="n">refer</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">config_file</span>
<span class="w">                                             </span><span class="n">backend</span><span class="w"> </span><span class="n">extensions</span><span class="w"> </span><span class="n">usage</span><span class="w"> </span><span class="n">section</span><span class="w"> </span><span class="n">below</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">more</span><span class="w"> </span><span class="n">details</span><span class="p">.</span>


<span class="w">  </span><span class="o">--</span><span class="n">config_file</span><span class="w">                  </span><span class="o">&lt;</span><span class="kt">FILE</span><span class="o">&gt;</span><span class="w">      </span><span class="n">Path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">JSON</span><span class="w"> </span><span class="n">config</span><span class="w"> </span><span class="n">file</span><span class="p">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">config</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">currently</span>
<span class="w">                                             </span><span class="n">supports</span><span class="w"> </span><span class="n">options</span><span class="w"> </span><span class="n">related</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">backend</span><span class="w"> </span><span class="n">extensions</span><span class="p">,</span>
<span class="w">                                             </span><span class="n">context</span><span class="w"> </span><span class="n">priority</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">graph</span><span class="w"> </span><span class="n">configs</span><span class="p">.</span><span class="w"> </span><span class="n">Please</span><span class="w"> </span><span class="n">refer</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">SDK</span>
<span class="w">                                             </span><span class="n">documentation</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">more</span><span class="w"> </span><span class="n">details</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">log_level</span><span class="w">                    </span><span class="o">&lt;</span><span class="n">VAL</span><span class="o">&gt;</span><span class="w">       </span><span class="n">Specifies</span><span class="w"> </span><span class="n">max</span><span class="w"> </span><span class="n">logging</span><span class="w"> </span><span class="n">level</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">set</span><span class="p">.</span><span class="w"> </span><span class="n">Valid</span><span class="w"> </span><span class="n">settings</span><span class="o">:</span>
<span class="w">                                             </span><span class="n">error</span><span class="p">,</span><span class="w"> </span><span class="n">warn</span><span class="p">,</span><span class="w"> </span><span class="n">info</span><span class="p">,</span><span class="w"> </span><span class="n">debug</span><span class="p">,</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">verbose</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">shared_buffer</span><span class="w">                            </span><span class="n">Specifies</span><span class="w"> </span><span class="n">creation</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">shared</span><span class="w"> </span><span class="n">buffers</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">graph</span><span class="w"> </span><span class="n">I</span><span class="o">/</span><span class="n">O</span><span class="w"> </span><span class="n">between</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">application</span>
<span class="w">                                             </span><span class="n">and</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">device</span><span class="o">/</span><span class="n">coprocessor</span><span class="w"> </span><span class="n">associated</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">backend</span><span class="w"> </span><span class="n">directly</span><span class="p">.</span>
<span class="w">                                             </span><span class="n">This</span><span class="w"> </span><span class="n">option</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">currently</span><span class="w"> </span><span class="n">supported</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">Android</span><span class="w"> </span><span class="n">only</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">synchronous</span><span class="w">                              </span><span class="n">Specifies</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">graphs</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">executed</span><span class="w"> </span><span class="n">synchronously</span><span class="w"> </span><span class="n">rather</span><span class="w"> </span><span class="n">than</span><span class="w"> </span><span class="n">asynchronously</span><span class="p">.</span>
<span class="w">                                             </span><span class="n">If</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">backend</span><span class="w"> </span><span class="n">does</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">support</span><span class="w"> </span><span class="n">asynchronous</span><span class="w"> </span><span class="n">execution</span><span class="p">,</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">flag</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">unnecessary</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">num_inferences</span><span class="w">               </span><span class="o">&lt;</span><span class="n">VAL</span><span class="o">&gt;</span><span class="w">       </span><span class="n">Specifies</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">inferences</span><span class="p">.</span><span class="w"> </span><span class="n">Loops</span><span class="w"> </span><span class="n">over</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">input_list</span><span class="w"> </span><span class="n">until</span>
<span class="w">                                             </span><span class="n">the</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">inferences</span><span class="w"> </span><span class="n">has</span><span class="w"> </span><span class="n">transpired</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">duration</span><span class="w">                     </span><span class="o">&lt;</span><span class="n">VAL</span><span class="o">&gt;</span><span class="w">       </span><span class="n">Specifies</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">duration</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">graph</span><span class="w"> </span><span class="n">execution</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">seconds</span><span class="p">.</span>
<span class="w">                                             </span><span class="n">Loops</span><span class="w"> </span><span class="n">over</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">input_list</span><span class="w"> </span><span class="n">until</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">amount</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">time</span><span class="w"> </span><span class="n">has</span><span class="w"> </span><span class="n">transpired</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">keep_num_outputs</span><span class="w">             </span><span class="o">&lt;</span><span class="n">VAL</span><span class="o">&gt;</span><span class="w">       </span><span class="n">Specifies</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">outputs</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">saved</span><span class="p">.</span>
<span class="w">                                             </span><span class="n">Once</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">outputs</span><span class="w"> </span><span class="n">reach</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">limit</span><span class="p">,</span><span class="w"> </span><span class="n">subsequent</span><span class="w"> </span><span class="n">outputs</span><span class="w"> </span><span class="n">would</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">just</span><span class="w"> </span><span class="n">discarded</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">batch_multiplier</span><span class="w">             </span><span class="o">&lt;</span><span class="n">VAL</span><span class="o">&gt;</span><span class="w">       </span><span class="n">Specifies</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">value</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">which</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">batch</span><span class="w"> </span><span class="n">value</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">input</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="n">tensors</span><span class="w"> </span><span class="n">dimensions</span>
<span class="w">                                             </span><span class="n">will</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">multiplied</span><span class="p">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">modified</span><span class="w"> </span><span class="n">input</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="n">tensors</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">used</span><span class="w"> </span><span class="n">only</span><span class="w"> </span><span class="n">during</span>
<span class="w">                                             </span><span class="n">the</span><span class="w"> </span><span class="n">execute</span><span class="w"> </span><span class="n">graphs</span><span class="p">.</span><span class="w"> </span><span class="n">Composed</span><span class="w"> </span><span class="n">graphs</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">still</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">tensor</span><span class="w"> </span><span class="n">dimensions</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">model</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">timeout</span><span class="w">                      </span><span class="o">&lt;</span><span class="n">VAL</span><span class="o">&gt;</span><span class="w">       </span><span class="n">Specifies</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">value</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">timeout</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">execution</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">graph</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">micro</span><span class="w"> </span><span class="n">seconds</span><span class="p">.</span><span class="w"> </span><span class="n">Please</span><span class="w"> </span><span class="n">note</span>
<span class="w">                                             </span><span class="n">using</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">option</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">backend</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">does</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">support</span><span class="w"> </span><span class="n">timeout</span><span class="w"> </span><span class="n">signals</span><span class="w"> </span><span class="n">results</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">error</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">retrieve_context_timeout</span><span class="w">     </span><span class="o">&lt;</span><span class="n">VAL</span><span class="o">&gt;</span><span class="w">       </span><span class="n">Specifies</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">value</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">timeout</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">initialization</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">graph</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">micro</span><span class="w"> </span><span class="n">seconds</span><span class="p">.</span><span class="w"> </span><span class="n">Please</span><span class="w"> </span><span class="n">note</span>
<span class="w">                                             </span><span class="n">using</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">option</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">backend</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">does</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">support</span><span class="w"> </span><span class="n">timeout</span><span class="w"> </span><span class="n">signals</span><span class="w"> </span><span class="n">results</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">error</span><span class="p">.</span>
<span class="w">                                             </span><span class="n">Also</span><span class="w"> </span><span class="n">note</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">option</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">only</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">used</span><span class="w"> </span><span class="n">when</span><span class="w"> </span><span class="n">loading</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">saved</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="n">through</span>
<span class="w">                                             </span><span class="o">--</span><span class="n">retrieve_context</span><span class="w"> </span><span class="n">option</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">max_input_cache_tensor_sets</span><span class="w">  </span><span class="o">&lt;</span><span class="n">VAL</span><span class="o">&gt;</span><span class="w">       </span><span class="n">Specifies</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">maximum</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">input</span><span class="w"> </span><span class="n">tensor</span><span class="w"> </span><span class="n">sets</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">cached</span><span class="p">.</span>
<span class="w">                                             </span><span class="n">Use</span><span class="w"> </span><span class="n">value</span><span class="w"> </span><span class="s">&quot;-1&quot;</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">cache</span><span class="w"> </span><span class="n">all</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">input</span><span class="w"> </span><span class="n">tensors</span><span class="w"> </span><span class="n">created</span><span class="p">.</span>
<span class="w">                                             </span><span class="n">Note</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">options</span><span class="w"> </span><span class="o">--</span><span class="n">max_input_cache_tensor_sets</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="o">--</span><span class="n">max_input_cache_size_mb</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">mutually</span><span class="w"> </span><span class="n">exclusive</span><span class="p">.</span>
<span class="w">                                             </span><span class="n">Only</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">options</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">specified</span><span class="w"> </span><span class="n">at</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">time</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">max_input_cache_size_mb</span><span class="w">      </span><span class="o">&lt;</span><span class="n">VAL</span><span class="o">&gt;</span><span class="w">       </span><span class="n">Specifies</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">maximum</span><span class="w"> </span><span class="n">cache</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">mega</span><span class="w"> </span><span class="n">bytes</span><span class="p">(</span><span class="n">MB</span><span class="p">).</span>
<span class="w">                                             </span><span class="n">Note</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">options</span><span class="w"> </span><span class="o">--</span><span class="n">max_input_cache_tensor_sets</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="o">--</span><span class="n">max_input_cache_size_mb</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">mutually</span><span class="w"> </span><span class="n">exclusive</span><span class="p">.</span>
<span class="w">                                             </span><span class="n">Only</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">options</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">specified</span><span class="w"> </span><span class="n">at</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">time</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">set_output_tensors</span><span class="w">          </span><span class="o">&lt;</span><span class="n">VAL</span><span class="o">&gt;</span><span class="w">        </span><span class="n">Provide</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">comma</span><span class="o">-</span><span class="n">separated</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">intermediate</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="n">tensor</span><span class="w"> </span><span class="n">names</span><span class="p">,</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">which</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">outputs</span>
<span class="w">                                             </span><span class="n">will</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">written</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">addition</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">final</span><span class="w"> </span><span class="n">graph</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="n">tensors</span><span class="p">.</span><span class="w"> </span><span class="n">Note</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">options</span><span class="w"> </span><span class="o">--</span><span class="n">debug</span><span class="w"> </span><span class="n">and</span>
<span class="w">                                             </span><span class="o">--</span><span class="n">set_output_tensors</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">mutually</span><span class="w"> </span><span class="n">exclusive</span><span class="p">.</span><span class="w"> </span><span class="n">Only</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">options</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">specified</span><span class="w"> </span><span class="n">at</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">time</span><span class="p">.</span>
<span class="w">                                             </span><span class="n">Also</span><span class="w"> </span><span class="n">note</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">option</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">used</span><span class="w"> </span><span class="n">when</span><span class="w"> </span><span class="n">graph</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">retrieved</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="n">binary</span><span class="p">,</span>
<span class="w">                                             </span><span class="n">since</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">graph</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">already</span><span class="w"> </span><span class="n">finalized</span><span class="w"> </span><span class="n">when</span><span class="w"> </span><span class="n">retrieved</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="n">binary</span><span class="p">.</span>
<span class="w">                                             </span><span class="n">The</span><span class="w"> </span><span class="n">syntax</span><span class="w"> </span><span class="n">is</span><span class="o">:</span><span class="w"> </span><span class="n">graphName0</span><span class="o">:</span><span class="n">tensorName0</span><span class="p">,</span><span class="n">tensorName1</span><span class="p">;</span><span class="n">graphName1</span><span class="o">:</span><span class="n">tensorName0</span><span class="p">,</span><span class="n">tensorName1</span>

<span class="w"> </span><span class="o">--</span><span class="n">use_mmap</span><span class="w">                                  </span><span class="n">Specifies</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="n">binary</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">being</span><span class="w"> </span><span class="n">read</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">loaded</span>
<span class="w">                                             </span><span class="n">using</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">Memory</span><span class="o">-</span><span class="n">mapped</span><span class="w"> </span><span class="p">(</span><span class="n">MMAP</span><span class="p">)</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">I</span><span class="o">/</span><span class="n">O</span><span class="p">.</span><span class="w"> </span><span class="n">Please</span><span class="w"> </span><span class="n">note</span><span class="w"> </span><span class="n">some</span><span class="w"> </span><span class="n">platforms</span>
<span class="w">                                             </span><span class="n">may</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">support</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">due</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">OS</span><span class="w"> </span><span class="n">limitations</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">which</span><span class="w"> </span><span class="k">case</span><span class="w"> </span><span class="no">an</span><span class="w"> </span><span class="no">error</span>
<span class="w">                                             </span><span class="no">is</span><span class="w"> </span><span class="no">thrown</span><span class="w"> </span><span class="no">when</span><span class="w"> </span><span class="no">this</span><span class="w"> </span><span class="no">option</span><span class="w"> </span><span class="no">is</span><span class="w"> </span><span class="no">used</span><span class="p">.</span>

<span class="w"> </span><span class="o">--</span><span class="no">validate_binary</span><span class="w">                           </span><span class="no">Specifies</span><span class="w"> </span><span class="no">that</span><span class="w"> </span><span class="no">the</span><span class="w"> </span><span class="no">context</span><span class="w"> </span><span class="no">binary</span><span class="w"> </span><span class="no">will</span><span class="w"> </span><span class="no">be</span><span class="w"> </span><span class="no">validated</span><span class="w"> </span><span class="no">before</span><span class="w"> </span><span class="no">creating</span><span class="w"> </span><span class="no">a</span><span class="w"> </span><span class="no">context</span><span class="p">.</span>
<span class="w">                                             </span><span class="no">This</span><span class="w"> </span><span class="no">option</span><span class="w"> </span><span class="no">can</span><span class="w"> </span><span class="no">only</span><span class="w"> </span><span class="no">be</span><span class="w"> </span><span class="no">used</span><span class="w"> </span><span class="no">with</span><span class="w"> </span><span class="no">backends</span><span class="w"> </span><span class="no">that</span><span class="w"> </span><span class="no">support</span><span class="w"> </span><span class="no">binary</span><span class="w"> </span><span class="no">validation</span><span class="p">.</span>

<span class="w"> </span><span class="o">--</span><span class="no">platform_options</span><span class="w">             </span><span class="o">&lt;</span><span class="no">VAL</span><span class="o">&gt;</span><span class="w">        </span><span class="no">Specifies</span><span class="w"> </span><span class="no">values</span><span class="w"> </span><span class="no">to</span><span class="w"> </span><span class="no">pass</span><span class="w"> </span><span class="no">as</span><span class="w"> </span><span class="no">platform</span><span class="w"> </span><span class="no">options</span><span class="p">.</span><span class="w"> </span><span class="no">Multiple</span><span class="w"> </span><span class="no">platform</span><span class="w"> </span><span class="no">options</span><span class="w"> </span><span class="no">can</span><span class="w"> </span><span class="no">be</span><span class="w"> </span><span class="no">provided</span>
<span class="w">                                             </span><span class="no">using</span><span class="w"> </span><span class="no">the</span><span class="w"> </span><span class="no">syntax</span><span class="p">:</span><span class="w"> </span><span class="n">key0</span><span class="o">:</span><span class="n">value0</span><span class="p">;</span><span class="n">key1</span><span class="o">:</span><span class="n">value1</span><span class="p">;</span><span class="n">key2</span><span class="o">:</span><span class="n">value2</span>

<span class="w"> </span><span class="o">--</span><span class="n">graph_profiling_start_delay</span><span class="w">  </span><span class="o">&lt;</span><span class="n">VAL</span><span class="o">&gt;</span><span class="w">        </span><span class="n">Specifies</span><span class="w"> </span><span class="n">graph</span><span class="w"> </span><span class="n">profiling</span><span class="w"> </span><span class="n">start</span><span class="w"> </span><span class="n">delay</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">seconds</span><span class="p">.</span><span class="w"> </span><span class="n">Please</span><span class="w"> </span><span class="n">Note</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">option</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">only</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">used</span>
<span class="w">                                             </span><span class="n">in</span><span class="w"> </span><span class="n">conjunction</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">graph</span><span class="o">-</span><span class="n">level</span><span class="w"> </span><span class="n">profiling</span><span class="w"> </span><span class="n">handles</span><span class="p">.</span>

<span class="w"> </span><span class="o">--</span><span class="n">dlc_path</span><span class="w">                     </span><span class="o">&lt;</span><span class="n">VAL</span><span class="o">&gt;</span><span class="w">        </span><span class="n">Paths</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">comma</span><span class="w"> </span><span class="n">separated</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">Deep</span><span class="w"> </span><span class="n">Learning</span><span class="w"> </span><span class="n">Containers</span><span class="w"> </span><span class="p">(</span><span class="n">DLC</span><span class="p">)</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">which</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">load</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">models</span><span class="p">.</span>
<span class="w">                                             </span><span class="n">Necessitates</span><span class="w"> </span><span class="n">libQnnModelDlc</span><span class="p">.</span><span class="n">so</span><span class="w"> </span><span class="n">as</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="o">--</span><span class="n">model</span><span class="w"> </span><span class="n">argument</span><span class="p">.</span>
<span class="w">                                             </span><span class="n">To</span><span class="w"> </span><span class="n">compose</span><span class="w"> </span><span class="n">multiple</span><span class="w"> </span><span class="n">graphs</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">context</span><span class="p">,</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="n">comma</span><span class="o">-</span><span class="n">separated</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">DLC</span><span class="w"> </span><span class="n">files</span><span class="p">.</span>
<span class="w">                                             </span><span class="n">The</span><span class="w"> </span><span class="n">syntax</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="o">&lt;</span><span class="n">qnn_model_name_1</span><span class="p">.</span><span class="n">dlc</span><span class="o">&gt;</span><span class="p">,</span><span class="o">&lt;</span><span class="n">qnn_model_name_2</span><span class="p">.</span><span class="n">dlc</span><span class="o">&gt;</span>
<span class="w">                                             </span><span class="nl">Default</span><span class="p">:</span><span class="w"> </span><span class="n">None</span>

<span class="w"> </span><span class="o">--</span><span class="n">graph_profiling_num_executions</span><span class="w">  </span><span class="o">&lt;</span><span class="n">VAL</span><span class="o">&gt;</span><span class="w">     </span><span class="n">Specifies</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">maximum</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">QnnGraph_execute</span><span class="o">/</span><span class="n">QnnGraph_executeAsync</span><span class="w"> </span><span class="n">calls</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">profiled</span><span class="p">.</span>
<span class="w">                                             </span><span class="n">Please</span><span class="w"> </span><span class="n">Note</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">option</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">only</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">used</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">conjunction</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">graph</span><span class="o">-</span><span class="n">level</span><span class="w"> </span><span class="n">profiling</span><span class="w"> </span><span class="n">handles</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">version</span><span class="w">                                  </span><span class="n">Print</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">QNN</span><span class="w"> </span><span class="n">SDK</span><span class="w"> </span><span class="n">version</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">help</span><span class="w">                                     </span><span class="n">Show</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">help</span><span class="w"> </span><span class="n">message</span><span class="p">.</span>
</pre></div>
</div>
<p>See <code class="docutils literal notranslate"><span class="pre">&lt;QNN_SDK_ROOT&gt;/examples/QNN/NetRun</span></code> folder for reference example on how to use <code class="docutils literal notranslate"><span class="pre">qnn-net-run</span></code> tool.</p>
<p><strong>Typical arguments:</strong></p>
<p><code class="docutils literal notranslate"><span class="pre">--backend</span></code> - The appropriate argument depends on what target and backend you want to run on</p>
<blockquote>
<div><p>Android (aarch64): <code class="docutils literal notranslate"><span class="pre">&lt;QNN_SDK_ROOT&gt;/lib/aarch64-android/</span></code></p>
<ul class="simple">
<li><p>CPU - <code class="docutils literal notranslate"><span class="pre">libQnnCpu.so</span></code></p></li>
<li><p>GPU - <code class="docutils literal notranslate"><span class="pre">libQnnGpu.so</span></code></p></li>
<li><p>HTA - <code class="docutils literal notranslate"><span class="pre">libQnnHta.so</span></code></p></li>
<li><p>DSP (Hexagon v65) - <code class="docutils literal notranslate"><span class="pre">libQnnDspV65Stub.so</span></code></p></li>
<li><p>DSP (Hexagon v66) - <code class="docutils literal notranslate"><span class="pre">libQnnDspV66Stub.so</span></code></p></li>
<li><p>DSP               - <code class="docutils literal notranslate"><span class="pre">libQnnDsp.so</span></code></p></li>
<li><p>HTP (Hexagon v68) - <code class="docutils literal notranslate"><span class="pre">libQnnHtp.so</span></code></p></li>
<li><p>[Deprecated] HTP Alternate Prepare (Hexagon v68) - <code class="docutils literal notranslate"><span class="pre">libQnnHtpAltPrepStub.so</span></code></p></li>
<li><p>Saver - <code class="docutils literal notranslate"><span class="pre">libQnnSaver.so</span></code></p></li>
</ul>
<p>Linux x86: <code class="docutils literal notranslate"><span class="pre">&lt;QNN_SDK_ROOT&gt;/lib/x86_64-linux-clang/</span></code></p>
<ul class="simple">
<li><p>CPU - <code class="docutils literal notranslate"><span class="pre">libQnnCpu.so</span></code></p></li>
<li><p>HTP (Hexagon v68) - <code class="docutils literal notranslate"><span class="pre">libQnnHtp.so</span></code></p></li>
<li><p>Saver - <code class="docutils literal notranslate"><span class="pre">libQnnSaver.so</span></code></p></li>
</ul>
<p>Windows x86: <code class="docutils literal notranslate"><span class="pre">&lt;QNN_SDK_ROOT&gt;/lib/x86_64-windows-msvc/</span></code></p>
<ul class="simple">
<li><p>CPU - <code class="docutils literal notranslate"><span class="pre">QnnCpu.dll</span></code></p></li>
<li><p>Saver - <code class="docutils literal notranslate"><span class="pre">QnnSaver.dll</span></code></p></li>
</ul>
<p>WoS: <code class="docutils literal notranslate"><span class="pre">&lt;QNN_SDK_ROOT&gt;/lib/aarch64-windows-msvc/</span></code></p>
<ul class="simple">
<li><p>CPU - <code class="docutils literal notranslate"><span class="pre">QnnCpu.dll</span></code></p></li>
<li><p>DSP (Hexagon v66) - <code class="docutils literal notranslate"><span class="pre">QnnDspV66Stub.dll</span></code></p></li>
<li><p>DSP               - <code class="docutils literal notranslate"><span class="pre">QnnDsp.dll</span></code></p></li>
<li><p>HTP (Hexagon v68) - <code class="docutils literal notranslate"><span class="pre">QnnHtp.dll</span></code></p></li>
<li><p>Saver - <code class="docutils literal notranslate"><span class="pre">QnnSaver.dll</span></code></p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Hexagon based backend libraries are emulations on x86_64 platforms</p>
</div>
<p><code class="docutils literal notranslate"><span class="pre">--input_list</span></code> - This argument provides a file containing paths to input files to be used for graph
execution. Input files can be specified with the below format:</p>
<blockquote>
<div><div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="n">input_layer_name</span><span class="o">&gt;:=&lt;</span><span class="n">input_layer_path</span><span class="o">&gt;</span><span class="p">[</span><span class="o">&lt;</span><span class="n">space</span><span class="o">&gt;&lt;</span><span class="n">input_layer_name</span><span class="o">&gt;:=&lt;</span><span class="n">input_layer_path</span><span class="o">&gt;</span><span class="p">]</span>
<span class="p">[</span><span class="o">&lt;</span><span class="n">input_layer_name</span><span class="o">&gt;:=&lt;</span><span class="n">input_layer_path</span><span class="o">&gt;</span><span class="p">[</span><span class="o">&lt;</span><span class="n">space</span><span class="o">&gt;&lt;</span><span class="n">input_layer_name</span><span class="o">&gt;:=&lt;</span><span class="n">input_layer_path</span><span class="o">&gt;</span><span class="p">]]</span>
<span class="p">...</span>
</pre></div>
</div>
</div></blockquote>
<p>Below is an example containing 3 sets of inputs with layer names “Input_1” and “Input_2”, and files
located in the relative path “Placeholder_1/real_input_inputs_1/”:</p>
<blockquote>
<div><div class="highlight-c notranslate"><div class="highlight"><pre><span></span>Input_1:=Placeholder_1/real_input_inputs_1/0-0#e6fb51.rawtensor Input_2:=Placeholder_1/real_input_inputs_1/0-1#8a171b.rawtensor
Input_1:=Placeholder_1/real_input_inputs_1/1-0#67c965.rawtensor Input_2:=Placeholder_1/real_input_inputs_1/1-1#54f1ff.rawtensor
Input_1:=Placeholder_1/real_input_inputs_1/2-0#b42dc6.rawtensor Input_2:=Placeholder_1/real_input_inputs_1/2-1#346a0e.rawtensor
</pre></div>
</div>
</div></blockquote>
<p>Note:  If the batch dimension of the model is greater than 1, the number of batch elements in the
input file has to either match the batch dimension specified in the model or it has to be one. In the
latter case, qnn-net-run will combine multiple lines into a single input tensor.</p>
<p><code class="docutils literal notranslate"><span class="pre">--op_packages</span></code> - This argument is only needed if you are using custom op packages. The native QNN
ops are already included as part of the backend libraries.</p>
<blockquote>
<div><p>When using custom op packages, each provided op package requires a colon separated command line
argument containing the path to the op package shared library (.so) file, as well as the name of the
interface provider, formatted as <code class="docutils literal notranslate"><span class="pre">&lt;op_package_path&gt;:&lt;interface_provider&gt;</span></code>.</p>
<p>The interface_provider argument must be the name of the function in the op package library that
satisfies the <a class="reference internal" href="../api-rst/typedef_QnnOpPackage_8h_1a71759daf7267945c50a6e5417026e869.html#exhale-typedef-qnnoppackage-8h-1a71759daf7267945c50a6e5417026e869"><span class="std std-ref">QnnOpPackage_InterfaceProvider_t</span></a>
interface. In the skeleton code created by <code class="docutils literal notranslate"><span class="pre">qnn-op-package-generator</span></code>, this function will be named
<code class="docutils literal notranslate"><span class="pre">&lt;package_name&gt;&lt;backend&gt;InterfaceProvider</span></code>.</p>
<p>See <a class="reference internal" href="generating_op_packages.html"><span class="doc">Generating Op Packages</span></a> for more information.</p>
</div></blockquote>
<p><code class="docutils literal notranslate"><span class="pre">--config_file</span></code> - This argument is only needed if you need to specify context priority or provide backend extensions
related parameters. These parameters are specified through a JSON file. The template of the JSON file is shown below:</p>
<blockquote>
<div><div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">  </span><span class="s">&quot;backend_extensions&quot;</span><span class="w"> </span><span class="o">:</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="s">&quot;shared_library_path&quot;</span><span class="w"> </span><span class="o">:</span><span class="w">  </span><span class="s">&quot;path_to_shared_library&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;config_file_path&quot;</span><span class="w"> </span><span class="o">:</span><span class="w">  </span><span class="s">&quot;path_to_config_file&quot;</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">  </span><span class="s">&quot;context_configs&quot;</span><span class="w"> </span><span class="o">:</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="s">&quot;context_priority&quot;</span><span class="w"> </span><span class="o">:</span><span class="w">  </span><span class="s">&quot;low | normal | normal_high | high&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;async_execute_queue_depth&quot;</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">uint32_value</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;enable_graphs&quot;</span><span class="w"> </span><span class="o">:</span><span class="w">  </span><span class="p">[</span><span class="s">&quot;&lt;graph_name_1&gt;&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;&lt;graph_name_2&gt;&quot;</span><span class="p">,</span><span class="w"> </span><span class="p">...],</span>
<span class="w">      </span><span class="s">&quot;memory_limit_hint&quot;</span><span class="w">  </span><span class="o">:</span><span class="w"> </span><span class="n">uint64_value</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;is_persistent_binary&quot;</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">boolean_value</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;cache_compatibility_mode&quot;</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="s">&quot;permissive | strict&quot;</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">  </span><span class="s">&quot;graph_configs&quot;</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="s">&quot;graph_name&quot;</span><span class="w"> </span><span class="o">:</span><span class="w">  </span><span class="s">&quot;graph_name_1&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;graph_priority&quot;</span><span class="w"> </span><span class="o">:</span><span class="w">  </span><span class="s">&quot;low | normal | normal_high | high&quot;</span>
<span class="w">      </span><span class="s">&quot;graph_profiling_start_delay&quot;</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">double_value</span>
<span class="w">      </span><span class="s">&quot;graph_profiling_num_executions&quot;</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">uint64_value</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">],</span>
<span class="w">  </span><span class="s">&quot;profile_configs&quot;</span><span class="w"> </span><span class="o">:</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="s">&quot;num_max_events&quot;</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">uint64_value</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">  </span><span class="s">&quot;async_graph_execution_config&quot;</span><span class="w"> </span><span class="o">:</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="s">&quot;input_tensors_creation_tasks_limit&quot;</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">uint32_value</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;execute_enqueue_tasks_limit&quot;</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">uint32_value</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>All the options in the JSON file are optional. <em>context_priority</em> is used to specify priority of the context
as a context config. <em>async_execute_queue_depth</em> is used to specify the number of executions that can be in the queue at a given time.
While using a context binary, <em>enable_graphs</em> is used to implement the <em>graph selection</em> functionality.
<em>memory_limit_hint</em> is used to set the peak memory limit hint of a deserialized context in MBs.
<em>is_persistent_binary</em> indicates that the context binary pointer is available during QnnContext_createFromBinary and
until QnnContext_free is called.</p>
<p><em>Set Cache Compatibility Mode</em> : <em>cache_compatibility_mode</em> specifies the mode used to check whether cache record is optimal for the device.
The available modes indicate binary cache compatibility:</p>
<ul class="simple">
<li><p>“permissive”: Binary cache is compatible if it could run on the device; default.</p></li>
<li><p>“strict”: Binary cache is compatible if it could run on the device and fully utilize hardware capability.
If it cannot fully utilize hardware, selecting this option results in a recommendation to prepare the cache again.
This option returns an error if it is not supported by the selected backend.</p></li>
</ul>
<p><em>Graph Selection</em> : Allows to specify a subset of graphs in a context to be loaded and executed.
If <em>enable_graphs</em> is specified, only those graphs are loaded. If a graph name is selected and it
doesn’t exist, that would be an error. If <em>enable_graphs</em> is not specified or passed as an empty list,
default behaviour continues where all graphs in a context are loaded.</p>
<p><em>graph_configs</em> can be used to specify asynchronous execution order and depth, if a backend
supports asynchronous execution. Every set of graph configs has to be specified along with a graph name.
<em>graph_profiling_start_delay</em> is used to set the profiling start delay time in seconds.
<em>graph_profiling_num_executions</em> is used to set the maximum number of QnnGraph_execute/QnnGraph_executeAsync calls
that will be profiled.</p>
<p><em>profile_configs</em> can be used to specify the max profile events per profiling handle.</p>
<p><em>async_graph_execution_config</em> can be used to specify the limits on number of tasks that run in parallel when graphs are executed
asynchronously using graphExecuteAsync. <em>input_tensors_creation_tasks_limit</em> specifies the maximum number of tasks in which input tensor sets
are populated, which can be used for graph execution. <em>execute_enqueue_tasks_limit</em> specifies the maximum number of tasks
in which the backend graphExecuteAsync will be called using the pre-populated input tensors. If unspecified, these values will be set to the specified
“async_execute_queue_depth” or 10 which is the default for “async_execute_queue_depth”.</p>
<p><em>backend_extensions</em> is used to exercise custom options in a particular backend. This can be done by providing an
extensions shared library (.so) and a config file, if necessary. This is also required to enable various performance modes,
which can be exercised using backend config. Currently, HTP supports it through <code class="docutils literal notranslate"><span class="pre">libQnnHtpNetRunExtensions.so</span></code> shared
library and DSP supports it through <code class="docutils literal notranslate"><span class="pre">libQnnDspNetRunExtensions.so</span></code>. For different custom options which can be enabled
with HTP see <a class="reference internal" href="htp/htp_backend.html#qnn-htp-backend-extensions"><span class="std std-ref">HTP Backend Extensions</span></a></p>
</div></blockquote>
<p><code class="docutils literal notranslate"><span class="pre">--shared_buffer</span></code> - This argument is only needed to indicate qnn-net-run to use shared buffers for zero-copy use case with
a device/coprocessor associated with a particular backend (for ex., DSP with HTP backend) for graph input and output tensor data.
This option is supported on Android only. qnn-net-run implements this feature using rpcmem APIs, which further create shared
buffers using ION/DMA-BUF memory allocator on Android, available through the shared library libcdsprpc.so. In addition to
specifying this option, for qnn-net-run to be able to discover libcdsprpc.so, the path in which the shared library is present
needs to be appended to LD_LIBRARY_PATH variable.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">export</span><span class="w"> </span><span class="n">LD_LIBRARY_PATH</span><span class="o">=</span><span class="n">$LD_LIBRARY_PATH</span><span class="o">:/</span><span class="n">vendor</span><span class="o">/</span><span class="n">lib64</span>
</pre></div>
</div>
<div class="section" id="running-quantized-model-on-htp-backend-with-qnn-net-run">
<h4>Running Quantized Model on HTP backend with qnn-net-run<a class="headerlink" href="#running-quantized-model-on-htp-backend-with-qnn-net-run" title="Permalink to this heading">¶</a></h4>
<p>The HTP backend currently allows to finalize / create an optimized version of a quantized QNN model
offline, on Linux development host (using <code class="docutils literal notranslate"><span class="pre">x86_64-linux-clang</span></code> backend library) and then execute
the finalized model on device (using <code class="docutils literal notranslate"><span class="pre">hexagon-v68</span></code> backend libraries).</p>
<p>First, configure the environment by following instructions in <a class="reference internal" href="setup.html"><span class="doc">Setup</span></a> section. Next,
build QNN Model library from your network, using artifacts produced by one of QNN converters.
See <a class="reference internal" href="tutorial1.html#building-example-model"><span class="std std-ref">Building Example Model</span></a> for reference.
Lastly, use the <code class="docutils literal notranslate"><span class="pre">qnn-context-binary-generator</span></code> utility to generate a serialized representation of the
finalized graph to execute the serialized binary on device.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span># Generate the optimized serialized representation of QNN Model on Linux development host.
<span class="hll"><span class="linenos">2</span>$ qnn-context-binary-generator --binary_file qnngraph.serialized.bin \
</span><span class="linenos">3</span>                               --model &lt;path_to_model_library&gt;/libQnnModel.so \ # a x86_64-linux-clang built quantized QNN model
<span class="hll"><span class="linenos">4</span>                               --backend ${QNN_SDK_ROOT}/lib/x86_64-linux-clang/libQnnHtp.so \
</span><span class="linenos">5</span>                               --output_dir &lt;output_dir_for_result_and_qnngraph_serialized_binary&gt; \
</pre></div>
</div>
<p>To use produced serialized representation of the finalized graph (<code class="docutils literal notranslate"><span class="pre">qnngraph.serialized.bin</span></code>)
ensure the below binaries are available on the android device:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">libQnnHtpV68Stub.so</span></code> (ARM)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">libQnnHtpPrepare.so</span></code> (ARM)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">libQnnModel.so</span></code> (ARM)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">libQnnHtpV68Skel.so</span></code> (cDSP v68)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">qnngraph.serialized.bin</span></code> (serialized binary from run on Linux development host)</p></li>
</ul>
<p>See <code class="docutils literal notranslate"><span class="pre">&lt;QNN_SDK_ROOT&gt;/examples/QNN/NetRun/android/android-qnn-net-run.sh</span></code> script for reference
on how to use <code class="docutils literal notranslate"><span class="pre">qnn-net-run</span></code> tool on android device.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="cp"># Run the optimized graph on HTP target</span>
<span class="hll"><span class="linenos">2</span><span class="n">$</span><span class="w"> </span><span class="n">qnn</span><span class="o">-</span><span class="n">net</span><span class="o">-</span><span class="n">run</span><span class="w"> </span><span class="o">--</span><span class="n">retrieve_context</span><span class="w"> </span><span class="n">qnngraph</span><span class="p">.</span><span class="n">serialized</span><span class="p">.</span><span class="n">bin</span><span class="w"> </span>\
</span><span class="hll"><span class="linenos">3</span><span class="w">              </span><span class="o">--</span><span class="n">backend</span><span class="w"> </span><span class="o">&lt;</span><span class="n">path_to_model_library</span><span class="o">&gt;/</span><span class="n">libQnnHtp</span><span class="p">.</span><span class="n">so</span><span class="w"> </span>\
</span><span class="linenos">4</span><span class="w">              </span><span class="o">--</span><span class="n">output_dir</span><span class="w"> </span><span class="o">&lt;</span><span class="n">output_dir_for_result</span><span class="o">&gt;</span><span class="w"> </span>\
<span class="linenos">5</span><span class="w">              </span><span class="o">--</span><span class="n">input_list</span><span class="w"> </span><span class="o">&lt;</span><span class="n">path_to_input_list</span><span class="p">.</span><span class="n">txt</span><span class="o">&gt;</span>
</pre></div>
</div>
</div>
<div class="section" id="running-float-model-on-htp-backend-with-qnn-net-run">
<h4>Running Float Model on HTP backend with qnn-net-run<a class="headerlink" href="#running-float-model-on-htp-backend-with-qnn-net-run" title="Permalink to this heading">¶</a></h4>
<p>The QNN HTP backend can support running float32 models on select Qualcomm SoCs.</p>
<p>First, configure the environment by following instructions in <a class="reference internal" href="setup.html"><span class="doc">Setup</span></a> section. Next,
build QNN Model library from your network, using artifacts produced by one of QNN converters.
See <a class="reference internal" href="tutorial1.html#building-example-model"><span class="std std-ref">Building Example Model</span></a> for reference.</p>
<p>Lastly, configure <em>backend_extensions</em> parameters through a JSON file and set custom options for the HTP backend.
Pass this file to qnn-net-run using <code class="docutils literal notranslate"><span class="pre">--config_file</span></code> argument. <em>backend_extensions</em> take two parameters, an extensions shared library (.so) (for HTP use <code class="docutils literal notranslate"><span class="pre">libQnnHtpNetRunExtensions.so</span></code>) and
a config file for the backend.</p>
<blockquote>
<div><p>Below is the template for the JSON file:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">  </span><span class="s">&quot;backend_extensions&quot;</span><span class="w"> </span><span class="o">:</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="s">&quot;shared_library_path&quot;</span><span class="w"> </span><span class="o">:</span><span class="w">  </span><span class="s">&quot;path_to_shared_library&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;config_file_path&quot;</span><span class="w"> </span><span class="o">:</span><span class="w">  </span><span class="s">&quot;path_to_config_file&quot;</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>For HTP backend extensions configurations, you can set “vtcm_mb”, “fp16_relaxed_precision” and “graph_names” through a config file.</p>
<p>Here is an example of the config file:</p>
</div></blockquote>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="p">{</span>
<span class="linenos"> 2</span><span class="w">   </span><span class="s">&quot;graphs&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[</span>
<span class="linenos"> 3</span><span class="w">      </span><span class="p">{</span>
<span class="hll"><span class="linenos"> 4</span><span class="w">        </span><span class="s">&quot;vtcm_mb&quot;</span><span class="o">:</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span><span class="w">  </span><span class="c1">// Provides performance infrastructure configuration options that are memory specific.</span>
</span><span class="linenos"> 5</span><span class="w">                       </span><span class="c1">// Optional; if not set, QNN HTP defaults to 4.</span>
<span class="linenos"> 6</span><span class="w">        </span><span class="s">&quot;fp16_relaxed_precision&quot;</span><span class="o">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w">  </span><span class="c1">// Ensures that operations will run with relaxed precision math i.e. float16 math</span>
<span class="linenos"> 7</span>
<span class="linenos"> 8</span><span class="w">        </span><span class="s">&quot;graph_names&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[</span><span class="w"> </span><span class="s">&quot;qnn_model&quot;</span><span class="w"> </span><span class="p">]</span><span class="w">  </span><span class="c1">// Provide the list of names of the graph for the inference as specified when using qnn converter tools</span>
<span class="linenos"> 9</span><span class="w">                                        </span><span class="c1">// &quot;qnn_model&quot; must be the name of the .cpp file generated during the model conversion (without the .cpp file extension)</span>
<span class="linenos">10</span><span class="w">        </span><span class="p">.....</span>
<span class="linenos">11</span><span class="w">      </span><span class="p">},</span>
<span class="linenos">12</span><span class="w">      </span><span class="p">{</span>
<span class="linenos">13</span><span class="w">         </span><span class="p">.....</span><span class="w">  </span><span class="c1">// Other graph object</span>
<span class="linenos">14</span><span class="w">      </span><span class="p">}</span>
<span class="linenos">15</span><span class="w">   </span><span class="p">]</span>
<span class="linenos">16</span><span class="p">}</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>“fp16_relaxed_precision” is the key configuration to enable running QNN float models on HTP float runtime.
HTP Graph Configurations such as fp16_relaxed_precision, vtcm_mb etc are only applied if at least one “graph_name” is provided in backend extensions config.</p>
</div>
<p>See <code class="docutils literal notranslate"><span class="pre">&lt;QNN_SDK_ROOT&gt;/examples/QNN/NetRun/android/android-qnn-net-run.sh</span></code> script for reference
on how to use <code class="docutils literal notranslate"><span class="pre">qnn-net-run</span></code> tool on android device.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span># Run the optimized graph on HTP target
<span class="hll"><span class="linenos">2</span>$ qnn-net-run --model &lt;path_to_model_library&gt;/libQnnModel.so \ # a x86_64-linux-clang built float QNN model
</span><span class="hll"><span class="linenos">3</span>              --backend ${QNN_SDK_ROOT}/lib/x86_64-linux-clang/libQnnHtp.so \
</span><span class="linenos">4</span>              --config_file &lt;path_to_JSON_file.json&gt; \
<span class="linenos">5</span>              --output_dir &lt;output_dir_for_result&gt; \
<span class="linenos">6</span>              --input_list &lt;path_to_input_list.txt&gt;
</pre></div>
</div>
</div>
</div>
<div class="section" id="qnn-throughput-net-run">
<h3>qnn-throughput-net-run<a class="headerlink" href="#qnn-throughput-net-run" title="Permalink to this heading">¶</a></h3>
<p>The <strong>qnn-throughput-net-run</strong> tool is used to exercise the execution of multiple models on a QNN backend
or on different backends in a multi-threaded fashion. It allows repeated execution of models on a specified
backend for a specified duration or number of iterations.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="nl">Usage</span><span class="p">:</span>
<span class="o">------</span>
<span class="n">qnn</span><span class="o">-</span><span class="n">throughput</span><span class="o">-</span><span class="n">net</span><span class="o">-</span><span class="n">run</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">config</span><span class="w"> </span><span class="o">&lt;</span><span class="n">config_file</span><span class="o">&gt;</span><span class="p">.</span><span class="n">json</span><span class="p">]</span>
<span class="w">                       </span><span class="p">[</span><span class="o">--</span><span class="n">output</span><span class="w"> </span><span class="o">&lt;</span><span class="n">results</span><span class="o">&gt;</span><span class="p">.</span><span class="n">json</span><span class="p">]</span>

<span class="n">REQUIRED</span><span class="w"> </span><span class="n">argument</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="o">:</span>
<span class="w"> </span><span class="o">--</span><span class="n">config</span><span class="w">        </span><span class="o">&lt;</span><span class="kt">FILE</span><span class="o">&gt;</span><span class="p">.</span><span class="n">json</span><span class="w">       </span><span class="n">Path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">json</span><span class="w"> </span><span class="n">config</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="p">.</span>

<span class="n">OPTIONAL</span><span class="w"> </span><span class="n">argument</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="o">:</span>
<span class="w"> </span><span class="o">--</span><span class="n">output</span><span class="w">        </span><span class="o">&lt;</span><span class="kt">FILE</span><span class="o">&gt;</span><span class="p">.</span><span class="n">json</span><span class="w">       </span><span class="n">Specify</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">json</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">used</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">save</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">performance</span><span class="w"> </span><span class="n">test</span><span class="w"> </span><span class="n">results</span><span class="p">.</span>
</pre></div>
</div>
<p><strong>Configuration JSON File:</strong></p>
<p><strong>qnn-throughput-net-run</strong> uses configuration file as input to run the models on the backends.
The configuration json file comprises of four objects (required) - <strong>backends</strong>, <strong>models</strong>, <strong>contexts</strong> and
<strong>testCase</strong>.</p>
<p>Below is an example of a json configuration file. Please refer the following <a class="reference internal" href="#qtnr-config-link"><span class="std std-ref">section</span></a>
for detailed information on the four configuration objects <a class="reference internal" href="#backends-link"><span class="std std-ref">backends</span></a>, <a class="reference internal" href="#models-link"><span class="std std-ref">models</span></a>,
<a class="reference internal" href="#contexts-link"><span class="std std-ref">contexts</span></a> and <a class="reference internal" href="#testcase-link"><span class="std std-ref">testCase</span></a>.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">  </span><span class="s">&quot;backends&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="s">&quot;backendName&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;cpu_backend&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;backendPath&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;libQnnCpu.so&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;profilingLevel&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;BASIC&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;backendExtensions&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;libQnnHtpNetRunExtensions.so&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;perfProfile&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;high_performance&quot;</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="s">&quot;backendName&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;gpu_backend&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;backendPath&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;libQnnGpu.so&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;profilingLevel&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;OFF&quot;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">],</span>
<span class="w">  </span><span class="s">&quot;models&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="s">&quot;modelName&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;model_1&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;modelPath&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;libqnn_model_1.so&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;loadFromCachedBinary&quot;</span><span class="o">:</span><span class="w"> </span><span class="nb">false</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;inputPath&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;model_1-input_list.txt&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;inputDataType&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;FLOAT&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;postProcessor&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;MSE&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;outputPath&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;model_1-output&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;outputDataType&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;FLOAT_ONLY&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;saveOutput&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;NATIVE_ALL&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;groundTruthPath&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;model_1-golden_list.txt&quot;</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="s">&quot;modelName&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;model_2&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;modelPath&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;libqnn_model_2.so&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;loadFromCachedBinary&quot;</span><span class="o">:</span><span class="w"> </span><span class="nb">false</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;inputPath&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;model_2-input_list.txt&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;inputDataType&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;FLOAT&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;postProcessor&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;MSE&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;outputPath&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;model_2-output&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;outputDataType&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;FLOAT_ONLY&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s">&quot;saveOutput&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;NATIVE_LAST&quot;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">],</span>
<span class="w">  </span><span class="s">&quot;contexts&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="s">&quot;contextName&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;cpu_context_1&quot;</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="s">&quot;contextName&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;gpu_context_1&quot;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">],</span>
<span class="w">  </span><span class="s">&quot;testCase&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="s">&quot;iteration&quot;</span><span class="o">:</span><span class="w"> </span><span class="mi">5</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;logLevel&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;error&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;threads&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">      </span><span class="p">{</span>
<span class="w">        </span><span class="s">&quot;threadName&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;cpu_thread_1&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;backend&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;cpu_backend&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;context&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;cpu_context_1&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;model&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;model_1&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;interval&quot;</span><span class="o">:</span><span class="w"> </span><span class="mi">10</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;loopUnit&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;count&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;loop&quot;</span><span class="o">:</span><span class="w"> </span><span class="mi">1</span>
<span class="w">      </span><span class="p">},</span>
<span class="w">      </span><span class="p">{</span>
<span class="w">        </span><span class="s">&quot;threadName&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;gpu_thread_1&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;backend&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;gpu_backend&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;context&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;gpu_context_1&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;model&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;model_2&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;interval&quot;</span><span class="o">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;loopUnit&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;count&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;loop&quot;</span><span class="o">:</span><span class="w"> </span><span class="mi">10</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">    </span><span class="p">]</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p id="backends-link"><span id="qtnr-config-link"></span><strong>backends</strong> : Property value is an array of json objects, where each object contains the needed backend
information on which the models are executed.
Each object of the array has the following properties as key/value pairs.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Key</p></th>
<th class="head"><p>Value Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Optional / Required</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">backendName</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Required</span></code></p></td>
<td><p>Is a unique identifier for the testcase to designate on which backend the model should be run.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">backendPath</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Required</span></code></p></td>
<td><p>Specifies the on device backend .so library file path.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">profilingLevel</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">OFF</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional</span></code></p></td>
<td><p>Sets the QNN profiling level for the backend.
Possible values: OFF, BASIC, DETAILED.</p>
<blockquote>
<div><ul class="simple">
<li><p>BASIC - Captures execution and init times.</p></li>
<li><p>DETAILED - In addition to BASIC captures per Op timing for execution, if backend supports.</p></li>
</ul>
</div></blockquote>
</td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">backendExtensions</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional</span></code></p></td>
<td><p>Enables backend specific options through optional backend extensions
shared library and config file.
<code class="docutils literal notranslate"><span class="pre">Syntax:</span> <span class="pre">path_to_shared_library</span></code>.</p>
<p>This is required to enable various performance modes which are
exercised using <code class="docutils literal notranslate"><span class="pre">perfProfile</span></code> option.
Currently, HTP supports it through <code class="docutils literal notranslate"><span class="pre">libQnnHtpNetRunExtensions.so</span></code> shared library.</p>
</td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">perfProfile</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">default</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional</span></code></p></td>
<td><p>Specifies performance profile to set.</p>
<p>Possible values: <code class="docutils literal notranslate"><span class="pre">low_balanced,</span> <span class="pre">balanced,</span> <span class="pre">default,</span> <span class="pre">high_performance,</span></code>
<code class="docutils literal notranslate"><span class="pre">sustained_high_performance,</span> <span class="pre">burst,</span> <span class="pre">low_power_saver,</span> <span class="pre">power_saver,</span></code>
<code class="docutils literal notranslate"><span class="pre">high_power_saver,</span> <span class="pre">extreme_power_saver</span> <span class="pre">and</span> <span class="pre">system_settings</span></code>.</p>
</td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">opPackagePath</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Native</span> <span class="pre">QNN</span> <span class="pre">Ops.</span></code>
<code class="docutils literal notranslate"><span class="pre">part</span> <span class="pre">of</span> <span class="pre">the</span> <span class="pre">backend</span></code>
<code class="docutils literal notranslate"><span class="pre">libraries</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional</span></code></p></td>
<td><p>Comma seperated list of custom op packages and interface providers for registration.</p>
<p><code class="docutils literal notranslate"><span class="pre">Syntax:</span> <span class="pre">op_package_1_path:interface_provider_1[,op_package_2_path:interface_provider_2…]</span></code></p>
</td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">platformOption</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional</span></code></p></td>
<td><p>Enables backend specific platform options through QnnBackend_Config_t.</p>
<p><code class="docutils literal notranslate"><span class="pre">Syntax:</span> <span class="pre">&quot;key:value&quot;</span></code></p>
</td>
</tr>
</tbody>
</table>
<p id="models-link"><strong>models</strong> : Property value is an array of json objects, where each object contatins details about a model and
corresponding input data and post-processing information.
Each object of the array has the following properties as key/value pairs.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Key</p></th>
<th class="head"><p>Value Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Optional / Required</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">modelName</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Required</span></code></p></td>
<td><p>Is a unique identifier for the testcase to designate which model to run.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">modelPath</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Required</span></code></p></td>
<td><p>Specifies the &lt;model&gt;.so / &lt;serialized_context&gt;.bin file path.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">loadFromCachedBinary</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">bool</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">false</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional</span></code></p></td>
<td><p>Set to <code class="docutils literal notranslate"><span class="pre">true</span></code> if &lt;serialized_context&gt;.bin is used in <code class="docutils literal notranslate"><span class="pre">modelPath</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">inputPath</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional</span></code></p></td>
<td><p>Path to a file listing the inputs for the model.</p>
<p>If there are multiple graphs in the &lt;model&gt;.so / &lt;serialized_context&gt;.bin,
this has to be comma-separated list of input path of individual graph.
Syntax: Graph1_input_path[,Graph2_input_path,…]</p>
<p>If not set, Random Input Data is used.</p>
</td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">inputDataType</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">NATIVE</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional</span></code></p></td>
<td><p>Possible values: NATIVE, FLOAT.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">postProcessor</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional</span></code></p></td>
<td><p>Possible values: NONE, MSE, MSE_FLOAT32, MSE_INT8, MSE_INT16.
If there are multiple graphs in the &lt;model&gt;.so / &lt;serialized_context&gt;.bin, this has to be
comma-separated list of postProcessor values.
Syntax: MSE[,NONE,…]</p>
<p>MSE will output a mean squared error result for each execution with the golden file specified by the parameter
<code class="docutils literal notranslate"><span class="pre">groundTruthPath</span></code>. If the <code class="docutils literal notranslate"><span class="pre">groundTruthPath</span></code> is not specified, the first execution output result is used to
compute the MSE. If the datatype of the file specified in <code class="docutils literal notranslate"><span class="pre">groundTruthPath</span></code> is different from the network’s
output type, users need to specify the relevant datatype in the <code class="docutils literal notranslate"><span class="pre">postProcessor</span></code> parameter.</p>
</td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">outputPath</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional</span></code></p></td>
<td><p>If <code class="docutils literal notranslate"><span class="pre">postProcessor</span></code> is not <code class="docutils literal notranslate"><span class="pre">NONE</span></code>, output files and profiling logs will be saved to this directory.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">outputDataType</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">NATIVE_ONLY</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional</span></code></p></td>
<td><p>Possible values: NATIVE_ONLY, FLOAT_ONLY, FLOAT_AND_NATIVE.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">saveOutput</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">NONE</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional</span></code></p></td>
<td><p>Possible values: NONE, NATIVE_LAST,NATIVE_ALL.</p>
<blockquote>
<div><ul class="simple">
<li><p>NATIVE_LAST - Saves only the result of the last network execution to the <code class="docutils literal notranslate"><span class="pre">outputPath</span></code>.</p></li>
<li><p>NATIVE_ALL - Saves the results of all network executions to the <code class="docutils literal notranslate"><span class="pre">outputPath</span></code>.</p></li>
</ul>
</div></blockquote>
</td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">groundTruthPath</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">NONE</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional</span></code></p></td>
<td><p>Specifies the golden file path for computing the MSE.
If there are multiple graphs in the &lt;model&gt;.so / &lt;serialized_context&gt;.bin,
this has to be comma-separated list of ground truth path of individual graph.
Syntax: Graph1_ground_truth_path_[,Graph2_ground_truth_path_,…]</p></td>
</tr>
</tbody>
</table>
<p id="contexts-link"><strong>contexts</strong> : Property value is an array of json objects, where each object contains all the context information.
Each object of the array has the following properties as key/value pairs.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Key</p></th>
<th class="head"><p>Value Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Optional / Required</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">contextName</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Required</span></code></p></td>
<td><p>Is a unique identifier for the testcase to designate the context in which a model should be created.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">priority</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">DEFAULT</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional</span></code></p></td>
<td><p>Specifies the priority of the context.
Possible values: DEFAULT, LOW, NORMAL, HIGH.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">executeAsyncQueueDepth</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional</span></code></p></td>
<td><p>Specfies the queue depth for async execution.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">cacheCompatibilityMode</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional</span></code></p></td>
<td><p>Specifies the cache compatibility check mode; valid values are: “permissive” (default), and “strict”.</p></td>
</tr>
</tbody>
</table>
<p id="testcase-link"><strong>testCase</strong> : Property value is a json object that specifies the testing configuration that controls
multi-threaded execution.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Key</p></th>
<th class="head"><p>Value Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Optional / Required</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">iteration</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Required</span></code></p></td>
<td><p>Number of times the entire use case is repeated. If the value is <code class="docutils literal notranslate"><span class="pre">negative</span></code>, test runs forever
until keyboard interrupt.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">logLevel</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional</span></code></p></td>
<td><p>Specifies max logging level to be set. Valid settings: <code class="docutils literal notranslate"><span class="pre">error</span></code>, <code class="docutils literal notranslate"><span class="pre">warn</span></code>, <code class="docutils literal notranslate"><span class="pre">info</span></code>, <code class="docutils literal notranslate"><span class="pre">debug</span></code>, and <code class="docutils literal notranslate"><span class="pre">verbose</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">threads</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Required</span></code></p></td>
<td><p>Property value is an array of json objects, where each object contains all the thread details,
that are to be executed by the qnn-throughput-net-run. Each object of the array has the below
properties listed under <a class="reference internal" href="#threads-link"><span class="std std-ref">threads</span></a> as  key/value pairs.</p></td>
</tr>
</tbody>
</table>
<p id="threads-link"><code class="docutils literal notranslate"><span class="pre">threads</span></code> : Property value is an array containing all the threads and corresponding backend, context
and models information.
Each element of the array can have the following required/optional property.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Key</p></th>
<th class="head"><p>Value Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Optional / Required</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">threadName</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Required</span></code></p></td>
<td><p>Is a unique identifier for the testcase to identify the thread and save the output results.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">backend</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Required</span></code></p></td>
<td><p>Specifies the backend to be used when this thread executes the graph.
The value specified should match with one of the <code class="docutils literal notranslate"><span class="pre">backendName</span></code> entry in the <code class="docutils literal notranslate"><span class="pre">backends</span></code> property of the configuration json.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">context</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Required</span></code></p></td>
<td><p>Specifies the context to be used when this thread executes the graph.
The value specified should match with one of the <code class="docutils literal notranslate"><span class="pre">contextName</span></code> entry in the <code class="docutils literal notranslate"><span class="pre">contexts</span></code> property of the configuration json.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">model</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Required</span></code></p></td>
<td><p>Specifies the model to be used by the thread for execution.
The value specified should match with one of the <code class="docutils literal notranslate"><span class="pre">modelName</span></code> entry in the <code class="docutils literal notranslate"><span class="pre">models</span></code> property of the configuration json.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">initModelInLoop</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">bool</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">false</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional</span></code></p></td>
<td><p>Set it to <code class="docutils literal notranslate"><span class="pre">true</span></code> if the model needs to be initialized repeatedly for every iteration. The value cannot be set to <code class="docutils literal notranslate"><span class="pre">true</span></code>
if <code class="docutils literal notranslate"><span class="pre">loadFromCachedBinary</span></code> from <code class="docutils literal notranslate"><span class="pre">models</span></code> property is  <code class="docutils literal notranslate"><span class="pre">true</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">loadInputDataInLoop</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">bool</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">false</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional</span></code></p></td>
<td><p>Set it to <code class="docutils literal notranslate"><span class="pre">true</span></code> if the input needs to be reloaded for every loop of execution.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">useRandomData</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">bool</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">false</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional</span></code></p></td>
<td><p>Set it to <code class="docutils literal notranslate"><span class="pre">true</span></code> if random data is needed to be used as input.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">interval</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">0</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional</span></code></p></td>
<td><p>Repesents the interval (in microseconds) between each graph execution in the thread.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">loopUnit</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">count</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional</span></code></p></td>
<td><p>Possible values: count, second.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">loop</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">1</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional</span></code></p></td>
<td><p>Value is taken either as seconds or count based on the value for the <code class="docutils literal notranslate"><span class="pre">loopUnit</span></code>.
If <code class="docutils literal notranslate"><span class="pre">loopUnit</span></code> is <code class="docutils literal notranslate"><span class="pre">second</span></code>, the value specifies the number of seconds the threads repeats execution.
If <code class="docutils literal notranslate"><span class="pre">loopUnit</span></code> is <code class="docutils literal notranslate"><span class="pre">count</span></code>, the value specifies number of times thread repeats execution.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">executeAsynchronous</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">bool</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">false</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional</span></code></p></td>
<td><p>Set it to <code class="docutils literal notranslate"><span class="pre">true</span></code> if the graphs should be executed asynchronously rather than synchronously.
If the backend does not support asynchronous execution, this option results in an error.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">backendConfig</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">string</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">-</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Optional</span></code></p></td>
<td><p>Specifies the backend config file to enable backend specific options through <code class="docutils literal notranslate"><span class="pre">backendExtensions</span></code>
shared library.
<code class="docutils literal notranslate"><span class="pre">Syntax:</span> <span class="pre">path_to_backend_config_file</span></code>.</p></td>
</tr>
</tbody>
</table>
<p>An example json file <code class="docutils literal notranslate"><span class="pre">sample_config.json</span></code> file can be found at <code class="docutils literal notranslate"><span class="pre">&lt;QNN_SDK_ROOT&gt;/examples/QNN/ThroughputNetRun</span></code>.</p>
</div>
</div>
<div class="section" id="analysis">
<h2>Analysis<a class="headerlink" href="#analysis" title="Permalink to this heading">¶</a></h2>
<div class="section" id="qnn-quantization-checker-beta">
<h3>qnn-quantization-checker (<a class="reference internal" href="#qnn-ai-tools-beta-note"><span class="std std-ref">Beta</span></a>)<a class="headerlink" href="#qnn-quantization-checker-beta" title="Permalink to this heading">¶</a></h3>
<p>The <strong>qnn-quantization-checker</strong> tool is used to analyze the
activations, weights and biases of all the possible
quantization options available in the qnn-converter for each subsequent
layer of a single model file or a directory of models. The analysis
involves comparing the weight, bias and activation tensors of the quantized
options to their unquantized counterparts. The tool runs the model to
generate floating-point activations and analyzes floating-point weights, biases
and activations to determine the quality of the encodings. It finally outputs the
problematic weight, bias and activation tensors for all quantization options.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">X86</span><span class="o">-</span><span class="n">Linux</span><span class="o">/</span><span class="w"> </span><span class="n">WSL</span><span class="w"> </span><span class="n">Usage</span><span class="o">:</span><span class="w"> </span><span class="n">qnn</span><span class="o">-</span><span class="n">quantization</span><span class="o">-</span><span class="n">checker</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">model</span><span class="w"> </span><span class="n">MODEL_PATH</span><span class="p">]</span>
<span class="w">                                </span><span class="p">[</span><span class="o">--</span><span class="n">input_list</span><span class="w"> </span><span class="n">INPUT_LIST_PATH</span><span class="p">]</span>
<span class="w">                                </span><span class="p">[</span><span class="o">--</span><span class="n">activation_width</span><span class="w"> </span><span class="n">ACT_BW</span><span class="p">]</span>
<span class="w">                                </span><span class="p">[</span><span class="o">--</span><span class="n">bias_width</span><span class="w"> </span><span class="n">BIAS_BW</span><span class="p">]</span>
<span class="w">                                </span><span class="p">[</span><span class="o">--</span><span class="n">output_dir</span><span class="w"> </span><span class="n">OUTPUT_DIR_PATH</span><span class="p">]</span>
<span class="w">                                </span><span class="p">[</span><span class="o">--</span><span class="n">skip_building_model</span><span class="p">]</span>
<span class="w">                                </span><span class="p">[</span><span class="o">--</span><span class="n">skip_generator</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">skip_runner</span><span class="p">]</span>
<span class="w">                                </span><span class="p">[</span><span class="o">--</span><span class="n">generate_histogram</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">per_channel_histogram</span><span class="p">]</span>
<span class="w">                                </span><span class="p">[</span><span class="o">--</span><span class="n">output_csv</span><span class="p">]</span>
<span class="w">                                </span><span class="o">--</span><span class="n">config_file</span><span class="w"> </span><span class="n">CONFIG_FILE_PATH</span>

<span class="n">X86</span><span class="o">-</span><span class="n">Windows</span><span class="o">/</span><span class="w"> </span><span class="n">Windows</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">Snapdragon</span><span class="w"> </span><span class="n">Usage</span><span class="o">:</span><span class="w"> </span><span class="n">python</span><span class="w"> </span><span class="n">qnn</span><span class="o">-</span><span class="n">quantization</span><span class="o">-</span><span class="n">checker</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">model</span><span class="w"> </span><span class="n">MODEL_PATH</span><span class="p">]</span>
<span class="w">                                </span><span class="p">[</span><span class="o">--</span><span class="n">input_list</span><span class="w"> </span><span class="n">INPUT_LIST_PATH</span><span class="p">]</span>
<span class="w">                                </span><span class="p">[</span><span class="o">--</span><span class="n">activation_width</span><span class="w"> </span><span class="n">ACT_BW</span><span class="p">]</span>
<span class="w">                                </span><span class="p">[</span><span class="o">--</span><span class="n">bias_width</span><span class="w"> </span><span class="n">BIAS_BW</span><span class="p">]</span>
<span class="w">                                </span><span class="p">[</span><span class="o">--</span><span class="n">output_dir</span><span class="w"> </span><span class="n">OUTPUT_DIR_PATH</span><span class="p">]</span>
<span class="w">                                </span><span class="p">[</span><span class="o">--</span><span class="n">skip_building_model</span><span class="p">]</span>
<span class="w">                                </span><span class="p">[</span><span class="o">--</span><span class="n">skip_generator</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">skip_runner</span><span class="p">]</span>
<span class="w">                                </span><span class="p">[</span><span class="o">--</span><span class="n">generate_histogram</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">per_channel_histogram</span><span class="p">]</span>
<span class="w">                                </span><span class="p">[</span><span class="o">--</span><span class="n">output_csv</span><span class="p">]</span>
<span class="w">                                </span><span class="o">--</span><span class="n">config_file</span><span class="w"> </span><span class="n">CONFIG_FILE_PATH</span>
</pre></div>
</div>
<dl>
<dt>required arguments:</dt><dd><blockquote>
<div><dl class="simple">
<dt>–config_file - Path to the config file specifying all possible options required for execution.</dt><dd><p>E.g., [RUN_IN_REPAIR_MODE, MODEL_PATH, INPUT_LIST_PATH, ACTIVATION_WIDTH, BIAS_WIDTH, WEIGHT_WIDTH, INPUT_LAYOUT, OUTPUT_DIR_PATH, CLANG_PATH, BASH_PATH, BIN_PATH, PY3_PATH, TENSORFLOW_HOME, TFLITE_HOME, ONNX_HOME, QUANTIZATION_OVERRIDES, WEIGHT_COMPARISON_ALGORITHMS, BIAS_COMPARISON_ALGORITHMS, ACT_COMPARISON_ALGORITHMS, INPUT_DATA_ANALYSIS_ALGORITHMS, OUTPUT_CSV, GENERATE_HISTOGRAM, QUANTIZATION_VARIATIONS, QUANTIZATION_ALGORITHMS]</p>
</dd>
</dl>
</div></blockquote>
<dl class="simple">
<dt>optional arguments:</dt><dd><p>–model - Path to model graph file, it is required if MODEL_PATH is not set in the config_file.
–input_list - Path to a text file containing a list of input files. It is required if the MODEL_PATH/model refers to a single model file and INPUT_LIST_PATH is not present in the config_file.
–activation_width - [Optional] Bit-width to use for activations. E.g., 8, 16. Default is 8. It can also be set using ACTIVATION_WIDTH field in the config_file.
–bias_width - [Optional] Bit-width to use for biases. E.g., 8, 32. Default is 8. It can also be set using BIAS_WIDTH in the config_file.
–output_dir - [Optional] Path to store the unquantized output files. It can also be set using OUTPUT_DIR_PATH in the config_file.
–skip_building_model - [Optional] Stop the tool from building the model. It is assumed the model.so is pre-built.
–skip_generator - [Optional] Stop the tool from running the QNN converter. It is assumed that the necessary files are already available.
–skip_runner - [Optional] Stop the tool from running the model. It is assumed that the necessary files are already available.
–generate_histogram - [Optional] Generate histogram analysis for weights/biases. Default is to skip histgoram generation.
–per_channel_histogram - [Optional] Generate per channel histogram analysis for weights/biases. Default is to skip histgoram generation.
–output_csv - [Optional] Store analysis results in csv files in the output directory.</p>
</dd>
</dl>
</dd>
</dl>
<p>Please note: the arguments accepted on the command line will override those in the config file if there is overlap.</p>
<p>Here is an example of different algorithms and thresholds mentioned in the config file:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">   </span><span class="c1">// All other required user options can be added here</span>

<span class="w">   </span><span class="s">&quot;WEIGHT_COMPARISON_ALGORITHMS&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;minmax&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;threshold&quot;</span><span class="o">:</span><span class="s">&quot;10&quot;</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;maxdiff&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;threshold&quot;</span><span class="o">:</span><span class="s">&quot;10&quot;</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;sqnr&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;threshold&quot;</span><span class="o">:</span><span class="s">&quot;26&quot;</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;stats&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;threshold&quot;</span><span class="o">:</span><span class="s">&quot;2&quot;</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;data_range_analyzer&quot;</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;data_distribution_analyzer&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;threshold&quot;</span><span class="o">:</span><span class="s">&quot;0.6&quot;</span><span class="p">}],</span>
<span class="w">   </span><span class="s">&quot;BIAS_COMPARISON_ALGORITHMS&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;minmax&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;threshold&quot;</span><span class="o">:</span><span class="s">&quot;10&quot;</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;maxdiff&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;threshold&quot;</span><span class="o">:</span><span class="s">&quot;10&quot;</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;sqnr&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;threshold&quot;</span><span class="o">:</span><span class="s">&quot;26&quot;</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;stats&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;threshold&quot;</span><span class="o">:</span><span class="s">&quot;2&quot;</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;data_range_analyzer&quot;</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;data_distribution_analyzer&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;threshold&quot;</span><span class="o">:</span><span class="s">&quot;0.6&quot;</span><span class="p">}],</span>
<span class="w">   </span><span class="s">&quot;ACT_COMPARISON_ALGORITHMS&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;minmax&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;threshold&quot;</span><span class="o">:</span><span class="s">&quot;10&quot;</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;data_range_analyzer&quot;</span><span class="p">}]</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The tool generates a <strong>html</strong> directory to include all .html files containing clear indicators of failures and successes for different combinations of quantization options and input files.
The tool also produces a <strong>csv</strong> directory to include all .csv files storing detailed computation results for each metric.
Additionally, the tool generates an output log file under the <strong>qnn-quantization-checker-log</strong> directory which contains all log outputs.</p>
<p><strong>Sample Command</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">qnn</span><span class="o">-</span><span class="n">quantization</span><span class="o">-</span><span class="n">checker</span><span class="w"> </span><span class="o">--</span><span class="n">config_file</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">python</span><span class="o">/</span><span class="n">qti</span><span class="o">/</span><span class="n">aisw</span><span class="o">/</span><span class="n">quantization_checker</span><span class="o">/</span><span class="n">configs</span><span class="o">/</span><span class="n">quant_checker_config_normal</span><span class="p">.</span><span class="n">json</span>
</pre></div>
</div>
<p><strong>Sample Config File</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="s">&quot;CLANG_PATH&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;{Path_To_Clang_Lib}/clang-14.0.0/usr/bin&quot;</span><span class="p">,</span>
<span class="s">&quot;PY3_PATH&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;/{PY3_PATH}/py3env/bin&quot;</span><span class="p">,</span>
<span class="s">&quot;BASH_PATH&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;/bin&quot;</span><span class="p">,</span>
<span class="s">&quot;BIN_PATH&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;/usr/bin&quot;</span><span class="p">,</span>
<span class="s">&quot;TENSORFLOW_HOME&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;/{Path_To_Tensorflow}/bionic/py3&quot;</span><span class="p">,</span>
<span class="s">&quot;TFLITE_HOME&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;/{Path_To_TFLITE}/bionic/py3&quot;</span><span class="p">,</span>
<span class="s">&quot;ONNX_HOME&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;/{Path_To_Onnx}/bionic/py3&quot;</span><span class="p">,</span>
<span class="s">&quot;MODEL_PATH&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;/{Path_To_Model}/model_frozen.pb&quot;</span><span class="p">,</span>
<span class="s">&quot;INPUT_LIST_PATH&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;/{Path_To_Input}/input_list.txt&quot;</span><span class="p">,</span>
<span class="s">&quot;OUTPUT_DIR_PATH&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;{Path_To_Output_Dir}&quot;</span>
<span class="s">&quot;WEIGHT_COMPARISON_ALGORITHMS&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;minmax&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;threshold&quot;</span><span class="o">:</span><span class="s">&quot;10&quot;</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;maxdiff&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;threshold&quot;</span><span class="o">:</span><span class="s">&quot;10&quot;</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;sqnr&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;threshold&quot;</span><span class="o">:</span><span class="s">&quot;26&quot;</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;stats&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;threshold&quot;</span><span class="o">:</span><span class="s">&quot;2&quot;</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;data_range_analyzer&quot;</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;data_distribution_analyzer&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;threshold&quot;</span><span class="o">:</span><span class="s">&quot;0.6&quot;</span><span class="p">}],</span>
<span class="s">&quot;BIAS_COMPARISON_ALGORITHMS&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;minmax&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;threshold&quot;</span><span class="o">:</span><span class="s">&quot;10&quot;</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;maxdiff&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;threshold&quot;</span><span class="o">:</span><span class="s">&quot;10&quot;</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;sqnr&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;threshold&quot;</span><span class="o">:</span><span class="s">&quot;26&quot;</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;stats&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;threshold&quot;</span><span class="o">:</span><span class="s">&quot;2&quot;</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;data_range_analyzer&quot;</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;data_distribution_analyzer&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;threshold&quot;</span><span class="o">:</span><span class="s">&quot;0.6&quot;</span><span class="p">}],</span>
<span class="s">&quot;ACT_COMPARISON_ALGORITHMS&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;minmax&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;threshold&quot;</span><span class="o">:</span><span class="s">&quot;10&quot;</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;data_range_analyzer&quot;</span><span class="p">}],</span>
<span class="s">&quot;INPUT_DATA_ANALYSIS_ALGORITHMS&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[{</span><span class="s">&quot;algo_name&quot;</span><span class="o">:</span><span class="s">&quot;stats&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;threshold&quot;</span><span class="o">:</span><span class="s">&quot;2&quot;</span><span class="p">}],</span>
<span class="s">&quot;OUTPUT_CSV&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;True&quot;</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<div class="section" id="viewing-the-results-html-csv-or-log-files">
<h4>Viewing the results (html, csv or log files)<a class="headerlink" href="#viewing-the-results-html-csv-or-log-files" title="Permalink to this heading">¶</a></h4>
<p>All results are stored in output directories under the user specified directory.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="cp"># In user_specified_dir, all .html files are stored in &quot;user_specified_dir/html&quot;, .csv files are stored in &quot;user_specified_dir/csv&quot; and log files are stored in &quot;user_specified_dir/snpe-quantization-checker-log&quot;.</span>
<span class="n">cd</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">Path_to_the_model</span><span class="p">}</span><span class="o">/</span><span class="n">user_specified_dir</span><span class="o">/</span><span class="n">html</span>
<span class="cp"># This directory contains all the html files produced by the snpe-quantization-checker which can be opened with any standard web browser.</span>
<span class="n">cd</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">Path_to_the_model</span><span class="p">}</span><span class="o">/</span><span class="n">user_specified_dir</span><span class="o">/</span><span class="n">csv</span>
<span class="cp"># This directory contains all the csv files produced by the snpe-quantization-checker which can be viewed with any standard text editor or spreadsheet application such as Excel, LibreOffice, etc.</span>
<span class="n">cd</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">Path_to_the_model</span><span class="p">}</span><span class="o">/</span><span class="n">user_specified_dir</span><span class="o">/</span><span class="n">snpe</span><span class="o">-</span><span class="n">quantization</span><span class="o">-</span><span class="n">checker</span><span class="o">-</span><span class="n">log</span>
<span class="cp"># This directory contains all the log files produced by the snpe-quantization-checker. The files are time stamped and can be viewed in any text editor.</span>

<span class="n">If</span><span class="w"> </span><span class="n">histogram</span><span class="w"> </span><span class="n">generation</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">specified</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">config</span><span class="w"> </span><span class="n">file</span><span class="p">,</span><span class="w"> </span><span class="n">histogram</span><span class="w"> </span><span class="n">analysis</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">weights</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">biases</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">also</span><span class="w"> </span><span class="n">produced</span><span class="w"> </span><span class="n">into</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">respective</span><span class="w"> </span><span class="n">directories</span><span class="w"> </span><span class="n">as</span><span class="w"> </span><span class="n">followed</span><span class="o">:</span>
<span class="n">cd</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">Path_to_the_model</span><span class="p">}</span><span class="o">/</span><span class="n">user_specified_dir</span><span class="o">/</span><span class="n">hist_analysis_weights</span>
<span class="cp"># This directory contains all the png files representing pixelwise data distribution for unquantized and quantized weights. The files can be viewed in any image viewer.</span>
<span class="n">cd</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">Path_to_the_model</span><span class="p">}</span><span class="o">/</span><span class="n">user_specified_dir</span><span class="o">/</span><span class="n">hist_analysis_biases</span>
<span class="cp"># This directory contains all the png files representing pixelwise data distribution for unquantized and quantized biases. The files can be viewed in any image viewer.</span>
</pre></div>
</div>
<div class="line-block">
<div class="line"><strong>HTML Results Files</strong></div>
</div>
<p>Each HTML file contains a summary of the results for each quantization option and for each input file provided.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Snapshot of HTML content has been added for clarity.</p>
</div>
<div class="figure align-default">
<img alt="../_static/resources/qnn_quantatization_checker_html_sample.png" src="../_static/resources/qnn_quantatization_checker_html_sample.png" />
</div>
<div class="line-block">
<div class="line"><strong>CSV Results Files</strong></div>
</div>
<p>Each CSV file contains detailed computation results for a specific node type (activation/weight/bias) and quantization option.
Each row in the csv file displays the op name, node name, passes accuracy (True/False), computation result (accuracy differences), threshold being used for each algorithm and the algorithm name.
Computation results(accuracy differences) format can be somewhat different according to different algorithms/metrics.</p>
<p>The following are a few short notes about the different algorithms and the information each csv row contains:</p>
<ul class="simple">
<li><dl class="simple">
<dt>minmax: Indicates the difference between the unquantized minimum and the dequantized minimum value. Correspondingly, indicates the same difference for the maximum unquantized and dequantized value.</dt><dd><p>Example computation result: “min: #VALUE max: #VALUE”</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>maxdiff: Calculates the absolute difference between the unquantized and dequantized data for all data points and displays the maximum value of the result.</dt><dd><p>Example computation result: “#VALUE”</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>sqnr: Calculates the signal to quantization noise ratio between the two tensors of unquantized and dequantized data.</dt><dd><p>Example computation result: “#VALUE”</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>data_range_analyzer: Calculates the difference between the maximum and minimum values in a tensor and compares that to the maximum value supported by the bit-width used to determine if the range of values can be reasonably represented by the selected quantization bit width.</dt><dd><p>Example computation result: “unique dec places: #INT_VALUE data range : #VALUE”. Information in the computation results field includes how many unique decimal places we need to express the unquantized data in quantized format and what is the actual data range.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>data_distribution_analyzer: Calculates the clustering of the data to find whether a large number of unique unquantized values are quantized to the same value or not.</dt><dd><p>Example computation result: “Distribution of pixels above threshold: #VALUE”</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>stats: Calculates some basic statistics on the received data such as the min, max, median, variance, standard deviation, the mode and the skew. The skew is used to indicate how symmetric the data is.</dt><dd><p>Example computation result: skew: #VALUE min: #VALUE max: #VALUE median: #VALUE variance: #VALUE stdDev: #VALUE mode: #VALUE</p>
</dd>
</dl>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Snapshot of a CSV file content for weight data for one of the quantization options has been added below.</p>
</div>
<div class="figure align-default">
<img alt="../_static/resources/qnn_quantatization_checker_csv_weights.png" src="../_static/resources/qnn_quantatization_checker_csv_weights.png" />
</div>
<p>Separate .csv files are available for activations, weights and biases for each quantization option.
The activation related results also include analysis for each input file provided.</p>
<div class="line-block">
<div class="line"><strong>Log Result File</strong></div>
</div>
<p>The log files contain the following information:</p>
<ol class="arabic simple">
<li><p>All the commands executed as part of the script’s run. This will
include different runs of the qnn-converter tool with different
quantization options</p></li>
<li><p>Activations Analysis Failures</p></li>
<li><p>Weights Analysis Failures</p></li>
<li><p>Biases Analysis Failures</p></li>
</ol>
<p>A sample log output looks like below:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="o">&lt;====</span><span class="n">ACTIVATIONS</span><span class="w"> </span><span class="n">ANALYSIS</span><span class="w"> </span><span class="n">FAILURES</span><span class="o">====&gt;</span>

<span class="n">Results</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">enhanced</span><span class="w"> </span><span class="n">quantization</span><span class="o">:</span>
<span class="o">|</span><span class="w">         </span><span class="n">Op</span><span class="w"> </span><span class="n">Name</span><span class="w">         </span><span class="o">|</span><span class="w"> </span><span class="n">Activation</span><span class="w"> </span><span class="n">Node</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Passes</span><span class="w"> </span><span class="n">Accuracy</span><span class="w"> </span><span class="o">|</span><span class="w">         </span><span class="n">Accuracy</span><span class="w"> </span><span class="n">Difference</span><span class="w">          </span><span class="o">|</span><span class="w"> </span><span class="n">Threshold</span><span class="w"> </span><span class="n">Used</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Algorithm</span><span class="w"> </span><span class="n">Used</span><span class="w"> </span><span class="o">|</span>
<span class="o">|</span><span class="w">  </span><span class="n">conv_tanh_comp1_conv0</span><span class="w">  </span><span class="o">|</span><span class="w">    </span><span class="n">ReLU_6919</span><span class="w">    </span><span class="o">|</span><span class="w">      </span><span class="n">False</span><span class="w">      </span><span class="o">|</span><span class="w"> </span><span class="n">minabs_diff</span><span class="o">:</span><span class="w"> </span><span class="mf">0.59</span><span class="w"> </span><span class="n">maxabs_diff</span><span class="o">:</span><span class="w"> </span><span class="mf">17.16</span><span class="w"> </span><span class="o">|</span><span class="w">      </span><span class="mf">0.05</span><span class="w">      </span><span class="o">|</span><span class="w">     </span><span class="n">minmax</span><span class="w">     </span><span class="o">|</span>
</pre></div>
</div>
<p>where,</p>
<ol class="arabic simple">
<li><p>Op Name : Op Name as expressed in qnn_model.cpp</p></li>
<li><p>Activation Node : Activation Node Name in the Op</p></li>
<li><p>Passes Accuracy : Pass if the quantized activation(or weight or
bias) meets threshold when compared with values from float32 graph</p></li>
<li><p>Accuracy Difference : Details about the accuracy per the algorithm
used</p></li>
<li><p>Threshold Used : The threshold used to influence the result of
“Passes Accuracy” column</p></li>
<li><p>Algorithm Used : Metric used to compare actual quantized activations/weights/biases
against unquantized float data or analyze the quality of unquantized float data.
Metrics can be minmax, maxdiff, sqnr, stats, data_range_analyzer, data_distribution_analyzer.</p></li>
</ol>
<div class="line-block">
<div class="line"><strong>Repair Mode</strong></div>
</div>
<p>This feature adds the ability to ‘repair’ nodes in a graph that demonstrate poor accuracy due to quantization.
The tool is run and identifies nodes that demonstrate poor accuracy due to quantization. Based on the rule that was violated a corrective action is determined to improve the performance (accuracy).
The tool is then rerun using the corrective action decided upon and the nodes evaluated again to see if there is an improvement in accuracy.
Determining the performance of nodes is accomplished using the current set of comparison algorithms already used within the quantization checker tool.
The current implementation is limited to an increase in the bit-width for the worst performing tensor (either weights, biases or activations) as the corrective action to be taken, i.e., the ‘repair’.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The repair mode does not guarantee an improvement in accuracy and is merely an attempt at the easiest change that can be made, thus eliminating one additional debug step for the end user.</p>
</div>
<dl class="simple">
<dt>The repair mode also supports manual mode by supplying the following argument to the config file:</dt><dd><dl class="simple">
<dt>RUN_IN_REPAIR_MODE  type: bool (True/False)</dt><dd><p>Description: run the [snpe, qnn]-quantization-checker in repair mode. This mode will identify nodes that demonstrate poor accuracy due to quantization and further adjust the quantization encodings to improve the accuracy of identified nodes.
The tool is then rerun with the updated encoding values and a report of the accuracy improvement is given upon completion.</p>
</dd>
</dl>
</dd>
</dl>
<p>Example output from the repair mode:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="nl">MINMAX</span><span class="p">:</span><span class="w"> </span><span class="n">Percentage</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">Failed</span><span class="w"> </span><span class="n">Nodes</span>

<span class="o">---------------------------------------------------------------------------------------------------------------------</span>
<span class="o">|</span><span class="w"> </span><span class="n">Tensor</span><span class="w"> </span><span class="n">Type</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">unquantized</span><span class="w"> </span><span class="o">|</span><span class="w">         </span><span class="n">tf</span><span class="w">          </span><span class="o">|</span><span class="w">       </span><span class="n">tf_cle</span><span class="w">        </span><span class="o">|</span><span class="w">      </span><span class="n">tf_pcq</span><span class="w">         </span><span class="o">|</span><span class="w">     </span><span class="n">tf_cle_pcq</span><span class="w">      </span><span class="o">|</span>
<span class="o">---------------------------------------------------------------------------------------------------------------------</span>
<span class="o">---------------------------------------------------------------------------------------------------------------------</span>
<span class="o">|</span><span class="w">   </span><span class="n">Weights</span><span class="w">   </span><span class="o">|</span><span class="w">     </span><span class="n">N</span><span class="o">/</span><span class="n">A</span><span class="w">     </span><span class="o">|</span><span class="w">        </span><span class="mf">0.0</span><span class="w">          </span><span class="o">|</span><span class="w">         </span><span class="mf">0.0</span><span class="w">         </span><span class="o">|</span><span class="w">         </span><span class="mf">0.0</span><span class="w">         </span><span class="o">|</span><span class="w">         </span><span class="mf">0.0</span><span class="w">         </span><span class="o">|</span>
<span class="o">---------------------------------------------------------------------------------------------------------------------</span>
<span class="o">|</span><span class="w">   </span><span class="n">Biases</span><span class="w">    </span><span class="o">|</span><span class="w">     </span><span class="n">N</span><span class="o">/</span><span class="n">A</span><span class="w">     </span><span class="o">|</span><span class="w">        </span><span class="mf">0.0</span><span class="w">          </span><span class="o">|</span><span class="w">         </span><span class="mf">0.0</span><span class="w">         </span><span class="o">|</span><span class="w">         </span><span class="mf">0.0</span><span class="w">         </span><span class="o">|</span><span class="w">         </span><span class="mf">0.0</span><span class="w">         </span><span class="o">|</span>
<span class="o">---------------------------------------------------------------------------------------------------------------------</span>
<span class="o">|</span><span class="w"> </span><span class="n">Activations</span><span class="w"> </span><span class="o">|</span><span class="w">     </span><span class="n">N</span><span class="o">/</span><span class="n">A</span><span class="w">     </span><span class="o">|</span><span class="w"> </span><span class="mf">0.11634671320535195</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="mf">0.11634671320535195</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="mf">0.11634671320535195</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="mf">0.11634671320535195</span><span class="w"> </span><span class="o">|</span>
<span class="o">---------------------------------------------------------------------------------------------------------------------</span>
</pre></div>
</div>
<div class="line-block">
<div class="line"><strong>Troubleshooting Steps</strong></div>
</div>
<p>The following steps are intended for end users who are having difficulty with using the tool. These are common areas to investigate and are not comprehensive.</p>
<dl class="simple">
<dt>The tool has some requirements/dependencies which originate from the snpe binaries requirements:</dt><dd><ul class="simple">
<li><p>clang 9 must be installed</p></li>
<li><p>a python 3 venv must be used</p></li>
<li><p>the bash shell interpreter must be installed</p></li>
<li><dl class="simple">
<dt>at least one of the following frameworks must be installed depending on which type of model is being analyzed:</dt><dd><ul>
<li><p>Tensorflow</p></li>
<li><p>Onnx</p></li>
<li><p>TFLite</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
<dt>Additionally a few checks can be made to ensure the model runs on the first try:</dt><dd><ul class="simple">
<li><p>ensure the path specified to the model to be analyzed is correct</p></li>
<li><p>ensure the path to the output directory exists and is writeable</p></li>
<li><p>use the provided pre-defined config files and fill out the details according to your local setup, i.e., local paths</p></li>
<li><p>failures highlighted by the tool may be acceptable failures depending on the model, it is up to the user to determine whether a failure indicates a problem or not based on the users knowledge of the particular model being analyzed</p></li>
<li><p>python errors are generally due to a version mismatch and the tool will only work with Python 3.6 and above and currently is focused on Python 3.10</p></li>
</ul>
</dd>
</dl>
</div>
</div>
<div class="section" id="qnn-accuracy-evaluator-beta">
<h3>qnn-accuracy-evaluator (<a class="reference internal" href="#qnn-ai-tools-beta-note"><span class="std std-ref">Beta</span></a>)<a class="headerlink" href="#qnn-accuracy-evaluator-beta" title="Permalink to this heading">¶</a></h3>
<p>The <strong>qnn-accuracy-evaluator</strong> tool provides a framework to evaluate end-to-end accuracy metrics for a model on a given dataset.
In addition, the tool can be used to identify the best quantization options for a model on a given set of inputs.</p>
<p><strong>Dependencies</strong></p>
<p>The QNN Accuracy Evaluator assumes that the platform dependencies and environment setup instructions have been followed as outlined in the <a class="reference internal" href="setup.html"><span class="doc">Setup</span></a> page.
Certain additional python packages are required by this tool, refer to <a class="reference internal" href="setup.html#optional-python-packages"><span class="std std-ref">Optional Python packages</span></a>.</p>
<div class="section" id="usage">
<h4>Usage<a class="headerlink" href="#usage" title="Permalink to this heading">¶</a></h4>
<p>User needs to set QNN_SDK_ROOT environment variable to root directory of QNN SDK.
The following environment variables might need to be set with appropriate values:
QNN_MODEL_ZOO : Path to model zoo. If not set model_zoo base directory path is assumed to present at “/home/model_zoo”.
Note: This environment variable is required only if the model path supplied is not absolute and relative to the set model zoo path.
ADB_PATH : Set the path to the ADB binary. If not set ADB is assumed to be present at “/opt/bin/adb”.</p>
<p>There are two modes of usage for the tool - minimal mode and config mode.</p>
<p><strong>Minimal mode</strong> option is to perform accuracy analysis on all possible
quantization options and rank them in order based on the given comparator when compared against the cpu fp32 outputs.</p>
<p><strong>Config mode</strong> supports accuracy analysis of a model on a given dataset by customizing the backends, quantization options
and reference inference schemas.</p>
</div>
<div class="section" id="supported-models">
<h4>Supported models<a class="headerlink" href="#supported-models" title="Permalink to this heading">¶</a></h4>
<p>The qnn-accuracy-evaluator currently supports ONNX and PyTorch (torchscript) models.</p>
<p><em>Note</em>: For PyTorch models, it is mandatory to provide the model’s input and output node info, wherever applicable,
as per QNN’s naming convention which can be observed when such models are executed using qnn-pytorch-converter.</p>
</div>
<div class="section" id="minimal-mode">
<h4>Minimal Mode<a class="headerlink" href="#minimal-mode" title="Permalink to this heading">¶</a></h4>
<p><strong>Minimal mode</strong> option is to perform accuracy analysis on all possible
quantization options and rank them in order based on the given comparator when compared against the cpu fp32 outputs.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>qnn-accuracy-evaluator Options

minimal mode options:
    -model MODEL          path to model or model directory
    -target_arch {aarch64-android,x86_64-linux-clang}
                          Target architecture to compile.
    -backend BACKEND      Backend to run the inference.
                          For target_arch x86_64-linux-clang, allowed backends are {htp}
                          For target_arch aarch64-android, allowed backends are {cpu,gpu,dspv69,dspv73,dspv75}
    -preproc_file PREPROC_FILE
                          Path to a file specifying paths to raw inputs.
    -comparator COMPARATOR
                          comparator to be used.
    -tol_thresh TOL_THRESH
                          Tolerance threshold to be used for the comparator
    -act_bw ACT_BW        [Optional] bitwidth to use for activations. E.g., 8,
                          1.  Default is 8.
    -bias_bw BIAS_BW      [Optional] bitwidth to use for biases. either 8
                          (default) or 32.
    -box_input BOX_INPUT  Path to the json file. Used only with the box
                          comparator

other options:
    -input_info INPUT_INFO INPUT_INFO
                            The name and dimension of all the input buffers to the
                            network specified in the format [input_name comma-
                            separated-dimensions], for example: &#39;data&#39;
                            1,224,224,3. This option is mandatory for pytorch
                            models in minimal mode.
    -onnx_symbol ONNX_SYMBOL [ONNX_SYMBOL ...]
                            Replace onnx symbols in input/output shapes.Can be
                            passed multiple timesDefault replaced by 1. e.g
                            __unk_200:1
    -device_id DEVICE_ID  Target device-id/device-serial to be provided
    -work_dir WORK_DIR    working directory path. default is ./qacc_temp
    -silent               Run in silent mode
    -debug                Set logging level to DEBUG within the tool
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<div class="line-block">
<div class="line">The HTP backend emulation on the host is not guaranteed to generate the same output tensor values as when running on an HTP on a Qualcomm SoC.</div>
<div class="line">For HTP backend execution on Android device, user must specify the backend along with the version such as “dspv69/v73/v75” and target-arch as “aarch64-android” in the CLI command.</div>
</div>
</div>
<p><strong>Sample Command</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">qnn</span><span class="o">-</span><span class="n">accuracy</span><span class="o">-</span><span class="n">evaluator</span><span class="w"> </span><span class="o">-</span><span class="n">model</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_MODEL_ZOO</span><span class="p">}</span><span class="o">/</span><span class="n">onnx</span><span class="o">-</span><span class="n">cnns_mobilenet</span><span class="o">/</span><span class="n">Source_model</span><span class="o">/</span><span class="n">model</span><span class="p">.</span><span class="n">onnx</span>
<span class="w">                       </span><span class="o">-</span><span class="n">preproc_file</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_MODEL_ZOO</span><span class="p">}</span><span class="o">/</span><span class="n">onnx</span><span class="o">-</span><span class="n">cnns_mobilenet</span><span class="o">/</span><span class="n">inputs</span><span class="o">/</span><span class="n">input_list</span><span class="p">.</span><span class="n">txt</span>
<span class="w">                       </span><span class="o">-</span><span class="n">backend</span><span class="w"> </span><span class="n">htp</span>
<span class="w">                       </span><span class="o">-</span><span class="n">target_arch</span><span class="w"> </span><span class="n">x86_64</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">clang</span>
<span class="w">                       </span><span class="o">-</span><span class="n">comparator</span><span class="w"> </span><span class="n">abs</span>
<span class="w">                       </span><span class="o">-</span><span class="n">tol_thresh</span><span class="w"> </span><span class="mf">0.1</span>
</pre></div>
</div>
<p><strong>Results</strong></p>
<p>The tool displays a table with quantization options ordered by output match based on the selected comparator and also
generates a csv file with the same data. Comparator column shows output match percentage/value based on the selected
comparator. Quant params column displays the quantization params used for that run. Other columns also show backend,
runtime/compile params used. The information is also stored in a csv file at <cite>{work_dir}/metrics-info.csv</cite>.</p>
<p>The quantization option combinations that are run in minimal mode:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="nl">param_quantizer</span><span class="p">:</span><span class="w">  </span><span class="n">tf</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">symmetric</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">enhanced</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">adjusted</span>
<span class="nl">algorithms</span><span class="p">:</span><span class="w"> </span><span class="k">default</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">cle</span>
<span class="nl">use_per_channel_quantization</span><span class="p">:</span><span class="w"> </span><span class="n">True</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">False</span>
<span class="nl">use_per_row_quantization</span><span class="p">:</span><span class="w"> </span><span class="n">True</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">False</span>
</pre></div>
</div>
<p>Each quantization option work directory is stored at <cite>{work_dir}/infer/schema{i}_qnn_{quant_option}</cite>. QNN IR files are stored at
<cite>{work_dir}/infer/schema{i}_qnn_{quant_option}/qnn_ir</cite> and outputs are stored at <cite>{work_dir}/infer/schema{i}_qnn_{quant_option}/Result_{i}</cite>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Snapshot of console log has been added for clarity.</p>
</div>
<div class="figure align-default">
<img alt="../_static/resources/qnn_acc_eval_output.png" src="../_static/resources/qnn_acc_eval_output.png" />
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Snapshot of csv file has been added for clarity.</p>
</div>
<div class="figure align-default">
<img alt="../_static/resources/qnn_acc_eval_csv.png" src="../_static/resources/qnn_acc_eval_csv.png" />
</div>
</div>
<div class="section" id="config-mode">
<h4>Config Mode<a class="headerlink" href="#config-mode" title="Permalink to this heading">¶</a></h4>
<p><strong>Config mode</strong> supports accuracy analysis of a model on a given dataset by customizing the backends,
quantization options and reference inference schemas. Sample config files can be found at
<cite>${QNN_SDK_ROOT}/lib/python/qti/aisw/accuracy_evaluator/configs/samples/model_configs</cite>.</p>
<p>The high-level structure of a model config is shown below:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">model</span>
<span class="w">    </span><span class="n">info</span>
<span class="w">    </span><span class="n">globals</span>
<span class="w">    </span><span class="n">dataset</span>
<span class="w">    </span><span class="n">processing</span>
<span class="w">        </span><span class="n">preprocessing</span>
<span class="w">        </span><span class="n">postprocessing</span>
<span class="w">    </span><span class="n">inference</span><span class="o">-</span><span class="n">engine</span>
<span class="w">    </span><span class="n">evaluator</span>
</pre></div>
</div>
<p>User needs to provide all dataset information under the dataset section in the model config file, failing which,
an error is thrown. An example of this is shown below:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>dataset:
    name: COCO2014
    path: &#39;/home/ml-datasets/COCO/2014/&#39;
    inputlist_file: inputlist.txt
    calibration:
        type: index
        file: calibration-index.txt
</pre></div>
</div>
<p>Details of the dataset fields is as follows:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Field</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>name</p></td>
<td><p>Name of the dataset</p></td>
</tr>
<tr class="row-odd"><td><p>path</p></td>
<td><p>Base directory of the dataset files</p></td>
</tr>
<tr class="row-even"><td><p>inputlist_file</p></td>
<td><div class="line-block">
<div class="line">Text file containing all the pre-processed input files relative to the path field, one input per line.</div>
<div class="line">For models having multiple inputs, the inputs in each line have to be comma separated</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>calibration</p></td>
<td><dl class="simple">
<dt>Specifies the calibration file type to be used with quantization. Optional. It has following params</dt><dd><ul class="simple">
<li><dl class="simple">
<dt>type: Type can be ‘index’, ‘raw’ or ‘dataset’</dt><dd><ul>
<li><p>index - File provided contains the indexes to be picked from inputlist for calibration</p></li>
<li><p>raw - File provided contains entries of pre-processed raw files for calibration</p></li>
<li><p>dataset - File provided contains images processed separately and passed to inference</p></li>
</ul>
</dd>
</dl>
</li>
<li><p>file: pre-processed calibration file name</p></li>
</ul>
</dd>
</dl>
</td>
</tr>
</tbody>
</table>
<p>The inference engine is used to run the model on multiple inference schemas. A sample inference engine section is shown below,
followed by the description of the different configurable entries in the inference section.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">inference</span><span class="o">-</span><span class="n">engine</span><span class="o">:</span>
<span class="w">    </span><span class="nl">model_path</span><span class="p">:</span><span class="w"> </span><span class="n">MLPerfModels</span><span class="o">/</span><span class="n">ResNetV1</span><span class="mf">.5</span><span class="o">/</span><span class="n">modelFiles</span><span class="o">/</span><span class="n">ONNX</span><span class="o">/</span><span class="n">resnet50_v1</span><span class="p">.</span><span class="n">onnx</span>
<span class="w">    </span><span class="nl">simplify_model</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="n">True</span>
<span class="w">    </span><span class="nl">inference_schemas</span><span class="p">:</span>
<span class="w">        </span><span class="o">-</span><span class="w"> </span><span class="n">inference_schema</span><span class="o">:</span>
<span class="w">            </span><span class="nl">name</span><span class="p">:</span><span class="w"> </span><span class="n">qnn</span>
<span class="w">            </span><span class="nl">backend</span><span class="p">:</span><span class="w"> </span><span class="n">htp</span>
<span class="w">            </span><span class="nl">target_arch</span><span class="p">:</span><span class="w"> </span><span class="n">x86_64</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">clang</span>
<span class="w">            </span><span class="nl">precision</span><span class="p">:</span><span class="w"> </span><span class="n">quant</span>
<span class="w">            </span><span class="nl">tag</span><span class="p">:</span><span class="w"> </span><span class="n">htp_int8</span>
<span class="w">            </span><span class="nl">converter_params</span><span class="p">:</span>
<span class="w">                </span><span class="nl">param_quantizer</span><span class="p">:</span><span class="w"> </span><span class="n">tf</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">symmetric</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">enhanced</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">adjusted</span>
<span class="w">            </span><span class="nl">contextbin_params</span><span class="p">:</span>
<span class="w">                </span><span class="nl">profiling_level</span><span class="p">:</span><span class="w"> </span><span class="n">basic</span>
<span class="w">            </span><span class="nl">netrun_params</span><span class="p">:</span>
<span class="w">                </span><span class="nl">use_native_input_files</span><span class="p">:</span><span class="w"> </span><span class="n">True</span>
<span class="w">            </span><span class="nl">backend_extensions</span><span class="p">:</span>
<span class="w">                </span><span class="nl">num_cores</span><span class="p">:</span><span class="w"> </span><span class="mi">8</span>
<span class="w">                </span><span class="nl">soc_id</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span>
<span class="w">    </span><span class="nl">inputs_info</span><span class="p">:</span>
<span class="w">        </span><span class="o">-</span><span class="w"> </span><span class="n">input_tensor_0</span><span class="o">:</span>
<span class="w">              </span><span class="nl">type</span><span class="p">:</span><span class="w"> </span><span class="n">float32</span>
<span class="w">              </span><span class="nl">shape</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s">&quot;*&quot;</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">224</span><span class="p">,</span><span class="w"> </span><span class="mi">224</span><span class="p">]</span>
<span class="w">    </span><span class="nl">outputs_info</span><span class="p">:</span>
<span class="w">        </span><span class="o">-</span><span class="w"> </span><span class="n">ArgMax_0</span><span class="o">:</span>
<span class="w">              </span><span class="nl">type</span><span class="p">:</span><span class="w"> </span><span class="n">int64</span>
<span class="w">              </span><span class="nl">shape</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s">&quot;*&quot;</span><span class="p">]</span>
<span class="w">        </span><span class="o">-</span><span class="w"> </span><span class="n">softmax_tensor_0</span><span class="o">:</span>
<span class="w">              </span><span class="nl">type</span><span class="p">:</span><span class="w"> </span><span class="n">float32</span>
<span class="w">              </span><span class="nl">shape</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s">&quot;*&quot;</span><span class="p">,</span><span class="w"> </span><span class="mi">1001</span><span class="p">]</span>
</pre></div>
</div>
<p>Details of each configurable entry is given below:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Field</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>model_path</p></td>
<td><p>Absolute or relative path of the model. If the path is relative, it would be taken relative to MODEL_ZOO_PATH,
if set, else default /home/model_zoo</p></td>
</tr>
<tr class="row-odd"><td><p>simplify_model</p></td>
<td><p>Flag to enable or disable model simplification for ONNX models. By default, this flag is set to True and the model would be simplified. Note: Model simplification would be skipped for models having custom operators or for inference schemas having quantization_overrides parameter configured.</p></td>
</tr>
<tr class="row-even"><td><p>inference_schemas</p></td>
<td><dl class="simple">
<dt>List of inference schemas to perform inference on. Each inference_schema has further entries as the following:</dt><dd><ul class="simple">
<li><p>name - Name of the inference schema. Options: qnn, onnxrt, tensorflow, torchscript, tensorflow-session</p></li>
<li><p>precision - Precision to run inference on. Options: fp32, fp16, int8/quant</p></li>
<li><p>target_arch - Target architecture to run inference on. Options: x86_64-linux-clang, aarch64-android</p></li>
<li><p>backend - Backend to run inference on. Allowed backends for x86_64-linux-clang: {cpu,htp} and aarch64: {cpu,gpu,dspv69,dspv73,dspv75}.</p></li>
<li><p>tag - Tag unique for a inference schema</p></li>
<li><p>converter_params - Params to be passed as arguments to converter</p></li>
<li><p>contextbin_params - Params to be passed as arguments to context-binary-generator</p></li>
<li><p>netrun_params - Params to be passed as arguments to net-run</p></li>
<li><p>backend_extensions - Params to be passed as backend extensions config file to context-binary-generator and net-run</p></li>
</ul>
</dd>
</dl>
</td>
</tr>
<tr class="row-odd"><td><p>input_info</p></td>
<td><dl class="simple">
<dt>Information about each model input. Requires following params in the given order</dt><dd><ul class="simple">
<li><p>type - numpy type (float16, float32, float64, int8, int16, int32/int, int64)</p></li>
<li><p>shape - list of dimensions</p></li>
</ul>
</dd>
</dl>
</td>
</tr>
<tr class="row-even"><td><p>output_info</p></td>
<td><dl class="simple">
<dt>Information about each model output. Requires following params in the given order</dt><dd><ul class="simple">
<li><p>type - numpy type (float16, float32, float64, int8, int16, int32/int, int64)</p></li>
<li><p>shape - list of dimensions</p></li>
</ul>
</dd>
</dl>
</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<div class="line-block">
<div class="line">For HTP backend emulation on host, set backend to “htp” and target_arch as “x86_64-linux-clang” in the config file.</div>
<div class="line">For HTP backend execution on Android device, user must specify the backend along with the version such as dspv69 / dspv73 / dspv75 and target_arch as “aarch64-android” in the config file.</div>
</div>
</div>
<p>The evaluator section provides information about the comparator being used to compare the inference outputs, in case of multiple inference schemas. A sample evaluator
section is shown below, followed by the description of the different configurable entries in the section.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="nl">evaluator</span><span class="p">:</span>
<span class="w">    </span><span class="nl">comparator</span><span class="p">:</span>
<span class="w">        </span><span class="nl">enabled</span><span class="p">:</span><span class="w"> </span><span class="n">True</span>
<span class="w">        </span><span class="n">fetch</span><span class="o">-</span><span class="n">top</span><span class="o">:</span><span class="w"> </span><span class="mi">1</span>
<span class="w">        </span><span class="nl">type</span><span class="p">:</span><span class="w"> </span><span class="n">avg</span>
<span class="w">        </span><span class="nl">tol</span><span class="p">:</span><span class="w"> </span><span class="mf">0.001</span>
</pre></div>
</div>
<p>Details of each configurable entry is given below:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Field</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>comparator</p></td>
<td><dl class="simple">
<dt>If multiple inference schemas are provided, compare the inference outputs with the reference inference schema. If reference inference schema is not defined, the first inference schema is considered the reference. If only one inference schema is defined, comparator is not executed. Following params need to be provided</dt><dd><ul class="simple">
<li><p>enabled - By default enabled (True)</p></li>
<li><p>fetch-top - Fetch top ‘n’ highest mismatching outputs, Default 1</p></li>
<li><p>type - One of in-built comparators (abs, cos, topk, avg, l1norm, l2norm). Default avg</p></li>
<li><p>tol - Tolerance value. Default 0.001</p></li>
</ul>
</dd>
</dl>
</td>
</tr>
</tbody>
</table>
<p>Command line options available for config mode are as follows:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">qnn</span><span class="o">-</span><span class="n">acc</span><span class="o">-</span><span class="n">evaluator</span><span class="w"> </span><span class="n">options</span>

<span class="n">required</span><span class="w"> </span><span class="n">options</span><span class="o">:</span>
<span class="w">    </span><span class="o">-</span><span class="n">config</span><span class="w"> </span><span class="n">CONFIG</span><span class="w">        </span><span class="n">path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="n">config</span><span class="w"> </span><span class="n">yaml</span>

<span class="w">    </span><span class="n">pipeline</span><span class="w"> </span><span class="n">options</span><span class="o">:</span>
<span class="w">    </span><span class="o">-</span><span class="n">preproc_file</span><span class="w"> </span><span class="n">PREPROC_FILE</span>
<span class="w">                            </span><span class="n">preprocessed</span><span class="w"> </span><span class="n">input</span><span class="w"> </span><span class="n">file</span><span class="p">,</span><span class="w"> </span><span class="n">overrides</span><span class="w"> </span><span class="n">inputfile</span><span class="w"> </span><span class="n">provided</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="n">config</span>
<span class="w">    </span><span class="o">-</span><span class="n">calib_file</span><span class="w"> </span><span class="n">CALIB_FILE</span>
<span class="w">                            </span><span class="n">calibration</span><span class="w"> </span><span class="n">input</span><span class="w"> </span><span class="n">file</span><span class="p">,</span><span class="w"> </span><span class="n">overrides</span><span class="w"> </span><span class="n">calibration</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">provided</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="n">config</span>

<span class="n">other</span><span class="w"> </span><span class="n">options</span><span class="o">:</span>
<span class="w">    </span><span class="o">-</span><span class="n">device_id</span><span class="w"> </span><span class="n">DEVICE_ID</span><span class="w">    </span><span class="n">Target</span><span class="w"> </span><span class="n">device</span><span class="w"> </span><span class="n">id</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">provided</span>
<span class="w">    </span><span class="o">-</span><span class="n">work_dir</span><span class="w"> </span><span class="n">WORK_DIR</span><span class="w">      </span><span class="n">working</span><span class="w"> </span><span class="n">directory</span><span class="w"> </span><span class="n">path</span><span class="p">.</span><span class="w"> </span><span class="k">default</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="p">.</span><span class="o">/</span><span class="n">qacc_temp</span>
<span class="w">    </span><span class="o">-</span><span class="n">silent</span><span class="w">                 </span><span class="n">Run</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">silent</span><span class="w"> </span><span class="n">mode</span>
<span class="w">    </span><span class="o">-</span><span class="n">debug</span><span class="w">                  </span><span class="n">Set</span><span class="w"> </span><span class="n">logging</span><span class="w"> </span><span class="n">level</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">DEBUG</span><span class="w"> </span><span class="n">within</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">tool</span>
<span class="w">    </span><span class="o">-</span><span class="n">inference_schema</span><span class="w"> </span><span class="n">INFERENCE_SCHEMA</span>
<span class="w">                            </span><span class="n">run</span><span class="w"> </span><span class="n">only</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">inference</span><span class="w"> </span><span class="n">schema</span>
<span class="w">    </span><span class="o">-</span><span class="n">inference_schema_tag</span><span class="w"> </span><span class="n">INFERENCE_SCHEMA_TAG</span>
<span class="w">                            </span><span class="n">run</span><span class="w"> </span><span class="n">only</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">inference</span><span class="w"> </span><span class="n">schema</span><span class="w"> </span><span class="n">tag</span>
<span class="w">    </span><span class="o">-</span><span class="n">batchsize</span><span class="w"> </span><span class="n">BATCHSIZE</span><span class="w">    </span><span class="n">overrides</span><span class="w"> </span><span class="n">batchsize</span><span class="w"> </span><span class="n">provided</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="n">config</span>
<span class="w">    </span><span class="o">-</span><span class="n">onnx_symbol</span><span class="w"> </span><span class="n">ONNX_SYMBOL</span><span class="w"> </span><span class="p">[</span><span class="n">ONNX_SYMBOL</span><span class="w"> </span><span class="p">...]</span>
<span class="w">                            </span><span class="n">Replace</span><span class="w"> </span><span class="n">onnx</span><span class="w"> </span><span class="n">symbols</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">input</span><span class="o">/</span><span class="n">output</span><span class="w"> </span><span class="n">shapes</span><span class="p">.</span><span class="n">Can</span><span class="w"> </span><span class="n">be</span>
<span class="w">                            </span><span class="n">passed</span><span class="w"> </span><span class="n">multiple</span><span class="w"> </span><span class="n">timesDefault</span><span class="w"> </span><span class="n">replaced</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="mf">1.</span><span class="w"> </span><span class="n">e</span><span class="p">.</span><span class="n">g</span>
<span class="w">                            </span><span class="nl">__unk_200</span><span class="p">:</span><span class="mi">1</span>
<span class="w">    </span><span class="o">-</span><span class="n">set_global</span><span class="w"> </span><span class="n">SET_GLOBAL</span><span class="w"> </span><span class="p">[</span><span class="n">SET_GLOBAL</span><span class="w"> </span><span class="p">...]</span>
<span class="w">                            </span><span class="n">Replace</span><span class="w"> </span><span class="n">global</span><span class="w"> </span><span class="n">symbols</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">given</span><span class="w"> </span><span class="n">value</span><span class="p">.</span><span class="n">Can</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">passed</span>
<span class="w">                            </span><span class="n">multiple</span><span class="w"> </span><span class="n">times</span><span class="p">.</span><span class="w"> </span><span class="n">e</span><span class="p">.</span><span class="n">g</span><span class="w"> </span><span class="o">&lt;</span><span class="n">symbol</span><span class="o">&gt;:</span><span class="mi">2</span>
<span class="w">    </span><span class="o">-</span><span class="n">use_memory_plugins</span><span class="w">     </span><span class="n">Flag</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">enable</span><span class="w"> </span><span class="n">memory</span><span class="w"> </span><span class="n">plugins</span><span class="p">.</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Users can accelerate their evaluations using memory plugins to minimize unnecessary reading and writing of data during evaluation by passing the <code class="docutils literal notranslate"><span class="pre">-use_memory_plugins</span></code> flag to the evaluator command.</p>
</div>
<p><strong>Sample Command</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">qnn</span><span class="o">-</span><span class="n">accuracy</span><span class="o">-</span><span class="n">evaluator</span><span class="w"> </span><span class="o">-</span><span class="n">config</span><span class="w"> </span><span class="p">{</span><span class="n">path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">configs</span><span class="p">}</span><span class="o">/</span><span class="n">qnn_resnet50_config</span><span class="p">.</span><span class="n">yaml</span>
</pre></div>
</div>
<p><strong>Results</strong></p>
<p>Refer to the minimal mode section</p>
<p><strong>Config file options</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="o">-</span><span class="w"> </span><span class="n">inference_schema</span><span class="o">:</span>
<span class="w">    </span><span class="nl">name</span><span class="p">:</span><span class="w"> </span><span class="n">qnn</span>
<span class="w">    </span><span class="nl">target_arch</span><span class="p">:</span><span class="w"> </span><span class="n">x86_64</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">clang</span>
<span class="w">    </span><span class="nl">backend</span><span class="p">:</span><span class="w"> </span><span class="n">cpu</span>
<span class="w">    </span><span class="nl">precision</span><span class="p">:</span><span class="w"> </span><span class="n">fp32</span>
<span class="w">    </span><span class="nl">tag</span><span class="p">:</span><span class="w"> </span><span class="n">qnn_cpu_x86</span>

<span class="o">-</span><span class="w"> </span><span class="n">inference_schema</span><span class="o">:</span>
<span class="w">    </span><span class="nl">name</span><span class="p">:</span><span class="w"> </span><span class="n">qnn</span>
<span class="w">    </span><span class="nl">target_arch</span><span class="p">:</span><span class="w"> </span><span class="n">aarch64</span><span class="o">-</span><span class="n">android</span>
<span class="w">    </span><span class="nl">backend</span><span class="p">:</span><span class="w"> </span><span class="n">cpu</span>
<span class="w">    </span><span class="nl">precision</span><span class="p">:</span><span class="w"> </span><span class="n">fp32</span>
<span class="w">    </span><span class="nl">tag</span><span class="p">:</span><span class="w"> </span><span class="n">qnn_cpu_android</span>

<span class="o">-</span><span class="w"> </span><span class="n">inference_schema</span><span class="o">:</span>
<span class="w">    </span><span class="nl">name</span><span class="p">:</span><span class="w"> </span><span class="n">qnn</span>
<span class="w">    </span><span class="nl">target_arch</span><span class="p">:</span><span class="w"> </span><span class="n">aarch64</span><span class="o">-</span><span class="n">android</span>
<span class="w">    </span><span class="nl">backend</span><span class="p">:</span><span class="w"> </span><span class="n">gpu</span>
<span class="w">    </span><span class="nl">precision</span><span class="p">:</span><span class="w"> </span><span class="n">fp32</span>
<span class="w">    </span><span class="nl">tag</span><span class="p">:</span><span class="w"> </span><span class="n">qnn_gpu_android</span>

<span class="o">-</span><span class="w"> </span><span class="n">inference_schema</span><span class="o">:</span>
<span class="w">    </span><span class="nl">name</span><span class="p">:</span><span class="w"> </span><span class="n">qnn</span>
<span class="w">    </span><span class="nl">target_arch</span><span class="p">:</span><span class="w"> </span><span class="n">x86_64</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">clang</span>
<span class="w">    </span><span class="nl">backend</span><span class="p">:</span><span class="w"> </span><span class="n">htp</span>
<span class="w">    </span><span class="nl">precision</span><span class="p">:</span><span class="w"> </span><span class="n">quant</span>
<span class="w">    </span><span class="nl">tag</span><span class="p">:</span><span class="w"> </span><span class="n">htp_int8</span>
<span class="w">    </span><span class="nl">backend_extensions</span><span class="p">:</span>
<span class="w">        </span><span class="nl">vtcm_mb</span><span class="p">:</span><span class="w"> </span><span class="mi">4</span>
<span class="w">        </span><span class="nl">rpc_control_latency</span><span class="p">:</span><span class="w"> </span><span class="mi">100</span>
<span class="w">    </span><span class="nl">converter_params</span><span class="p">:</span>
<span class="w">        </span><span class="nl">param_quantizer</span><span class="p">:</span><span class="w"> </span><span class="n">tf</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">symmetric</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">enhanced</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">adjusted</span>
<span class="w">        </span><span class="nl">act_quantizer</span><span class="p">:</span><span class="w"> </span><span class="n">tf</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">symmetric</span>
<span class="w">        </span><span class="nl">algorithms</span><span class="p">:</span><span class="w"> </span><span class="k">default</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">cle</span>
<span class="w">        </span><span class="nl">use_per_channel_quantization</span><span class="p">:</span><span class="w"> </span><span class="n">True</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">False</span>
<span class="w">        </span><span class="nl">use_per_row_quantization</span><span class="p">:</span><span class="w"> </span><span class="n">True</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">False</span>
<span class="w">        </span><span class="nl">quantization_overrides</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;path to the ext quant json&quot;</span>
<span class="w">        </span><span class="nl">act_bw</span><span class="p">:</span><span class="w"> </span><span class="mi">8</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="mi">16</span>
<span class="w">        </span><span class="nl">bias_bw</span><span class="p">:</span><span class="w"> </span><span class="mi">8</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="mi">32</span>
<span class="w">        </span><span class="nl">weight_bw</span><span class="p">:</span><span class="w"> </span><span class="mi">8</span>

<span class="o">-</span><span class="w"> </span><span class="n">inference_schema</span><span class="o">:</span>
<span class="w">    </span><span class="nl">name</span><span class="p">:</span><span class="w"> </span><span class="n">qnn</span>
<span class="w">    </span><span class="nl">target_arch</span><span class="p">:</span><span class="w"> </span><span class="n">aarch64</span><span class="o">-</span><span class="n">android</span>
<span class="w">    </span><span class="nl">backend</span><span class="p">:</span><span class="w"> </span><span class="n">dspv69</span><span class="o">/</span><span class="n">dspv73</span><span class="o">/</span><span class="n">dspv75</span>
<span class="w">    </span><span class="nl">precision</span><span class="p">:</span><span class="w"> </span><span class="n">quant</span>
<span class="w">    </span><span class="nl">tag</span><span class="p">:</span><span class="w"> </span><span class="n">htp_int8</span>
<span class="w">    </span><span class="nl">backend_extensions</span><span class="p">:</span>
<span class="w">        </span><span class="nl">vtcm_mb</span><span class="p">:</span><span class="w"> </span><span class="mi">4</span>
<span class="w">        </span><span class="nl">rpc_control_latency</span><span class="p">:</span><span class="w"> </span><span class="mi">100</span>
<span class="w">    </span><span class="nl">converter_params</span><span class="p">:</span>
<span class="w">        </span><span class="nl">param_quantizer</span><span class="p">:</span><span class="w"> </span><span class="n">tf</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">symmetric</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">enhanced</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">adjusted</span>
<span class="w">        </span><span class="nl">act_quantizer</span><span class="p">:</span><span class="w"> </span><span class="n">tf</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">symmetric</span>
<span class="w">        </span><span class="nl">algorithms</span><span class="p">:</span><span class="w"> </span><span class="k">default</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">cle</span>
<span class="w">        </span><span class="nl">use_per_channel_quantization</span><span class="p">:</span><span class="w"> </span><span class="n">True</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">False</span>
<span class="w">        </span><span class="nl">use_per_row_quantization</span><span class="p">:</span><span class="w"> </span><span class="n">True</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">False</span>
<span class="w">        </span><span class="nl">quantization_overrides</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;path to the ext quant json&quot;</span>
<span class="w">        </span><span class="nl">act_bw</span><span class="p">:</span><span class="w"> </span><span class="mi">8</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="mi">16</span>
<span class="w">        </span><span class="nl">bias_bw</span><span class="p">:</span><span class="w"> </span><span class="mi">8</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="mi">32</span>
<span class="w">        </span><span class="nl">weight_bw</span><span class="p">:</span><span class="w"> </span><span class="mi">8</span>
</pre></div>
</div>
<p><strong>Plugins</strong></p>
<p>Plugins are Python classes used to implement different stages of the inference pipeline, such as dataset handling, preprocessing, postprocessing, and metrics logic.</p>
<p><strong>Dataset</strong> and <strong>pre-processing</strong> plugins perform transformations to the input before they are passed to inference.</p>
<p><strong>Post-processing</strong> plugins transform inference outputs.</p>
<p><strong>Metric</strong> plugins analyze inference outputs to assess their accuracy</p>
<p>Sample plugins are provided in the SDK at <cite>${QNN_SDK_ROOT}/lib/python/qti/aisw/accuracy_evaluator/plugins</cite>.</p>
<p>Users can implement their own plugins (custom plugins) to meet their specific requirements. To include custom plugins,
export the <cite>CUSTOM_PLUGIN_PATH</cite> environment variable pointing to the location of the custom plugin(s),
so that they are also included while registering the plugin(s).</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">export</span><span class="w"> </span><span class="n">CUSTOM_PLUGIN_PATH</span><span class="o">=/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">custom</span><span class="o">/</span><span class="n">plugins</span><span class="o">/</span><span class="n">directory</span>
</pre></div>
</div>
<p>In the model configuration file, plugins are defined as a transformation chain, as shown below:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="nl">transformations</span><span class="p">:</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="n">plugin</span><span class="o">:</span>
<span class="w">          </span><span class="nl">name</span><span class="p">:</span><span class="w"> </span><span class="n">resize</span>
<span class="w">          </span><span class="nl">params</span><span class="p">:</span>
<span class="w">              </span><span class="nl">dims</span><span class="p">:</span><span class="w"> </span><span class="mi">416</span><span class="p">,</span><span class="mi">416</span>
<span class="w">              </span><span class="nl">channel_order</span><span class="p">:</span><span class="w"> </span><span class="n">RGB</span>
<span class="w">              </span><span class="nl">type</span><span class="p">:</span><span class="w"> </span><span class="n">letterbox</span>

<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="n">plugin</span><span class="o">:</span>
<span class="w">          </span><span class="nl">name</span><span class="p">:</span><span class="w"> </span><span class="n">normalize</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="n">plugin</span><span class="o">:</span>
<span class="w">          </span><span class="nl">name</span><span class="p">:</span><span class="w"> </span><span class="n">convert_nchw</span>
</pre></div>
</div>
<p>Plugins required for dataset transformation are configured in the dataset section as shown below.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>dataset:
    name: ILSVRC2012
    path: &#39;/home/ml-datasets/imageNet/&#39;
    inputlist_file: inputlist.txt
    annotation_file: ground_truth.txt
    calibration:
        type: dataset
        file: calibration.txt
    transformations:
        - plugin:
              name: filter_dataset
              params:
                  random: False
                  max_inputs: -1
                  max_calib: -1
</pre></div>
</div>
<p>The preprocessing and postprocessing plugins that the user wishes to use are configured in the processing section as shown below:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="nl">processing</span><span class="p">:</span>
<span class="w">    </span><span class="nl">preprocessing</span><span class="p">:</span>
<span class="w">        </span><span class="nl">transformations</span><span class="p">:</span>
<span class="w">            </span><span class="o">-</span><span class="w"> </span><span class="n">plugin</span><span class="o">:</span>
<span class="w">                  </span><span class="nl">name</span><span class="p">:</span><span class="w"> </span><span class="n">resize</span>
<span class="w">                  </span><span class="nl">params</span><span class="p">:</span>
<span class="w">                      </span><span class="nl">dims</span><span class="p">:</span><span class="w"> </span><span class="mi">416</span><span class="p">,</span><span class="mi">416</span>
<span class="w">                      </span><span class="nl">channel_order</span><span class="p">:</span><span class="w"> </span><span class="n">RGB</span>
<span class="w">                      </span><span class="nl">type</span><span class="p">:</span><span class="w"> </span><span class="n">letterbox</span>

<span class="w">            </span><span class="o">-</span><span class="w"> </span><span class="n">plugin</span><span class="o">:</span>
<span class="w">                  </span><span class="nl">name</span><span class="p">:</span><span class="w"> </span><span class="n">normalize</span>

<span class="w">    </span><span class="nl">postprocessing</span><span class="p">:</span>
<span class="w">        </span><span class="nl">squash_results</span><span class="p">:</span><span class="w"> </span><span class="n">True</span>
<span class="w">        </span><span class="nl">transformations</span><span class="p">:</span>
<span class="w">            </span><span class="o">-</span><span class="w"> </span><span class="n">plugin</span><span class="o">:</span>
<span class="w">                  </span><span class="nl">name</span><span class="p">:</span><span class="w"> </span><span class="n">object_detection</span>
<span class="w">                  </span><span class="nl">params</span><span class="p">:</span>
<span class="w">                      </span><span class="nl">dims</span><span class="p">:</span><span class="w"> </span><span class="mi">416</span><span class="p">,</span><span class="mi">416</span>
<span class="w">                      </span><span class="nl">type</span><span class="p">:</span><span class="w"> </span><span class="n">letterbox</span>
<span class="w">                      </span><span class="nl">dtypes</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">float32</span><span class="p">,</span><span class="w"> </span><span class="n">float32</span><span class="p">,</span><span class="w"> </span><span class="n">float32</span><span class="p">,</span><span class="w"> </span><span class="n">float32</span><span class="p">]</span>
</pre></div>
</div>
<p>Metric calculation plugins are configured in the evaluator section as shown below.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="nl">evaluator</span><span class="p">:</span>
<span class="w">    </span><span class="nl">metrics</span><span class="p">:</span>
<span class="w">        </span><span class="o">-</span><span class="w"> </span><span class="n">plugin</span><span class="o">:</span>
<span class="w">              </span><span class="nl">name</span><span class="p">:</span><span class="w"> </span><span class="n">topk</span>
<span class="w">              </span><span class="nl">params</span><span class="p">:</span>
<span class="w">                  </span><span class="nl">kval</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span>
<span class="w">                  </span><span class="nl">softmax_index</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span>
<span class="w">                  </span><span class="nl">round</span><span class="p">:</span><span class="w"> </span><span class="mi">7</span>
<span class="w">                  </span><span class="nl">label_offset</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span>
</pre></div>
</div>
<p>Plugins that need to be executed for a pipeline stage are listed under ‘transformations’ (except metric plugins) and preceded by the ‘plugin’ keyword.
The following table lists details of each configurable entry for a plugin.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Field</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>name</p></td>
<td><p>Name of the plugin</p></td>
</tr>
<tr class="row-odd"><td><p>params</p></td>
<td><p>Parameters expected and required by the plugin</p></td>
</tr>
</tbody>
</table>
<p>A complete list of all plugins and their parameters can be found at <a class="reference internal" href="#accuracy-evaluator-plugins"><span class="std std-ref">Accuracy Evaluator Plugins</span></a></p>
<p><strong>Comparators</strong></p>
<p>Following are the comparators that can be used to compare the outputs. Some of the comparators output percentage match between
the two tensors and some output the absolute value, corresponding to the selected comparator.</p>
<ol class="arabic">
<li><p><strong>abs</strong> - Percentage match between the two tensors based on the relative tolerance threshold value</p></li>
<li><p><strong>cos</strong> - Percentage match between the two tensors based on the Cosine Similarity score</p></li>
<li><p><strong>topk</strong> - Percentage match between the two tensors based on the topk match between the two tensors</p></li>
<li><p><strong>avg</strong> - Percentage match between the two tensors based on the average difference between the two tensors</p></li>
<li><p><strong>l1norm</strong> - Percentage match between the two tensors based on the L1 Norm of the diff</p></li>
<li><p><strong>l2norm</strong> - Percentage match between the two tensors based on the L2 Norm of the diff</p></li>
<li><p><strong>std</strong> - Percentage match between the two tensors based on the standard deviation difference</p></li>
<li><p><strong>rme</strong> - Percentage match between the two tensors based on the RMSE between the tensors</p></li>
<li><p><strong>snr</strong> - Signal to Noise Ratio between the two tensors</p></li>
<li><p><strong>maxerror</strong> - max error value between the two tensors</p></li>
<li><p><strong>kld</strong> - KL Divergence value between the two tensors</p></li>
<li><p><strong>pixelbypixel</strong> - pixel by pixel plot difference between the two tensors. For each input i, plot is saved at <cite>{work_dir}/{schema}/Result_{i}</cite></p></li>
<li><p><strong>box</strong> - The box verifier requires the <cite>–box_input</cite> parameter which accepts the filename of a json file with the following format:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s">&quot;box&quot;</span><span class="o">:</span><span class="s">&quot;Result_0/detection_boxes_0.raw&quot;</span><span class="p">,</span>
<span class="s">&quot;class&quot;</span><span class="o">:</span><span class="s">&quot;Result_0/detection_classes_0.raw&quot;</span><span class="p">,</span>
<span class="s">&quot;score&quot;</span><span class="o">:</span><span class="s">&quot;Result_0/detection_scores_0.raw&quot;</span><span class="p">}</span>
</pre></div>
</div>
</li>
</ol>
<p><strong>Directory of models</strong></p>
<p>In Minimal mode, User can also pass a directory of models. The directory structure is expected to be in the following format:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">model_dir</span><span class="o">/</span>
<span class="w">  </span><span class="o">|</span><span class="n">__model1</span><span class="o">/</span>
<span class="w">  </span><span class="o">|</span><span class="w">     </span><span class="o">|</span><span class="n">__Source_model</span><span class="o">/</span>
<span class="w">  </span><span class="o">|</span><span class="w">     </span><span class="o">|</span><span class="w">       </span><span class="o">|</span><span class="n">__model</span><span class="p">.</span><span class="n">onnx</span>
<span class="w">  </span><span class="o">|</span><span class="w">     </span><span class="o">|</span><span class="n">__inputs</span><span class="o">/</span>
<span class="w">  </span><span class="o">|</span><span class="w">     </span><span class="o">|</span><span class="w">       </span><span class="o">|</span><span class="n">__data</span><span class="o">/</span><span class="n">images</span><span class="p">.</span><span class="n">raw</span>
<span class="w">  </span><span class="o">|</span><span class="w">     </span><span class="o">|</span><span class="w">       </span><span class="o">|</span><span class="n">__input_list</span><span class="p">.</span><span class="n">txt</span>
<span class="w">  </span><span class="o">|</span><span class="n">__model2</span><span class="o">/</span>
<span class="w">  </span><span class="o">|</span><span class="w">     </span><span class="o">|</span><span class="n">__Source_model</span><span class="o">/</span>
<span class="w">  </span><span class="o">|</span><span class="w">     </span><span class="o">|</span><span class="w">       </span><span class="o">|</span><span class="n">__model</span><span class="p">.</span><span class="n">onnx</span>
<span class="w">  </span><span class="o">|</span><span class="w">     </span><span class="o">|</span><span class="n">__inputs</span><span class="o">/</span>
<span class="w">  </span><span class="o">|</span><span class="w">     </span><span class="o">|</span><span class="w">       </span><span class="o">|</span><span class="n">__data</span><span class="o">/</span><span class="n">images</span><span class="p">.</span><span class="n">raw</span>
<span class="w">  </span><span class="o">|</span><span class="w">     </span><span class="o">|</span><span class="w">       </span><span class="o">|</span><span class="n">__input_list</span><span class="p">.</span><span class="n">txt</span>
</pre></div>
</div>
<p>The output directory for each model will be available at <cite>{work_dir}/{model_dir}</cite></p>
</div>
</div>
<div class="section" id="qnn-architecture-checker-beta">
<h3>qnn-architecture-checker (<a class="reference internal" href="#qnn-ai-tools-beta-note"><span class="std std-ref">Beta</span></a>)<a class="headerlink" href="#qnn-architecture-checker-beta" title="Permalink to this heading">¶</a></h3>
<p>Architecture Checker is a tool made for models running with HTP backend, including quantized 8-bit,
quantized 16-bit and FP16 models. It outputs a list of issues in the model that keep the model from
getting better performance while running on the HTP backend. Architecture checker tool can be invoked
with the modifier feature which will apply the recommended modifications for these issues. This will
help in visualizing the changes that can be applied to the model to make it a better fit on the HTP
backend.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">X86</span><span class="o">-</span><span class="n">Linux</span><span class="o">/</span><span class="w"> </span><span class="n">WSL</span><span class="w"> </span><span class="n">Usage</span><span class="o">:</span>
<span class="n">$</span><span class="w"> </span><span class="n">qnn</span><span class="o">-</span><span class="n">architecture</span><span class="o">-</span><span class="n">checker</span><span class="w"> </span><span class="o">-</span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="n">path</span><span class="o">&gt;/</span><span class="n">model</span><span class="p">.</span><span class="n">json</span>
<span class="w">                         </span><span class="o">-</span><span class="n">b</span><span class="w"> </span><span class="o">&lt;</span><span class="n">optional_path</span><span class="o">&gt;/</span><span class="n">model</span><span class="p">.</span><span class="n">bin</span>
<span class="w">                         </span><span class="o">-</span><span class="n">o</span><span class="w"> </span><span class="o">&lt;</span><span class="n">optional_output_path</span><span class="o">&gt;</span>
<span class="w">                         </span><span class="o">-</span><span class="n">m</span><span class="w"> </span><span class="o">&lt;</span><span class="n">optional_modifier_argument</span><span class="o">&gt;</span>

<span class="n">X86</span><span class="o">-</span><span class="n">Windows</span><span class="o">/</span><span class="w"> </span><span class="n">Windows</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">Snapdragon</span><span class="w"> </span><span class="n">Usage</span><span class="o">:</span>
<span class="n">$</span><span class="w"> </span><span class="n">python</span><span class="w"> </span><span class="n">qnn</span><span class="o">-</span><span class="n">architecture</span><span class="o">-</span><span class="n">checker</span><span class="w"> </span><span class="o">-</span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="n">path</span><span class="o">&gt;/</span><span class="n">model</span><span class="p">.</span><span class="n">json</span>
<span class="w">                         </span><span class="o">-</span><span class="n">b</span><span class="w"> </span><span class="o">&lt;</span><span class="n">optional_path</span><span class="o">&gt;/</span><span class="n">model</span><span class="p">.</span><span class="n">bin</span>
<span class="w">                         </span><span class="o">-</span><span class="n">o</span><span class="w"> </span><span class="o">&lt;</span><span class="n">optional_output_path</span><span class="o">&gt;</span>
<span class="w">                         </span><span class="o">-</span><span class="n">m</span><span class="w"> </span><span class="o">&lt;</span><span class="n">optional_modifier_argument</span><span class="o">&gt;</span>

<span class="w"> </span><span class="n">required</span><span class="w"> </span><span class="n">arguments</span><span class="o">:</span>
<span class="w">     </span><span class="o">-</span><span class="n">i</span><span class="w"> </span><span class="n">INPUT_JSON</span><span class="p">,</span><span class="w"> </span><span class="o">--</span><span class="n">input_json</span><span class="w"> </span><span class="n">INPUT_JSON</span>
<span class="w">                             </span><span class="n">Path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">json</span><span class="w"> </span><span class="n">file</span>

<span class="w"> </span><span class="n">optional</span><span class="w"> </span><span class="n">arguments</span><span class="o">:</span>
<span class="w">     </span><span class="o">-</span><span class="n">b</span><span class="w"> </span><span class="n">BIN</span><span class="p">,</span><span class="w"> </span><span class="o">--</span><span class="n">bin</span><span class="w"> </span><span class="n">BIN</span>
<span class="w">                     </span><span class="n">Path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">bin</span><span class="w"> </span><span class="n">file</span>
<span class="w">     </span><span class="o">-</span><span class="n">o</span><span class="w"> </span><span class="n">OUTPUT_PATH</span><span class="p">,</span><span class="w"> </span><span class="o">--</span><span class="n">output_path</span><span class="w"> </span><span class="n">OUTPUT_PATH</span>
<span class="w">                     </span><span class="n">Path</span><span class="w"> </span><span class="n">where</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="n">csv</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">saved</span><span class="p">.</span><span class="w"> </span><span class="n">If</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">specified</span><span class="p">,</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="n">csv</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">written</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">same</span><span class="w"> </span><span class="n">path</span><span class="w"> </span><span class="n">as</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">input</span><span class="w"> </span><span class="n">file</span>
<span class="w">     </span><span class="o">-</span><span class="n">m</span><span class="w"> </span><span class="n">MODIFY</span><span class="p">,</span><span class="w"> </span><span class="o">--</span><span class="n">modify</span><span class="w"> </span><span class="n">MODIFY</span>
<span class="w">                     </span><span class="n">The</span><span class="w"> </span><span class="n">query</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">select</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">modifications</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">apply</span><span class="p">.</span>
<span class="w">                         </span><span class="o">--</span><span class="n">modify</span><span class="w"> </span><span class="n">or</span><span class="w"> </span><span class="o">--</span><span class="n">modify</span><span class="w"> </span><span class="n">show</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">To</span><span class="w"> </span><span class="n">see</span><span class="w"> </span><span class="n">all</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">possible</span><span class="w"> </span><span class="n">modifications</span><span class="p">.</span><span class="w"> </span><span class="n">Display</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">rule</span><span class="w"> </span><span class="n">names</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">details</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">modifications</span><span class="p">.</span>
<span class="w">                         </span><span class="o">--</span><span class="n">modify</span><span class="w"> </span><span class="n">all</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">To</span><span class="w"> </span><span class="n">apply</span><span class="w"> </span><span class="n">all</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">possible</span><span class="w"> </span><span class="n">modifications</span><span class="w"> </span><span class="n">found</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">model</span><span class="p">.</span>
<span class="w">                         </span><span class="o">--</span><span class="n">modify</span><span class="w"> </span><span class="n">apply</span><span class="o">=</span><span class="n">rule_name1</span><span class="p">,</span><span class="n">rule_name2</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">To</span><span class="w"> </span><span class="n">apply</span><span class="w"> </span><span class="n">modifications</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">specified</span><span class="w"> </span><span class="n">rule</span><span class="w"> </span><span class="n">names</span><span class="p">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">rules</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">comma</span><span class="w"> </span><span class="n">separated</span><span class="w"> </span><span class="n">without</span><span class="w"> </span><span class="n">spaces</span>
</pre></div>
</div>
<div class="line-block">
<div class="line">Note:</div>
<div class="line">If running on a quantized model, the quantized model generated with one input image is
good enough to satisfy the quantization requirement to have the tool run properly.</div>
<div class="line">QNN_SDK_ROOT environment variable must be configured before running the tool.</div>
</div>
<div class="line-block">
<div class="line">Deprecation Note:</div>
<div class="line">The option of enabling architecture checker by passing ‘–arch_checker’ in each converter listed
above will be deprecated. E.g: Running qnn-tflite-converter -i &lt;path&gt;/model.tflite -d
&lt;network_input_name&gt; &lt;dims&gt; -o &lt;optional_output_path&gt; -p &lt;optional_package_name&gt;
–arch_checker will be deprecated.</div>
<div class="line">To enable the Architecture checker, run the converter tool without passing the ‘–arch_checker’
argument, then run the qnn-architecture-checker command to see the architecture checker output.</div>
<div class="line">The usage of “–modify” is only supported with the qnn-architecture-checker command.</div>
</div>
<p>The output is a csv file and will be saved as &lt;optional_output_path&gt;/&lt;model_name&gt;_architecture_checker.csv.
An example output is shown below:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"></th>
<th class="head"><p>Graph/Node_name</p></th>
<th class="head"><p>Issue</p></th>
<th class="head"><p>Recommendation</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Input_tensor_name:[dims]</p></th>
<th class="head"><p>Output_tensor_name:[dims]</p></th>
<th class="head"><p>Parameters</p></th>
<th class="head"><p>Previous node</p></th>
<th class="head"><p>Next nodes</p></th>
<th class="head"><p>Modification</p></th>
<th class="head"><p>Modification_info</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>Graph</p></td>
<td><p>This model uses 16-bit activation data. 16-bit activation data
takes twice the amount of memory than 8-bit activation data does.</p></td>
<td><p>Try to use a smaller datatype to get better performance. E.g., 8-bit</p></td>
<td><p>N/A</p></td>
<td><p>N/A</p></td>
<td><p>N/A</p></td>
<td><p>N/A</p></td>
<td><p>N/A</p></td>
<td><p>N/A</p></td>
<td><p>N/A</p></td>
<td><p>N/A</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>Node_name_1</p></td>
<td><p>The number of channels in the input/output tensor of
this convolution node is low (smaller than 32).</p></td>
<td><p>Try increasing the number of channels in the input/output
tensor to 32 or greater to get better performance.</p></td>
<td><p>Conv2d</p></td>
<td><p>input_1:[1, 250, 250, 3], __param_1:[5, 5, 3, 32], convolution_0_bias:[32]</p></td>
<td><p>output_1:[1, 123, 123, 32]</p></td>
<td><p>{‘package’: ‘qti.aisw’, ‘type’: ‘Conv2d’, …}</p></td>
<td><p>[‘previous_node_name’]</p></td>
<td><p>[‘next_node_name1’, ‘next_node_name2’]</p></td>
<td><p>N/A</p></td>
<td><p>N/A</p></td>
</tr>
</tbody>
</table>
<div class="line-block">
<div class="line"><strong>How to read the example output csv?</strong></div>
<div class="line">Row 1: This is an issue on the graph, the graph is using 16-bit activation data, as said in the
recommendation, changing the activation from 16 bit to 8 bit gives better performance.</div>
<div class="line">Row 2: The issue is on the node with QNN node name as “Node_name_1”. This node has three inputs:
input_1, __param_1 and convolution_0_bias where the dimensions are [1, 250, 250, 3], [5, 5, 3, 32]
and [32] respectively. This node has one output with QNN tensor name output_1 and the dimension
of this tensor is [1, 123, 123, 32]. The type of this node is Conv2d. The previous/next node names
and the full set of additional node parameters available in the Parameters column that can be used
to locate the node inside the original model. The issue for this node is the channel of the input
tensor is low, as the channel is smaller than 32, would recommend to increase the channel to at
least 32 to get better performance on HTP backend. Currently the input dimension is [1, 250, 250, 3]
and ideally have that to be [1, x, x, 32]. The Modification and Modification_info columns provide
details about the modifications applied to the node. If the Architecture Checker is not invoked
with modifier or if there aren’t any modifications applicable, then these value will be N/A.</div>
</div>
<div class="line-block">
<div class="line"><strong>Is the QNN node/tensor name the same in the original model?</strong></div>
<div class="line">It is not the same but should be similar. There is naming sanitization in converter in order
to meet the QNN naming standard. The input tensor, output tensor, previous node, next node
and all the additional parameters are avaliable in the output csv file to help locate the
correct node inside the original model.</div>
</div>
<p><strong>Sample Command</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">qnn</span><span class="o">-</span><span class="n">architecture</span><span class="o">-</span><span class="n">checker</span><span class="w"> </span><span class="o">--</span><span class="n">input_json</span><span class="w"> </span><span class="p">.</span><span class="o">/</span><span class="n">model_net</span><span class="p">.</span><span class="n">json</span>
<span class="w">                       </span><span class="o">--</span><span class="n">bin</span><span class="w"> </span><span class="p">.</span><span class="o">/</span><span class="n">model</span><span class="p">.</span><span class="n">bin</span>
<span class="w">                       </span><span class="o">--</span><span class="n">output_path</span><span class="w"> </span><span class="p">.</span><span class="o">/</span><span class="n">archCheckerOutput</span>
</pre></div>
</div>
<p><strong>Architecture Checker - Model Modifier</strong></p>
<p>For appying modifications to the model, the Architecture Checker can be invoked with “–modify” or
“–modify show” which will display a list of possible modifications. In this case, the Architecture
Checker tool will only show the rule names and modification detail. It will run without making any
changes to the model and generate the csv output. Using the rule names from the above run, the
Architecture Checker can be invoked with “–modify all” or “–modify apply=rule_name1,rule_name2”.
In this case, the rule specific changes will be applied to the model and the changes can be viewed
in the updated model json. Additionally, the output csv will also contain information related to
the modifications.</p>
<p>Consider the below csv output generated after applying “–modify apply=elwisediv” modification
on an example model.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"></th>
<th class="head"><p>Graph/Node_name</p></th>
<th class="head"><p>Issue</p></th>
<th class="head"><p>Recommendation</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Input_tensor_name:[dims]</p></th>
<th class="head"><p>Output_tensor_name:[dims]</p></th>
<th class="head"><p>Parameters</p></th>
<th class="head"><p>Previous node</p></th>
<th class="head"><p>Next nodes</p></th>
<th class="head"><p>Modification</p></th>
<th class="head"><p>Modification_info</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>Node_name_1</p></td>
<td><p>ElementWiseDivide usually has poor performance compared to ElementWiseMultiply.</p></td>
<td><p>Try replacing ElementWiseDivide with ElementWiseMultiply using the reciprocal
value to get better performance.</p></td>
<td><p>Eltwise_Binary</p></td>
<td><p>input_1:[1, 52, 52, 6], input_2:[1]</p></td>
<td><p>output_1:[1, 52, 52, 6]</p></td>
<td><p>{‘package’: ‘qti.aisw’, ‘eltwise_type’: ‘ElementWiseDivide’, …}</p></td>
<td><p>[‘previous_node_name’]</p></td>
<td><p>[‘next_node_name1’, ‘next_node_name2’]</p></td>
<td><p>Done</p></td>
<td><p>ElementWiseDivide has been replaced by ElementWiseMultiply using the reciprocal value</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>Node_name_2</p></td>
<td><p>The number of channels in the input/output tensor of this convolution node is low
(smaller than 32).</p></td>
<td><p>Try increasing the number of channels in the input/output tensor to 32 or greater
to get better performance.</p></td>
<td><p>Conv2d</p></td>
<td><p>input_3:[1, 250, 250, 3], __param_1:[5, 5, 3, 32], convolution_1_bias:[32]</p></td>
<td><p>output_2:[1, 123, 123, 32]</p></td>
<td><p>{‘package’: ‘qti.aisw’, ‘type’: ‘Conv2d’, …}</p></td>
<td><p>[‘previous_node_name’]</p></td>
<td><p>[‘next_node_name1’, ‘next_node_name2’]</p></td>
<td><p>N/A</p></td>
<td><p>N/A</p></td>
</tr>
</tbody>
</table>
<div class="line-block">
<div class="line"><strong>How to read the example output csv?</strong></div>
<div class="line">Row 1: The issue on the node with QNN node name as “Node_name_1” is that it has element wise
divide which gives a poor performance as compared to elementwise multipy. After invoking
architecture checker with “–modify apply=elwisediv”, the modifications have been successfully
applied i.e. the element wise divide is replaced by element wise multiply with a reciprocal
value. This information is available in the Modification and Modification_info columns.</div>
<div class="line">Row 2: The issue on the node with QNN node name as “Node_name_2” is that the node has input
tensor with number of channels less than 32. Its recommended to increase the number of channels
to 32 or greater for better performance. For this issue, the modification through the tool is
not applicable hence the Modification and Modification_info columns are N/A.</div>
<div class="line">After modifying the model, the above run will generate updated model.cpp, model_net.json and/or
model.bin along with the csv output. Running the Architecture Checker on the updated model json
will no longer show the element wise divide issue on Node_Name_1.</div>
</div>
<p>Following are the commands to invoke Architecture Checker with Modifier to display list of modifications:</p>
<p><strong>Sample Command</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">qnn</span><span class="o">-</span><span class="n">architecture</span><span class="o">-</span><span class="n">checker</span><span class="w"> </span><span class="o">--</span><span class="n">input_json</span><span class="w"> </span><span class="p">.</span><span class="o">/</span><span class="n">model_net</span><span class="p">.</span><span class="n">json</span>
<span class="w">                       </span><span class="o">--</span><span class="n">bin</span><span class="w"> </span><span class="p">.</span><span class="o">/</span><span class="n">model</span><span class="p">.</span><span class="n">bin</span>
<span class="w">                       </span><span class="o">--</span><span class="n">output_path</span><span class="w"> </span><span class="p">.</span><span class="o">/</span><span class="n">archCheckerOutput</span>
<span class="w">                       </span><span class="o">--</span><span class="n">modify</span>
</pre></div>
</div>
<p><strong>Sample Command</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">qnn</span><span class="o">-</span><span class="n">architecture</span><span class="o">-</span><span class="n">checker</span><span class="w"> </span><span class="o">--</span><span class="n">input_json</span><span class="w"> </span><span class="p">.</span><span class="o">/</span><span class="n">model_net</span><span class="p">.</span><span class="n">json</span>
<span class="w">                       </span><span class="o">--</span><span class="n">bin</span><span class="w"> </span><span class="p">.</span><span class="o">/</span><span class="n">model</span><span class="p">.</span><span class="n">bin</span>
<span class="w">                       </span><span class="o">--</span><span class="n">output_path</span><span class="w"> </span><span class="p">.</span><span class="o">/</span><span class="n">archCheckerOutput</span>
<span class="w">                       </span><span class="o">--</span><span class="n">modify</span><span class="w"> </span><span class="n">show</span>
</pre></div>
</div>
<p>Following are the commands to apply the modifications either on all possible modifications or specific rules:</p>
<p><strong>Sample Command</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">qnn</span><span class="o">-</span><span class="n">architecture</span><span class="o">-</span><span class="n">checker</span><span class="w"> </span><span class="o">--</span><span class="n">input_json</span><span class="w"> </span><span class="p">.</span><span class="o">/</span><span class="n">model_net</span><span class="p">.</span><span class="n">json</span>
<span class="w">                       </span><span class="o">--</span><span class="n">bin</span><span class="w"> </span><span class="p">.</span><span class="o">/</span><span class="n">model</span><span class="p">.</span><span class="n">bin</span>
<span class="w">                       </span><span class="o">--</span><span class="n">output_path</span><span class="w"> </span><span class="p">.</span><span class="o">/</span><span class="n">archCheckerOutput</span>
<span class="w">                       </span><span class="o">--</span><span class="n">modify</span><span class="w"> </span><span class="n">all</span>
</pre></div>
</div>
<p><strong>Sample Command</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">qnn</span><span class="o">-</span><span class="n">architecture</span><span class="o">-</span><span class="n">checker</span><span class="w"> </span><span class="o">--</span><span class="n">input_json</span><span class="w"> </span><span class="p">.</span><span class="o">/</span><span class="n">model_net</span><span class="p">.</span><span class="n">json</span>
<span class="w">                       </span><span class="o">--</span><span class="n">bin</span><span class="w"> </span><span class="p">.</span><span class="o">/</span><span class="n">model</span><span class="p">.</span><span class="n">bin</span>
<span class="w">                       </span><span class="o">--</span><span class="n">output_path</span><span class="w"> </span><span class="p">.</span><span class="o">/</span><span class="n">archCheckerOutput</span>
<span class="w">                       </span><span class="o">--</span><span class="n">modify</span><span class="w"> </span><span class="n">apply</span><span class="o">=</span><span class="n">prelu</span><span class="p">,</span><span class="n">elwisediv</span>
</pre></div>
</div>
<div class="line-block">
<div class="line">Note:</div>
<div class="line">The Architecture Checker with modifier is an enchancement to help visualize the changes
that can be applied on the model to better fit it on the HTP. To see the actual performance
improvements, the model may require retraining/redesigning.</div>
</div>
</div>
<div class="section" id="qnn-accuracy-debugger-beta">
<h3>qnn-accuracy-debugger (<a class="reference internal" href="#qnn-ai-tools-beta-note"><span class="std std-ref">Beta</span></a>)<a class="headerlink" href="#qnn-accuracy-debugger-beta" title="Permalink to this heading">¶</a></h3>
<p><strong>Dependencies</strong></p>
<p>The Accuracy Debugger depends on the setup outlined in <a class="reference internal" href="setup.html"><span class="doc">Setup</span></a>.
In particular, the following are required:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Platform dependencies are need to be met as per <a class="reference internal" href="setup.html#linux-platform-dependency"><span class="std std-ref">Platform Dependencies</span></a></p></li>
<li><p>The desired ML frameworks need to be installed. Accuracy debugger is verified to work with the ML framework versions mentioned at <a class="reference internal" href="setup.html#environment-setup-linux"><span class="std std-ref">Environment Setup</span></a></p></li>
</ol>
</div></blockquote>
<p>The following environment variables are used inside this guide (User may change the following path depending on their needs):</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>RESOURCESPATH = {Path to the directory where all models and input files reside}</p></li>
<li><p>PROJECTREPOPATH = {Path to your accuracy debugger project directory}</p></li>
</ol>
</div></blockquote>
<p><strong>Supported models</strong></p>
<p>The qnn-accuracy-debugger currently supports ONNX, TFLite, and Tensorflow 1.x models. Pytorch models are supported only in oneshot-layerwise debugging algorithm of tool.</p>
<p><strong>Overview</strong></p>
<p>The <strong>accuracy-debugger</strong> tool finds inaccuracies in a neural-network at the layer
level. The tool compares the golden outputs produced by running a model
through a specific ML framework (ie. Tensorflow, Onnx, TFlite) with the results produced
by running the same model through Qualcomm’s QNN Inference Engine. The inference engine can be run on a variety of computing mediums including GPU, CPU and DSP.</p>
<p>The following features are available in Accuracy Debugger. Each feature can be run with its corresponding option; for example, <code class="docutils literal notranslate"><span class="pre">qnn-accuracy-debugger</span> <span class="pre">--{option}</span></code>.</p>
<blockquote>
<div><ol class="arabic simple">
<li><p><strong>qnn-accuracy-debugger -–framework_diagnosis</strong> This feature uses a ML framework e.g. tensorflow, tflite or onnx, to run the model to get intermediate outputs.</p></li>
<li><p><strong>qnn-accuracy-debugger –-inference_engine</strong> This feature uses the QNN engine to run a model to retrieve intermediate outputs.</p></li>
<li><p><strong>qnn-accuracy-debugger –-verification</strong> This feature compares the output generated by the framework diagnosis and inference engine features using verifiers such as CosineSimilarity, RtolAtol, etc.</p></li>
<li><p><strong>qnn-accuracy-debugger –compare_encodings</strong> This feature extracts encodings from a given QNN net JSON file, compares them with the given AIMET encodings, and outputs an Excel sheet highlighting mismatches.</p></li>
<li><p><strong>qnn-accuracy-debugger –tensor_inspection</strong> This feature compares given target outputs with reference outputs.</p></li>
</ol>
</div></blockquote>
<dl class="simple">
<dt>Tip:</dt><dd><ul class="simple">
<li><p>You can use –help after the bin commands to see what other options (required or optional) you can add.</p></li>
<li><p>If no option is provided, Accuracy Debugger runs framework_diagnosis, inference_engine, and verification sequentially.</p></li>
</ul>
</dd>
</dl>
<p>Below are the instructons for running the Accuracy Debugger:</p>
<div class="section" id="framework-diagnosis">
<h4>Framework Diagnosis<a class="headerlink" href="#framework-diagnosis" title="Permalink to this heading">¶</a></h4>
<p>The Framework Diagnosis feature is designed to run models with different machine learning frameworks
(e.g. Tensorflow, etc). A selected model is run with a specific ML framework.
Golden outputs are produced for future comparison with inference results from the Inference Engine step.</p>
</div>
<div class="section" id="id1">
<h4>Usage<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h4>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>usage: qnn-accuracy-debugger --framework_diagnosis [-h]
                                   -f FRAMEWORK [FRAMEWORK ...]
                                   -m MODEL_PATH
                                   -i INPUT_TENSOR [INPUT_TENSOR ...]
                                   -o OUTPUT_TENSOR
                                   [-w WORKING_DIR]
                                   [--output_dirname OUTPUT_DIRNAME]
                                   [-v]
                                   [--disable_graph_optimization]
                                   [--onnx_custom_op_lib ONNX_CUSTOM_OP_LIB]

Script to generate intermediate tensors from an ML Framework.

optional arguments:
     -h, --help            show this help message and exit

required arguments:
     -f FRAMEWORK [FRAMEWORK ...], --framework FRAMEWORK [FRAMEWORK ...]
                             Framework type and version, version is optional. Currently
                             supported frameworks are [&quot;tensorflow&quot;,&quot;onnx&quot;,&quot;tflite&quot;] case
                             insensitive but spelling sensitive
     -m MODEL_PATH, --model_path MODEL_PATH
                             Path to the model file(s).
     -i INPUT_TENSOR [INPUT_TENSOR ...], --input_tensor INPUT_TENSOR [INPUT_TENSOR ...]
                             The name, dimensions, raw data, and optionally data
                             type of the network input tensor(s) specifiedin the
                             format &quot;input_name&quot; comma-separated-dimensions path-
                             to-raw-file, for example: &quot;data&quot; 1,224,224,3 data.raw
                             float32. Note that the quotes should always be
                             included in order to handle special characters,
                             spaces, etc. For multiple inputs specify multiple
                             --input_tensor on the command line like:
                             --input_tensor &quot;data1&quot; 1,224,224,3 data1.raw
                             --input_tensor &quot;data2&quot; 1,50,100,3 data2.raw float32.
     -o OUTPUT_TENSOR, --output_tensor OUTPUT_TENSOR
                             Name of the graph&#39;s specified output tensor(s).

     optional arguments:
     -w WORKING_DIR, --working_dir WORKING_DIR
                             Working directory for the framework_diagnosis to store
                             temporary files. Creates a new directory if the
                             specified working directory does not exist
     --output_dirname OUTPUT_DIRNAME
                             output directory name for the framework_diagnosis to
                             store temporary files under
                             &lt;working_dir&gt;/framework_diagnosis. Creates a new
                             directory if the specified working directory does not
                             exist
     -v, --verbose           Verbose printing
     --disable_graph_optimization
                             Disables basic model optimization
     --onnx_custom_op_lib ONNX_CUSTOM_OP_LIB
                             path to onnx custom operator library

Please note: All the command line arguments should either be provided through command line or through the config file. They will not override those in the config file if there is overlap.
</pre></div>
</div>
<p><strong>Sample Commands</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">qnn</span><span class="o">-</span><span class="n">accuracy</span><span class="o">-</span><span class="n">debugger</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">framework_diagnosis</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">framework</span><span class="w"> </span><span class="n">tensorflow</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">model_path</span><span class="w"> </span><span class="n">$RESOURCESPATH</span><span class="o">/</span><span class="n">samples</span><span class="o">/</span><span class="n">InceptionV3Model</span><span class="o">/</span><span class="n">inception_v3_2016_08_28_frozen</span><span class="p">.</span><span class="n">pb</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">input_tensor</span><span class="w"> </span><span class="s">&quot;input:0&quot;</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="mi">299</span><span class="p">,</span><span class="mi">299</span><span class="p">,</span><span class="mi">3</span><span class="w"> </span><span class="n">$RESOURCESPATH</span><span class="o">/</span><span class="n">samples</span><span class="o">/</span><span class="n">InceptionV3Model</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">chairs</span><span class="p">.</span><span class="n">raw</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">output_tensor</span><span class="w"> </span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">Predictions</span><span class="o">/</span><span class="n">Reshape_1</span><span class="o">:</span><span class="mi">0</span>

<span class="n">qnn</span><span class="o">-</span><span class="n">accuracy</span><span class="o">-</span><span class="n">debugger</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">framework_diagnosis</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">framework</span><span class="w"> </span><span class="n">onnx</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">model_path</span><span class="w"> </span><span class="n">$RESOURCESPATH</span><span class="o">/</span><span class="n">samples</span><span class="o">/</span><span class="n">dlv3onnx</span><span class="o">/</span><span class="n">dlv3plus_mbnet_513</span><span class="mi">-513</span><span class="n">_op9_mod_basic</span><span class="p">.</span><span class="n">onnx</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">input_tensor</span><span class="w"> </span><span class="n">Input</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">513</span><span class="p">,</span><span class="mi">513</span><span class="w"> </span><span class="n">$RESOURCESPATH</span><span class="o">/</span><span class="n">samples</span><span class="o">/</span><span class="n">dlv3onnx</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="mo">00000</span><span class="n">_1_3_513_513</span><span class="p">.</span><span class="n">raw</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">output_tensor</span><span class="w"> </span><span class="n">Output</span>

<span class="n">To</span><span class="w"> </span><span class="n">run</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">custom</span><span class="w"> </span><span class="n">operator</span><span class="o">:</span>
<span class="n">qnn</span><span class="o">-</span><span class="n">accuracy</span><span class="o">-</span><span class="n">debugger</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">framework_diagnosis</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">framework</span><span class="w"> </span><span class="n">onnx</span><span class="w"> </span>\
<span class="w">    </span><span class="o">-</span><span class="n">input_tensor</span><span class="w"> </span><span class="s">&quot;image&quot;</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">640</span><span class="p">,</span><span class="mi">640</span><span class="w"> </span><span class="n">$RESOURCESPATH</span><span class="o">/</span><span class="n">models</span><span class="o">/</span><span class="n">yolov3</span><span class="o">/</span><span class="n">batched</span><span class="o">-</span><span class="n">inp</span><span class="mi">-107</span><span class="mf">-0.</span><span class="n">raw</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">model_path</span><span class="w"> </span><span class="n">$RESOURCESPATH</span><span class="o">/</span><span class="n">models</span><span class="o">/</span><span class="n">yolov3</span><span class="o">/</span><span class="n">yolov3_640_640_with_abp_qnms</span><span class="p">.</span><span class="n">onnx</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">output_tensor</span><span class="w"> </span><span class="n">detection_boxes</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">onnx_custom_op_lib</span><span class="w"> </span><span class="n">$RESOURCESPATH</span><span class="o">/</span><span class="n">models</span><span class="o">/</span><span class="n">libCustomQnmsYoloOrt</span><span class="p">.</span><span class="n">so</span>
</pre></div>
</div>
<dl class="simple">
<dt>TIP:</dt><dd><ul class="simple">
<li><p>a working_directory, if not otherwise specified, is generated from wherever you are calling the script from; it is recommended to call all scripts from the same directory so all your outputs and results are stored under the same directory without having outputs everywhere</p></li>
<li><p>for tensorflow it is sometimes necessary to add the :0 after the input and output node name to signify the index of the node. Notice the :0 is dropped for onnx models.</p></li>
</ul>
</dd>
</dl>
<p><strong>Output</strong></p>
<p>The program also creates a directory named <em>latest</em> in <cite>working_directory/framework_diagnosis</cite> which is symbolically linked to the most recently generated directory. In the example below, <em>latest</em> will
have data that is symlinked to the data in the most recent directory <em>YYYY-MM-DD_HH:mm:ss</em>. Users may choose to override the directory name by passing it to –output_dirname (i.e. –output_dirname myTest1Ouput).</p>
<p>The <em>float data</em> produced by the <strong>Framework Diagnosis</strong> step offers precise reference material for the <strong>Verification</strong> component to diagnose the accuracy of the network generated by the <strong>Inference Engine</strong>.
Unless a path is otherwise specified, the Accuracy Debugger will create directories within the <cite>working_directory/framework_diagnosis</cite> directory found in the current working directory. The directories will be named with the date
and time of the program’s execution, and contain tensor data. Depending on the tensor naming convention of the model, there may be numerous sub-directories within the new
directory. This occurs when tensor names include a slash “/”. For example, for the tensor names ‘inception_3a/1x1/bn/sc’, ‘inception_3a/1x1/bn/sc_internal’ and ‘inception_3a/1x1/bn’, subdirectories will be generated.</p>
<div class="figure align-default">
<img alt="../_static/resources/framework_diagnosis.png" src="../_static/resources/framework_diagnosis.png" />
</div>
<p>The figure above shows a sample output from a framework_diagnosis run.
InceptionV3 and Logits contain the outputs of each layer before the last layer. Each output directory contains the .raw files corresponding to each node. Every raw file that can be seen is the output of an operation.
The outputs of the final layer are saved inside the Predictions directory. The file framework_diagnosis_options.json contains all the options used to run this feature.</p>
</div>
<div class="section" id="inference-engine">
<h4>Inference Engine<a class="headerlink" href="#inference-engine" title="Permalink to this heading">¶</a></h4>
<p>The Inference Engine feature is designed to find the outputs for a QNN model. The output produced by this step can be compared with the golden outputs produced by the framework diagnosis step.</p>
</div>
<div class="section" id="id2">
<h4>Usage<a class="headerlink" href="#id2" title="Permalink to this heading">¶</a></h4>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>usage: qnn-accuracy-debugger --inference_engine [-h]
                                   -p ENGINE_PATH
                                   -l INPUT_LIST
                                   -r {cpu,gpu,dsp,dspv65,dspv66,dspv68,dspv69,dspv73,htp}
                                   -a {aarch64-android,x86_64-linux-clang}
                                   [--stage {source,converted,compiled}]
                                   [-i INPUT_TENSOR [INPUT_TENSOR ...]]
                                   [-o OUTPUT_TENSOR] [-m MODEL_PATH]
                                   [-f FRAMEWORK [FRAMEWORK ...]]
                                   [-qmcpp QNN_MODEL_CPP_PATH]
                                   [-qmbin QNN_MODEL_BIN_PATH]
                                   [-qmb QNN_MODEL_BINARY_PATH]
                                   [--deviceId DEVICEID] [-v]
                                   [--host_device {x86}] [-w WORKING_DIR]
                                   [--output_dirname OUTPUT_DIRNAME]
                                   [--engine_version ENGINE_VERSION]
                                   [--debug_mode_off]
                                   [--print_version PRINT_VERSION]
                                   [--offline_prepare] [-bbw {8,32}]
                                   [-abw {8,16}]
                                   [--golden_dir_for_mapping GOLDEN_DIR_FOR_MAPPING]
                                   [-wbw {8}] [--lib_name LIB_NAME]
                                   [-bd BINARIES_DIR] [-qmn MODEL_NAME]
                                   [-pq {tf,enhanced,adjusted,symmetric}]
                                   [-qo QUANTIZATION_OVERRIDES]
                                   [--act_quantizer {tf,enhanced,adjusted,symmetric}]
                                   [--algorithms ALGORITHMS]
                                   [--ignore_encodings]
                                   [--per_channel_quantization]
                                   [-idt {float,native}]
                                   [-odt {float_only,native_only,float_and_native}]
                                   [--profiling_level {basic,detailed}]
                                   [--perf_profile {low_balanced,balanced,high_performance,sustained_high_performance,burst,low_power_saver,power_saver,high_power_saver,extreme_power_saver,system_settings}]
                                   [--log_level {error,warn,info,debug,verbose}]
                                   [--qnn_model_net_json QNN_MODEL_NET_JSON]
                                   [--qnn_netrun_config_file QNN_NETRUN_CONFIG_FILE]
                                   [--extra_converter_args EXTRA_CONVERTER_ARGS]
                                   [--extra_runtime_args EXTRA_RUNTIME_ARGS]
                                   [--compiler_config COMPILER_CONFIG]
                                   [--precision {int8,fp16}]
                                   [--add_layer_outputs ADD_LAYER_OUTPUTS]
                                   [--add_layer_types ADD_LAYER_TYPES]
                                   [--skip_layer_types SKIP_LAYER_TYPES]
                                   [--skip_layer_outputs SKIP_LAYER_OUTPUTS]



Script to run QNN inference engine.

optional arguments:
     -h, --help            show this help message and exit

Core Arguments:
     --stage {source,converted,compiled}
                             Specifies the starting stage in the Accuracy Debugger
                             pipeline.
                             Source: starting with source framework model [default].
                             Converted: starting with model.cpp and .bin files.
                             Compiled: starting with a model&#39;s .so binary.
     -r {cpu,gpu,dsp,dspv65,dspv66,dspv68,dspv69,dspv73,htp}, --runtime {cpu,gpu,dsp,dspv65,dspv66,dspv68,dspv69,dspv73,htp}
                             Runtime to be used.
                             Use HTP runtime for emulation on x86 host.
     -a {aarch64-android,x86_64-linux-clang}, --architecture {aarch64-android,x86_64-linux-clang}
                             Name of the architecture to use for inference engine.
     -l INPUT_LIST, --input_list INPUT_LIST
                             Path to the input list text.

Arguments required for SOURCE stage:
     -i INPUT_TENSOR [INPUT_TENSOR ...], --input_tensor INPUT_TENSOR [INPUT_TENSOR ...]
                             The name, dimension, and raw data of the network input
                             tensor(s) specified in the format &quot;input_name&quot; comma-
                             separated-dimensions path-to-raw-file, for example:
                             &quot;data&quot; 1,224,224,3 data.raw. Note that the quotes
                             should always be included in order to handle special
                             characters, spaces, etc. For multiple inputs specify
                             multiple --input_tensor on the command line like:
                             --input_tensor &quot;data1&quot; 1,224,224,3 data1.raw
                             --input_tensor &quot;data2&quot; 1,50,100,3 data2.raw.
     -o OUTPUT_TENSOR, --output_tensor OUTPUT_TENSOR
                             Name of the graph&#39;s output tensor(s).
     -m MODEL_PATH, --model_path MODEL_PATH
                             Path to the model file(s).
     -f FRAMEWORK [FRAMEWORK ...], --framework FRAMEWORK [FRAMEWORK ...]
                             Framework type to be used, followed optionally by
                             framework version.

Arguments required for CONVERTED stage:
     -qmcpp QNN_MODEL_CPP_PATH, --qnn_model_cpp_path QNN_MODEL_CPP_PATH
                             Path to the qnn model .cpp file
     -qmbin QNN_MODEL_BIN_PATH, --qnn_model_bin_path QNN_MODEL_BIN_PATH
                             Path to the qnn model .bin file

Arguments required for COMPILED stage:
     -qmb QNN_MODEL_BINARY_PATH, --qnn_model_binary_path QNN_MODEL_BINARY_PATH
                             Path to the qnn model .so binary.

Optional Arguments:
     --deviceId DEVICEID   The serial number of the device to use. If not
                             available, the first in a list of queried devices will
                             be used for validation.
     -v, --verbose         Verbose printing
     --host_device {x86}   The device that will be running conversion. Set to x86
                             by default.
     -w WORKING_DIR, --working_dir WORKING_DIR
                             Working directory for the inference_engine to store
                             temporary files. Creates a new directory if the
                             specified working directory does not exist
     --output_dirname OUTPUT_DIRNAME
                             output directory name for the inference_engine to
                             store temporary files under
                             &lt;working_dir&gt;/inference_engine .Creates a new
                             directory if the specified working directory does not
                             exist
     -p ENGINE_PATH, --engine_path ENGINE_PATH
                             Path to the inference engine.
     --debug_mode_off      Specifies if wish to turn off debug_mode mode.
     --print_version PRINT_VERSION
                             Print the QNN SDK version alongside the output.
     --offline_prepare     Use offline prepare to run qnn model.
     -bbw {8,32}, --bias_bitwidth {8,32}
                             option to select the bitwidth to use when quantizing
                             the bias. default 8
     -abw {8,16}, --act_bitwidth {8,16}
                             option to select the bitwidth to use when quantizing
                             the activations. default 8
     --golden_output_reference_directory GOLDEN_OUTPUT_REFERENCE_DIRECTORY, --golden_dir_for_mapping GOLDEN_DIR_FOR_MAPPING
                             Optional parameter to indicate the directory of the
                             goldens, it&#39;s used for tensor mapping without
                             framework.
     -wbw {8}, --weights_bitwidth {8}
                             option to select the bitwidth to use when quantizing
                             the weights. Only support 8 atm
     --float_bitwidth {32,16}
                             option to select the bitwidth to use when using float
                             for parameters(weights/bias) and activations for all
                             ops or specific Op (via encodings) selected through
                             encoding; either 32 (default) or 16
     -nif, --use_native_input_files
                             Specifies that the input files will be parsed in the
                             data type native to the graph. If not specified, input
                             files will be parsed in floating point.
     -nof, --use_native_output_files
                             Specifies that the output files will be generated in
                             the data type native to the graph. If not specified,
                             output files will be generated in floating point.
     --lib_name LIB_NAME   Name to use for model library (.so file)
     -bd BINARIES_DIR, --binaries_dir BINARIES_DIR
                             Directory to which to save model binaries, if they
                             don&#39;t yet exist.
     -mn MODEL_NAME, --model_name MODEL_NAME
                             Name of the desired output qnn model
     -pq {tf,enhanced,adjusted,symmetric}, --param_quantizer {tf,enhanced,adjusted,symmetric}
                             Param quantizer algorithm used.
     -qo QUANTIZATION_OVERRIDES, --quantization_overrides QUANTIZATION_OVERRIDES
                             Path to quantization overrides json file.
     --act_quantizer {tf,enhanced,adjusted,symmetric}
                             Optional parameter to indicate the activation
                             quantizer to use
     -fbw {16,32}, --float_bias_bitwidth {16,32}
                             option to select the bitwidth to use when biases are in float; default is 32
     -rqs RESTRICT_QUANTIZATION_STEPS, --restrict_quantization_steps RESTRICT_QUANTIZATION_STEPS
                             ENCODING_MIN, ENCODING_MAX
                             Specifies the number of steps to use to compute quantization encodings such that
                             scale = (max - min) / number of quantization steps.
                             The option should be passed as a space separated pair of hexadecimal string minimum and maximum values,
                             i.e. --restrict_quantization_steps &#39;MIN MAX&#39;. Note that this is a hexadecimal string
                             literal and not a signed integer. To supply a negative value an explicit minus sign is required.
                             e.g.: 8-bit range: --restrict_quantization_steps &#39;-0x80 0x7F&#39;
                             16-bit range: --restrict_quantization_steps &#39;-0x8000 0x7F7F&#39;
     --algorithms ALGORITHMS
                             Use this option to enable new optimization algorithms.
                             Usage is: --algorithms &lt;algo_name1&gt; ... The available
                             optimization algorithms are: &#39;cle &#39; - Cross layer
                             equalization includes a number of methods for
                             equalizing weights and biases across layers in order
                             to rectify imbalances that cause quantization errors.
     --ignore_encodings    Use only quantizer generated encodings, ignoring any
                             user or model provided encodings.
     --per_channel_quantization
                             Use per-channel quantization for convolution-based op
                             weights.
     -idt {float,native}, --input_data_type {float,native}
                             the input data type, must match with the supplied
                             inputs
     -odt {float_only,native_only,float_and_native}, --output_data_type {float_only,native_only,float_and_native}
                             the desired output data type
     --profiling_level {basic,detailed,backend}
                             Enables profiling and sets its level.
     --perf_profile {low_balanced,balanced,high_performance,sustained_high_performance,burst,low_power_saver,power_saver,high_power_saver,extreme_power_saver,system_settings}
     --log_level {error,warn,info,debug,verbose}
                             Enable verbose logging.
     --qnn_model_net_json QNN_MODEL_NET_JSON
                             Path to the qnn model net json. Only necessary if its being run from the converted stage. It has information about what structure the data is in within framework_diagnosis and inference_engine steps.
                             This file is required to generate model_graph_struct.json file which is good to have in the verification step.
     --qnn_netrun_config_file QNN_NETRUN_CONFIG_FILE
                             allow backend_extention features to be applied during
                             qnn-net-run
     --extra_converter_args EXTRA_CONVERTER_ARGS
                             additional convereter arguments in a string. example:
                             --extra_converter_args input_dtype=data
                             float;input_layout=data1 NCHW
     --extra_contextbin_args EXTRA_CONTEXTBIN_ARGS
                             additional context binary generator arguments in a quoted string.
                             example: --extra_contextbin_args &#39;arg1=value1;arg2=value2&#39;
     --extra_runtime_args EXTRA_RUNTIME_ARGS
                             additional convereter arguments in a quoted string.
                             example: --extra_runtime_args
                             profiling_level=basic;log_level=debug
     --compiler_config COMPILER_CONFIG
                             Path to the compiler config file.
     --precision {int8,fp16}
                             select precision
     --add_layer_outputs ADD_LAYER_OUTPUTS
                             Output layers to be dumped, e.g., 1579,232
     --add_layer_types ADD_LAYER_TYPES
                             Outputs of layer types to be dumped, e.g., Resize, Transpose; all enabled by default
     --skip_layer_types SKIP_LAYER_TYPES
                             Comma delimited layer types to skip snooping, e.g., Resize, Transpose
     --skip_layer_outputs SKIP_LAYER_OUTPUTS
                             Comma delimited layer output names to skip debugging, e.g., 1171, 1174



Please note: All the command line arguments should either be provided through command line or through the config file. They will not override those in the config file if there is overlap.
</pre></div>
</div>
<p>The inference engine config file can be found in <cite>{accuracy_debugger tool root directory}/python/qti/aisw/accuracy_debugger/lib/inference_engine/configs/config_files</cite> and is a <strong>JSON</strong> file. This config file
stores information that helps the inference engine determine which tool and parameters to read in.</p>
<p><strong>Sample Command</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">qnn</span><span class="o">-</span><span class="n">accuracy</span><span class="o">-</span><span class="n">debugger</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">inference_engine</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">framework</span><span class="w"> </span><span class="n">tensorflow</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">runtime</span><span class="w"> </span><span class="n">dspv73</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">model_path</span><span class="w"> </span><span class="n">$RESOURCESPATH</span><span class="o">/</span><span class="n">samples</span><span class="o">/</span><span class="n">InceptionV3Model</span><span class="o">/</span><span class="n">inception_v3_2016_08_28_frozen</span><span class="p">.</span><span class="n">pb</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">input_tensor</span><span class="w"> </span><span class="s">&quot;input:0&quot;</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="mi">299</span><span class="p">,</span><span class="mi">299</span><span class="p">,</span><span class="mi">3</span><span class="w"> </span><span class="n">$RESOURCESPATH</span><span class="o">/</span><span class="n">samples</span><span class="o">/</span><span class="n">InceptionV3Model</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">chairs</span><span class="p">.</span><span class="n">raw</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">output_tensor</span><span class="w"> </span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">Predictions</span><span class="o">/</span><span class="n">Reshape_1</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">architecture</span><span class="w"> </span><span class="n">x86_64</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">clang</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">input_list</span><span class="w"> </span><span class="n">$RESOURCESPATH</span><span class="o">/</span><span class="n">samples</span><span class="o">/</span><span class="n">InceptionV3Model</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">image_list</span><span class="p">.</span><span class="n">txt</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">verbose</span>
</pre></div>
</div>
<p><strong>Sample Command</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">qnn</span><span class="o">-</span><span class="n">accuracy</span><span class="o">-</span><span class="n">debugger</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">inference_engine</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">deviceId</span><span class="w"> </span><span class="mi">357415</span><span class="n">c4</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">framework</span><span class="w"> </span><span class="n">tensorflow</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">runtime</span><span class="w"> </span><span class="n">dspv73</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">architecture</span><span class="w"> </span><span class="n">aarch64</span><span class="o">-</span><span class="n">android</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">model_path</span><span class="w"> </span><span class="n">$RESOURCESPATH</span><span class="o">/</span><span class="n">samples</span><span class="o">/</span><span class="n">InceptionV3Model</span><span class="o">/</span><span class="n">inception_v3_2016_08_28_frozen</span><span class="p">.</span><span class="n">pb</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">input_tensor</span><span class="w"> </span><span class="s">&quot;input:0&quot;</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="mi">299</span><span class="p">,</span><span class="mi">299</span><span class="p">,</span><span class="mi">3</span><span class="w"> </span><span class="n">$RESOURCESPATH</span><span class="o">/</span><span class="n">samples</span><span class="o">/</span><span class="n">InceptionV3Model</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">chairs</span><span class="p">.</span><span class="n">raw</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">output_tensor</span><span class="w"> </span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">Predictions</span><span class="o">/</span><span class="n">Reshape_1</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">input_list</span><span class="w"> </span><span class="n">$RESOURCESPATH</span><span class="o">/</span><span class="n">samples</span><span class="o">/</span><span class="n">InceptionV3Model</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">image_list</span><span class="p">.</span><span class="n">txt</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">verbose</span>
</pre></div>
</div>
<dl class="simple">
<dt>Tip:</dt><dd><ul class="simple">
<li><p>for runtime (choose from ‘cpu’, ‘gpu’, ‘dsp’, ‘dspv65’, ‘dspv66’, ‘dspv68’, ‘dspv69’, ‘dspv73’, ‘htp’). Make sure the runtime is 73 for kailua, 69 for waipio, etc. Choose HTP runtime for emulation on x86 host.</p></li>
<li><p>the input_tensor (–i) and output_tensor (-o) does not need the :0 indexing like when runing tensorflow framework diagnosis</p></li>
<li><p>two files, namely tensor_mapping.json and qnn_model_graph_struct.json are generated to be used in verification, be sure to locate these 2 files in the working_directory/inference_engine/latest</p></li>
</ul>
</dd>
</dl>
<p>More example commands running from different stages:</p>
<p><strong>Sample Command</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>source file stage: same as example from above section (stage default is &quot;source&quot;)

running from converted stage (x86):
qnn-accuracy-debugger \
    --inference_engine \
    --stage converted \
    -qmcpp $RESOURCESPATH/samples/InceptionV3Model/inception_v3_2016_08_28_frozen_qnn_model.cpp \
    -qmbin $RESOURCESPATH/samples/InceptionV3Model/inception_v3_2016_08_28_frozen_qnn_model.bin \
    --runtime dspv73 \
    --architecture x86_64-linux-clang \
    --input_list $RESOURCESPATH/samples/InceptionV3Model/data/image_list.txt \
    --qnn_model_net_json $RESOURCESPATH/samples/InceptionV3Model/inception_v3_2016_08_28_frozen_qnn_model_net.json \
    --verbose \
    --framework tensorflow \
    --golden_output_reference_directory $RESOURCESPATH/samples/InceptionV3Model/golden_from_framework_diagnosis/

Android Devices (ie. MTP):
qnn-accuracy-debugger \
    --inference_engine \
    --stage converted \
    -qmcpp $RESOURCESPATH/samples/InceptionV3Model/inception_v3_2016_08_28_frozen_qnn_model.cpp \
    -qmbin $RESOURCESPATH/samples/InceptionV3Model/inception_v3_2016_08_28_frozen_qnn_model.bin \
    --deviceId f366ce60 \
    --runtime dspv73 \
    --architecture aarch64-android \
    --input_list $RESOURCESPATH/samples/InceptionV3Model/data/image_list.txt \
    --qnn_model_net_json $RESOURCESPATH/samples/InceptionV3Model/inception_v3_2016_08_28_frozen_qnn_model_net.json \
    --verbose \
    --framework tensorflow \
    --golden_output_reference_directory $RESOURCESPATH/samples/InceptionV3Model/golden_from_framework_diagnosis/


running in compiled stage (x86):

qnn-accuracy-debugger \
    --inference_engine \
    --stage compiled \
    --qnn_model_binary $RESOURCESPATH/samples/InceptionV3Model/qnn_model_binaries/x86_64-linux-clang/libqnn_model.so \
    --runtime dspv73 \
    --architecture x86_64-linux-clang \
    --input_list $RESOURCESPATH/samples/InceptionV3Model/data/image_list.txt \
    --verbose \
    --qnn_model_net_json $RESOURCESPATH/samples/InceptionV3Model/inception_v3_2016_08_28_frozen_qnn_model_net.json \
    --golden_output_reference_directory $RESOURCESPATH/samples/InceptionV3Model/golden_from_framework_diagnosis/

Android devices (ie MTP):
qnn-accuracy-debugger \
    --inference_engine \
    --stage compiled \
    --qnn_model_binary $RESOURCESPATH/samples/InceptionV3Model/qnn_model_binaries/aarch64-android/libqnn_model.so \
    --runtime dspv73 \
    --architecture aarch64-android \
    --input_list $RESOURCESPATH/samples/InceptionV3Model/data/image_list.txt \
    --verbose \
    --qnn_model_net_json $RESOURCESPATH/samples/InceptionV3Model/inception_v3_2016_08_28_frozen_qnn_model_net.json \
    --framework tensorflow \
    --golden_output_reference_directory $RESOURCESPATH/samples/InceptionV3Model/golden_from_framework_diagnosis/

To run onnx model with custom operator:
qnn-accuracy-debugger \
    --inference_engine \
    --framework onnx \
    --runtime dspv75
    --architecture aarch64_android \
    --model_path $RESOURCESPATH/AISW-77095/model.onnx \
    --input_tensor &quot;image&quot; 1,3,640,1794 $RESOURCESPATH/inputs/image.raw \
    --output_tensor uncertainty_jacobian_bb \
    --input_list $RESOURCESPATH/input_list.txt \
    --default_verifier mse \
    --engine QNN \
    --engine_path $QNN_SDK_ROOT \
    --extra_converter_args &#39;op_package_config=$RESOURCESPATH/CustomPreTopKOpPackageCPU_v2.xml;op_package_lib=$RESOURCESPATH/libCustomPreTopKOpPackageHtp.so:CustomPreTopKOpPackageHtpInterfaceProvider:&#39; \
    --extra_contextbin_args &#39;op_packages=$RESOURCESPATH/libQnnCustomPreTopKOpPackageHtp.so:CustomPreTopKOpPackageHtpInterfaceProvider:&#39; \
    --extra_runtime_args &#39;op_packages=$RESOURCESPATH/AISW-77095/libQnnCustomPreTopKOpPackageHtp_v75.so:CustomPreTopKOpPackageHtpInterfaceProvider&#39; \
    --debug_mode_off \
    --offline_prepare \
    --verbose
</pre></div>
</div>
<dl class="simple">
<dt>Tip:</dt><dd><ul class="simple">
<li><p>The qnn_model_net_json file is not required to run this step. However, it is needed to build the qnn_model_graph_struct.json, which can be used in the Verification step. The model_net.json file is generated when the original model is converted into a converted model. Hence if you are debugging this model from the converted model stage, it is recommended to ask for this model_net.json file.</p></li>
<li><p>framework and golden_dir_for_mapping, or just golden_dir_for_mapping itself is an alternative to the original model to be provided to generate the tensor_mapping.json. However, providing only the golden_dir_for_mapping, the get_tensor_mapping module will try its best to map but it is not guaranteed this mapping will be 100% accurate.</p></li>
</ul>
</dd>
</dl>
<p><strong>Output</strong></p>
<p>Once the inference engine has finished running, it will store the output in the specified directory
(or the current working directory by default) and store the files in that directory. By default, it
will store the output in <cite>working_directory/inference_engine</cite> in the current working directory.</p>
<div class="figure align-default">
<img alt="../_static/resources/inference_engine.png" src="../_static/resources/inference_engine.png" />
</div>
<p>The figure above shows the sample output from one of the runs of inference engine step. The output directory contains raw files. Each raw file is an output of an operation in the network. The model.bin and model.cpp files are created
by the model converter. The qnn_model_binaries directory contains the .so file that is generated by the modellibgenerator utility. The file image_list.txt contains the path for sample test images. The inference_engine_options.json file contains all the options with which this run was launched.
In addition to generating the .raw files, the inference_engine also generates the model’s graph structure in a .json file. The name of the file is the same as the name of the protobuf model file.
The model_graph_struct.json aids in providing structure related information of the converted model graph during the verification step. Specifically, it helps with organizing the nodes in order (for i.e. the beginning nodes should come earlier than ending nodes).
The model_net.json has information about what structure the data is in within the framework_diagnosis and inference_engine steps (data can be in different formats for e.g. channels first vs channels last).
The verification step uses this information so that data can be properly transposed and compared. It is an optional parameter which can be provided during inference engine step for generating the model_graph_struct.json file (mandated only when running inference engine from the converted stage).
Finally, the tensor_mapping file contains a mapping of the various intermediate output file names generated from the framework diagnosis step and the inference engine step.</p>
<div class="figure align-default">
<img alt="../_static/resources/inference_engine_2.png" src="../_static/resources/inference_engine_2.png" />
</div>
<p>The created .raw files are organized in the same manner as framework_diagnosis (see above).</p>
</div>
<div class="section" id="verification">
<h4>Verification<a class="headerlink" href="#verification" title="Permalink to this heading">¶</a></h4>
<p>The Verification step compares the output (from the intermediate tensors of a given model) produced by the framework diagnosis step with the output produced by the inference engine step.
Once the comparison is complete, the verification results are compiled and displayed visually in a format that can be easily interpreted by the user.</p>
<p>There are different types of verifiers for e.g.: CosineSimilarity, RtolAtol, etc. To see available verifiers please use the –help option (qnn-accuracy-debugger –verification –help).
Each verifier compares the Framework Diagnosis and Inference Engine output using an error metric. It also prepares reports and/or visualizations to help the user analyze the network’s error data.</p>
</div>
<div class="section" id="id3">
<h4>Usage<a class="headerlink" href="#id3" title="Permalink to this heading">¶</a></h4>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>usage: qnn-accuracy-debugger --verification [-h]

Script to run verification.

required arguments:
     --default_verifier DEFAULT_VERIFIER [DEFAULT_VERIFIER ...]
                             Default verifier used for verification. The options
                             &quot;RtolAtol&quot;, &quot;AdjustedRtolAtol&quot;, &quot;TopK&quot;, &quot;L1Error&quot;,
                             &quot;CosineSimilarity&quot;, &quot;MSE&quot;, &quot;MAE&quot;, &quot;SQNR&quot;, &quot;MeanIOU&quot;,
                             &quot;ScaledDiff&quot; are supported. An optional list of
                             hyperparameters can be appended. For example:
                             --default_verifier
                             rtolatol,rtolmargin,0.01,atolmargin,0,01. An optional
                             list of placeholders can be appended. For example:
                             --default_verifier CosineSimilarity param1 1 param2 2.
                             to use multiple verifiers, add additional
                             --default_verifier CosineSimilarity
     --golden_output_reference_directory GOLDEN_OUTPUT_REFERENCE_DIRECTORY, --framework_results FRAMEWORK_RESULTS
                             Path to root directory generated from framework
                             diagnosis. Paths may be absolute, or relative to the
                             working directory.
     --inference_results INFERENCE_RESULTS
                             Path to root directory generated from inference engine
                             diagnosis. Paths may be absolute, or relative to the
                             working directory.

optional arguments:
     --tensor_mapping TENSOR_MAPPING
                             Path to the file describing the tensor name mapping
                             between inference and golden tensors.can be generated
                             with in the inference engine step.
     --verifier_config VERIFIER_CONFIG
                             Path to the verifiers&#39; config file
     --graph_struct GRAPH_STRUCT
                             Path to the inference graph structure .json file. This file
                             aids in providing the structure related information of the converted model graph during this step.
     -v, --verbose         Verbose printing
     -w WORKING_DIR, --working_dir WORKING_DIR
                             Working directory for the verification to store
                             temporary files. Creates a new directory if the
                             specified working directory does not exist
     --output_dirname OUTPUT_DIRNAME
                             output directory name for the verification to store
                             temporary files under &lt;working_dir&gt;/verification.
                             Creates a new directory if the specified working
                             directory does not exist
     --qnn_model_json_path QNN_MODEL_JSON_PATH
                             Path to the model json for transforming intermediate
                             tensors to spatial-first axis order.

arguments for generating a new tensor_mapping.json:
     -m MODEL_PATH, --model_path MODEL_PATH
                             path to original model for tensor_mapping uses here.
     -e ENGINE_NAME [ENGINE_VERSION ...], --engine ENGINE_NAME [ENGINE_VERSION ...]
                             Name of engine that will be running inference,
                             optionally followed by the engine version. Used here
                             for tensor_mapping.
     -f FRAMEWORK [FRAMEWORK ...], --framework FRAMEWORK [FRAMEWORK ...]
                             Framework type to be used, followed optionally by
                             framework version. Used here for tensor_mapping.

Please note: All the command line arguments should either be provided through command line or through the config file. They will not override those in the config file if there is overlap.
</pre></div>
</div>
<p>The main verification process run using qnn-accuracy-debugger –verification optionally uses –tensor_mapping and –graph_struct to find files to compare.  These files are generated by the
inference engine step, and should be supplied to verification for best results.  By default they are named tensor_mapping.json and {model name}_graph_struct.json, and can be
found in the output directory of the inference engine results.</p>
<p><strong>Sample Command</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">Compare</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">framework</span><span class="w"> </span><span class="n">diagnosis</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">inference</span><span class="w"> </span><span class="n">engine</span><span class="o">:</span>

<span class="n">qnn</span><span class="o">-</span><span class="n">accuracy</span><span class="o">-</span><span class="n">debugger</span><span class="w"> </span>\
<span class="w">     </span><span class="o">--</span><span class="n">verification</span><span class="w"> </span>\
<span class="w">     </span><span class="o">--</span><span class="n">default_verifier</span><span class="w"> </span><span class="n">CosineSimilarity</span><span class="w"> </span><span class="n">param1</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">param2</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span>\
<span class="w">     </span><span class="o">--</span><span class="n">default_verifier</span><span class="w"> </span><span class="n">SQNR</span><span class="w"> </span><span class="n">param1</span><span class="w"> </span><span class="mi">5</span><span class="w"> </span><span class="n">param2</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span>\
<span class="w">     </span><span class="o">--</span><span class="n">golden_output_reference_directory</span><span class="w"> </span><span class="n">$PROJECTREPOPATH</span><span class="o">/</span><span class="n">working_directory</span><span class="o">/</span><span class="n">framework_diagnosis</span><span class="o">/</span><span class="mi">2022-10-31</span><span class="n">_17</span><span class="mo">-07</span><span class="mi">-58</span><span class="o">/</span><span class="w"> </span>\
<span class="w">     </span><span class="o">--</span><span class="n">inference_results</span><span class="w"> </span><span class="n">$PROJECTREPOPATH</span><span class="o">/</span><span class="n">working_directory</span><span class="o">/</span><span class="n">inference_engine</span><span class="o">/</span><span class="n">latest</span><span class="o">/</span><span class="n">output</span><span class="o">/</span><span class="n">Result_0</span><span class="o">/</span><span class="w"> </span>\
<span class="w">     </span><span class="o">--</span><span class="n">tensor_mapping</span><span class="w"> </span><span class="n">$PROJECTREPOPATH</span><span class="o">/</span><span class="n">working_directory</span><span class="o">/</span><span class="n">inference_engine</span><span class="o">/</span><span class="n">latest</span><span class="o">/</span><span class="n">tensor_mapping</span><span class="p">.</span><span class="n">json</span><span class="w"> </span>\
<span class="w">     </span><span class="o">--</span><span class="n">graph_struct</span><span class="w"> </span><span class="n">$PROJECTREPOPATH</span><span class="o">/</span><span class="n">working_directory</span><span class="o">/</span><span class="n">inference_engine</span><span class="o">/</span><span class="n">latest</span><span class="o">/</span><span class="n">qnn_model_graph_struct</span><span class="p">.</span><span class="n">json</span><span class="w"> </span>\
<span class="w">     </span><span class="o">--</span><span class="n">qnn_model_json_path</span><span class="w"> </span><span class="n">$PROJECTREPOPATH</span><span class="o">/</span><span class="n">working_directory</span><span class="o">/</span><span class="n">inference_engine</span><span class="o">/</span><span class="n">latest</span><span class="o">/</span><span class="n">qnn_model_net</span><span class="p">.</span><span class="n">json</span>
</pre></div>
</div>
<dl class="simple">
<dt>Tip:</dt><dd><ul class="simple">
<li><p>If you passed multiple images in the image_list.txt from run inference engine diagnosis, you’ll receive multiple output/Result_x, choose result that matches the input you used for framework diagnosis for comparison (ie. in framework you used chair.raw and inference chair.raw was the first item in the image_list.txt then choose output/Result_0, if chair.raw was the second item in image_list.txt, then choose output/Result_1).</p></li>
<li><p>It is recommended to always supply ‘graph_struct’ and ‘tensor_mapping’ to the command as it is used to line up the report and find the corresponding files for comparison. if tensor_mapping did not get generated from previous steps, you can supplement with ‘model_path’, ‘engine’, ‘framework’ to have module generate ‘tensor_mapping’ during runtime.</p></li>
<li><p>You can also compare inference_engine outputs to inference_engine outputs by passing the /output of the inference_engine output to the ‘framework_results’. If you want the outputs to be exact-name-matching, then you do not need to provide a tensor_mapping file.</p></li>
<li><p>Note that if you need to generate a tensor mapping instead of providing a path to prexisting tensor mapping file. You can provide the ‘model_path’ option.</p></li>
</ul>
</dd>
</dl>
<p>Verifier uses two optional config files. The first file is used to set parameters for specific
verifiers, as well as which tensors to use these verifiers on. The second file is used to map tensor
names from framework_diagnosis to the inference_engine, since certain tensors generated by
framework_diagnosis may have different names than tensors generated by inference_engine.</p>
<p>Verifier Config:</p>
<p>The verifier config file is a JSON file that tells verification which verifiers
(asides from the default verifier) to use and with which parameters and on what specific tensors.
If no config file is provided, the tool will only use the default verifier specified from the
command line, with its default parameters, on all the tensors. The JSON file is keyed by verifier
names, with each verifier as its own dictionary keyed by “parameters” and “tensors”.</p>
<p><strong>Config File</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>```json
{
    &quot;MeanIOU&quot;: {
        &quot;parameters&quot;: {
            &quot;background_classification&quot;: 1.0
        },
        &quot;tensors&quot;: [[&quot;Postprocessor/BatchMultiClassNonMaxSuppression_boxes&quot;, &quot;detection_classes:0&quot;]]
    },
    &quot;TopK&quot;: {
        &quot;parameters&quot;: {
            &quot;k&quot;: 5,
            &quot;ordered&quot;: false
        },
        &quot;tensors&quot;: [[&quot;Reshape_1:0&quot;], [&quot;detection_classes:0&quot;]]
    }
}
```
</pre></div>
</div>
<p>Note that the “tensors” field is a list of lists. This is done because specific verifiers
(e.g. MeanIOU) runs on two tensor at a time. Hence the two tensors are placed in a list. Otherwise
if a verifier only runs on one tensor, it will have a list of lists with only one tensor name in
each list.</p>
<p>Tensor Mapping:</p>
<p>Tensor mapping is a JSON file keyed by inference tensor names, of framework tensor names. If the
tensor mapping is not provided, the tool will assume inference and golden tensor names are
identical.</p>
<p><strong>Tensor Mapping File</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>```json
{
    &quot;Postprocessor/BatchMultiClassNonMaxSuppression_boxes&quot;: &quot;detection_boxes:0&quot;,
    &quot;Postprocessor/BatchMultiClassNonMaxSuppression_scores&quot;: &quot;detection_scores:0&quot;
}
```
</pre></div>
</div>
<p><strong>Output</strong></p>
<p>Verification’s output is divided into different verifiers. For example, if both RtolAtol and TopK
verifiers are used, there will be two sub-directories named “RtolAtol” and “TopK”. Availble verifiers can be
found by issuing –help option.</p>
<div class="figure align-default">
<img alt="../_static/resources/verification_2.png" src="../_static/resources/verification_2.png" />
</div>
<p>Under each sub-directory, the verification analysis for each tensor is organized similar to how
framework_diagnosis (see above) and inference_engine are organized. For each tensor, a CSV and HTML
file is generated. In addition to the tensor-specific analysis, the tool also generates a summary
CSV and HTML file which summarizes the data from all verifiers and their subsequent tensors. The following
figure shows how a sample summary generated in the verification step looks. Each row in this summary corresponds to
one tensor name that is identified by the framework diagnosis and inference engine steps. The final column shows
cosinesimilarity score which can vary between 0 to 1 (this range might be different for other verifiers). Higher scores denote similarity while lower scores indicate variance.
The developer can then further investigate those specific tensor details. Developer should inspect tensors from top-to-bottom order,
meaning if a tensor is broken at an earlier node, anything that was generated post that node is unreliable until that node
is properly fixed.</p>
<div class="figure align-default">
<img alt="../_static/resources/verification_results.png" src="../_static/resources/verification_results.png" />
</div>
</div>
<div class="section" id="compare-encodings">
<h4>Compare Encodings<a class="headerlink" href="#compare-encodings" title="Permalink to this heading">¶</a></h4>
<p>The Compare Encodings feature is designed to compare QNN and AIMET encodings. This feature takes QNN model net and AIMET encoding JSON files as inputs.
This feature executes in the following order.</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Extracts encodings from the given QNN model net JSON.</p></li>
<li><p>Compares extracted QNN encodings with given AIMET encodings.</p></li>
<li><p>Writes results to an Excel file that highlights mismatches.</p></li>
<li><p>Throws warnings if some encodings are present in QNN but not in AIMET and vice-versa.</p></li>
<li><p>Writes the extracted QNN encodings JSON file (for reference).</p></li>
</ol>
</div></blockquote>
</div>
<div class="section" id="id4">
<h4>Usage<a class="headerlink" href="#id4" title="Permalink to this heading">¶</a></h4>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="nl">usage</span><span class="p">:</span><span class="w"> </span><span class="n">qnn</span><span class="o">-</span><span class="n">accuracy</span><span class="o">-</span><span class="n">debugger</span><span class="w"> </span><span class="o">--</span><span class="n">compare_encodings</span><span class="w"> </span><span class="p">[</span><span class="o">-</span><span class="n">h</span><span class="p">]</span>
<span class="w">                             </span><span class="o">--</span><span class="n">input</span><span class="w"> </span><span class="n">INPUT</span>
<span class="w">                             </span><span class="o">--</span><span class="n">aimet_encodings_json</span><span class="w"> </span><span class="n">AIMET_ENCODINGS_JSON</span>
<span class="w">                             </span><span class="p">[</span><span class="o">--</span><span class="n">precision</span><span class="w"> </span><span class="n">PRECISION</span><span class="p">]</span>
<span class="w">                             </span><span class="p">[</span><span class="o">--</span><span class="n">params_only</span><span class="p">]</span>
<span class="w">                             </span><span class="p">[</span><span class="o">--</span><span class="n">activations_only</span><span class="p">]</span>
<span class="w">                             </span><span class="p">[</span><span class="o">--</span><span class="n">specific_node</span><span class="w"> </span><span class="n">SPECIFIC_NODE</span><span class="p">]</span>
<span class="w">                             </span><span class="p">[</span><span class="o">--</span><span class="n">working_dir</span><span class="w"> </span><span class="n">WORKING_DIR</span><span class="p">]</span>
<span class="w">                             </span><span class="p">[</span><span class="o">--</span><span class="n">output_dirname</span><span class="w"> </span><span class="n">OUTPUT_DIRNAME</span><span class="p">]</span>
<span class="w">                             </span><span class="p">[</span><span class="o">-</span><span class="n">v</span><span class="p">]</span>

<span class="n">Script</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">compare</span><span class="w"> </span><span class="n">QNN</span><span class="w"> </span><span class="n">encodings</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">AIMET</span><span class="w"> </span><span class="n">encodings</span>

<span class="n">optional</span><span class="w"> </span><span class="n">arguments</span><span class="o">:</span>
<span class="w">  </span><span class="o">-</span><span class="n">h</span><span class="p">,</span><span class="w"> </span><span class="o">--</span><span class="n">help</span><span class="w">            </span><span class="n">Show</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">help</span><span class="w"> </span><span class="n">message</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">exit</span>

<span class="n">required</span><span class="w"> </span><span class="n">arguments</span><span class="o">:</span>
<span class="w">  </span><span class="o">--</span><span class="n">input</span><span class="w"> </span><span class="n">INPUT</span>
<span class="w">                        </span><span class="n">Path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">QNN</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="n">net</span><span class="w"> </span><span class="n">JSON</span><span class="w"> </span><span class="n">file</span>
<span class="w">  </span><span class="o">--</span><span class="n">aimet_encodings_json</span><span class="w"> </span><span class="n">AIMET_ENCODINGS_JSON</span>
<span class="w">                        </span><span class="n">Path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">AIMET</span><span class="w"> </span><span class="n">encodings</span><span class="w"> </span><span class="n">JSON</span><span class="w"> </span><span class="n">file</span>

<span class="n">optional</span><span class="w"> </span><span class="n">arguments</span><span class="o">:</span>
<span class="w">  </span><span class="o">--</span><span class="n">precision</span><span class="w"> </span><span class="n">PRECISION</span>
<span class="w">                        </span><span class="n">Number</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">decimal</span><span class="w"> </span><span class="n">places</span><span class="w"> </span><span class="n">up</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">which</span><span class="w"> </span><span class="n">comparison</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">done</span><span class="w"> </span><span class="p">(</span><span class="k">default</span><span class="o">:</span><span class="w"> </span><span class="mi">17</span><span class="p">)</span>
<span class="w">  </span><span class="o">--</span><span class="n">params_only</span><span class="w">         </span><span class="n">Compare</span><span class="w"> </span><span class="n">only</span><span class="w"> </span><span class="n">parameters</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">encodings</span>
<span class="w">  </span><span class="o">--</span><span class="n">activations_only</span><span class="w">    </span><span class="n">Compare</span><span class="w"> </span><span class="n">only</span><span class="w"> </span><span class="n">activations</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">encodings</span>
<span class="w">  </span><span class="o">--</span><span class="n">specific_node</span><span class="w"> </span><span class="n">SPECIFIC_NODE</span>
<span class="w">                        </span><span class="n">Display</span><span class="w"> </span><span class="n">encoding</span><span class="w"> </span><span class="n">differences</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">given</span><span class="w"> </span><span class="n">node</span>
<span class="w">  </span><span class="o">--</span><span class="n">working_dir</span><span class="w"> </span><span class="n">WORKING_DIR</span>
<span class="w">                        </span><span class="n">Working</span><span class="w"> </span><span class="n">directory</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">compare_encodings</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">store</span><span class="w"> </span><span class="n">temporary</span><span class="w"> </span><span class="n">files</span><span class="p">.</span>
<span class="w">                        </span><span class="n">Creates</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">new</span><span class="w"> </span><span class="n">directory</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">specified</span><span class="w"> </span><span class="n">working</span><span class="w"> </span><span class="n">directory</span><span class="w"> </span><span class="n">does</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">exist</span><span class="p">.</span>
<span class="w">  </span><span class="o">--</span><span class="n">output_dirname</span><span class="w"> </span><span class="n">OUTPUT_DIRNAME</span>
<span class="w">                        </span><span class="n">Output</span><span class="w"> </span><span class="n">directory</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">compare_encodings</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">store</span><span class="w"> </span><span class="n">temporary</span><span class="w"> </span><span class="n">files</span>
<span class="w">                        </span><span class="n">under</span><span class="w"> </span><span class="o">&lt;</span><span class="n">working_dir</span><span class="o">&gt;/</span><span class="n">compare_encodings</span><span class="p">.</span><span class="w"> </span><span class="n">Creates</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">new</span><span class="w"> </span><span class="n">directory</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">the</span>
<span class="w">                        </span><span class="n">specified</span><span class="w"> </span><span class="n">working</span><span class="w"> </span><span class="n">directory</span><span class="w"> </span><span class="n">does</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">exist</span><span class="p">.</span>
<span class="w">  </span><span class="o">-</span><span class="n">v</span><span class="p">,</span><span class="w"> </span><span class="o">--</span><span class="n">verbose</span><span class="w">         </span><span class="n">Verbose</span><span class="w"> </span><span class="n">printing</span>
</pre></div>
</div>
<p><strong>Sample Commands</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="cp"># Compare both params and activations</span>
<span class="n">qnn</span><span class="o">-</span><span class="n">accuracy</span><span class="o">-</span><span class="n">debugger</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">compare_encodings</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">input</span><span class="w"> </span><span class="n">QNN_model_net</span><span class="p">.</span><span class="n">json</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">aimet_encodings_json</span><span class="w"> </span><span class="n">aimet_encodings</span><span class="p">.</span><span class="n">json</span>

<span class="cp"># Compare only params</span>
<span class="n">qnn</span><span class="o">-</span><span class="n">accuracy</span><span class="o">-</span><span class="n">debugger</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">compare_encodings</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">input</span><span class="w"> </span><span class="n">QNN_model_net</span><span class="p">.</span><span class="n">json</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">aimet_encodings_json</span><span class="w"> </span><span class="n">aimet_encodings</span><span class="p">.</span><span class="n">json</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">params_only</span>

<span class="cp"># Compare only activations</span>
<span class="n">qnn</span><span class="o">-</span><span class="n">accuracy</span><span class="o">-</span><span class="n">debugger</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">compare_encodings</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">input</span><span class="w"> </span><span class="n">QNN_model_net</span><span class="p">.</span><span class="n">json</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">aimet_encodings_json</span><span class="w"> </span><span class="n">aimet_encodings</span><span class="p">.</span><span class="n">json</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">activations_only</span>

<span class="cp"># Compare only a specific encoding</span>
<span class="n">qnn</span><span class="o">-</span><span class="n">accuracy</span><span class="o">-</span><span class="n">debugger</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">compare_encodings</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">input</span><span class="w"> </span><span class="n">QNN_model_net</span><span class="p">.</span><span class="n">json</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">aimet_encodings_json</span><span class="w"> </span><span class="n">aimet_encodings</span><span class="p">.</span><span class="n">json</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">specific_node</span><span class="w"> </span><span class="n">_2_22_Conv_output_0</span>
</pre></div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>A working_directory is generated from wherever this script is called from unless otherwise specified.</p>
</div>
<p><strong>Output</strong></p>
<p>The program creates a directory named <em>latest</em> in <cite>working_directory/compare_encodings</cite> which is symbolically linked to the most recently generated directory. In the example below, <em>latest</em> will
have data that is symlinked to the data in the most recent directory <em>YYYY-MM-DD_HH:mm:ss</em>. Users may choose to override the directory name by passing it to –output_dirname, e.g., <cite>–output_dirname myTest</cite>.</p>
<div class="figure align-default">
<img alt="../_static/resources/compare_encodings.png" src="../_static/resources/compare_encodings.png" />
</div>
<p>The figure above shows a sample output from a compare_encodings run.
The following details what each file contains.</p>
<blockquote>
<div><ul class="simple">
<li><p>compare_encodings_options.json contains all the options used to run this feature</p></li>
<li><p>encodings_diff.xlsx contains comparison results with mismatches highlighted</p></li>
<li><p>log.txt contains log statements for the run</p></li>
<li><p>extracted_encodings.json contains extracted QNN encodings</p></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="tensor-inspection">
<h4>Tensor inspection<a class="headerlink" href="#tensor-inspection" title="Permalink to this heading">¶</a></h4>
<p>Tensor inspection compares given reference output and target output tensors and dumps various statistics to represent differences between them.</p>
<p>The Tensor inspection feature can:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Plot histograms for golden and target tensors</p></li>
<li><p>Plot a graph indicating deviation between golden and target tensors</p></li>
<li><p>Plot a cumulative distribution graph (CDF) for golden vs target tensors</p></li>
<li><p>Plot a density (KDE) graph for target tensor highlighting target min/max and calibrated min/max values</p></li>
<li><p>Create a CSV file containing information about: target min/max; calibrated min/max; golden output min/max; target/calibrated min/max differences; and computed metrics (verifiers).</p></li>
</ol>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<div class="line-block">
<div class="line">Only data with matching target/golden filenames is inspected; other data is ignored</div>
<div class="line">Calibrated min/max values are extracted from a user provided encodings file. If an encodings file is not provided, density plot will be skipped and also the CSV summary output will not include calibrated min/max information.</div>
</div>
</div>
</div>
<div class="section" id="id5">
<h4>Usage<a class="headerlink" href="#id5" title="Permalink to this heading">¶</a></h4>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="nl">usage</span><span class="p">:</span><span class="w"> </span><span class="n">qnn</span><span class="o">-</span><span class="n">accuracy</span><span class="o">-</span><span class="n">debugger</span><span class="w"> </span><span class="o">--</span><span class="n">tensor_inspection</span><span class="w"> </span><span class="p">[</span><span class="o">-</span><span class="n">h</span><span class="p">]</span>
<span class="w">                        </span><span class="o">--</span><span class="n">golden_data</span><span class="w"> </span><span class="n">GOLDEN_DATA</span>
<span class="w">                        </span><span class="o">--</span><span class="n">target_data</span><span class="w"> </span><span class="n">TARGET_DATA</span>
<span class="w">                        </span><span class="o">--</span><span class="n">verifier</span><span class="w"> </span><span class="n">VERIFIER</span><span class="w"> </span><span class="p">[</span><span class="n">VERIFIER</span><span class="w"> </span><span class="p">...]</span>
<span class="w">                        </span><span class="p">[</span><span class="o">-</span><span class="n">w</span><span class="w"> </span><span class="n">WORKING_DIR</span><span class="p">]</span>
<span class="w">                        </span><span class="p">[</span><span class="o">--</span><span class="n">data_type</span><span class="w"> </span><span class="p">{</span><span class="n">int8</span><span class="p">,</span><span class="n">uint8</span><span class="p">,</span><span class="n">int16</span><span class="p">,</span><span class="n">uint16</span><span class="p">,</span><span class="n">float32</span><span class="p">}]</span>
<span class="w">                        </span><span class="p">[</span><span class="o">--</span><span class="n">target_encodings</span><span class="w"> </span><span class="n">TARGET_ENCODINGS</span><span class="p">]</span>
<span class="w">                        </span><span class="p">[</span><span class="o">-</span><span class="n">v</span><span class="p">]</span>

<span class="n">Script</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">inspection</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span>

<span class="n">required</span><span class="w"> </span><span class="n">arguments</span><span class="o">:</span>
<span class="w">  </span><span class="o">--</span><span class="n">golden_data</span><span class="w"> </span><span class="n">GOLDEN_DATA</span>
<span class="w">                        </span><span class="n">Path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">golden</span><span class="o">/</span><span class="n">framework</span><span class="w"> </span><span class="n">outputs</span><span class="w"> </span><span class="n">folder</span><span class="p">.</span><span class="w"> </span><span class="n">Paths</span><span class="w"> </span><span class="n">may</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">absolute</span><span class="w"> </span><span class="n">or</span>
<span class="w">                        </span><span class="n">relative</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">working</span><span class="w"> </span><span class="n">directory</span><span class="p">.</span>
<span class="w">  </span><span class="o">--</span><span class="n">target_data</span><span class="w"> </span><span class="n">TARGET_DATA</span>
<span class="w">                        </span><span class="n">Path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">target</span><span class="w"> </span><span class="n">outputs</span><span class="w"> </span><span class="n">folder</span><span class="p">.</span><span class="w"> </span><span class="n">Paths</span><span class="w"> </span><span class="n">may</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">absolute</span><span class="w"> </span><span class="n">or</span><span class="w"> </span><span class="n">relative</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span>
<span class="w">                        </span><span class="n">working</span><span class="w"> </span><span class="n">directory</span><span class="p">.</span>
<span class="w">  </span><span class="o">--</span><span class="n">verifier</span><span class="w"> </span><span class="n">VERIFIER</span><span class="w"> </span><span class="p">[</span><span class="n">VERIFIER</span><span class="w"> </span><span class="p">...]</span>
<span class="w">                        </span><span class="n">Verifier</span><span class="w"> </span><span class="n">used</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">verification</span><span class="p">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">options</span><span class="w"> </span><span class="s">&quot;RtolAtol&quot;</span><span class="p">,</span>
<span class="w">                        </span><span class="s">&quot;AdjustedRtolAtol&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;TopK&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;L1Error&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;CosineSimilarity&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;MSE&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;MAE&quot;</span><span class="p">,</span>
<span class="w">                        </span><span class="s">&quot;SQNR&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;MeanIOU&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;ScaledDiff&quot;</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">supported</span><span class="p">.</span>
<span class="w">                        </span><span class="n">An</span><span class="w"> </span><span class="n">optional</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">hyperparameters</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">appended</span><span class="p">,</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">example</span><span class="o">:</span>
<span class="w">                        </span><span class="o">--</span><span class="n">verifier</span><span class="w"> </span><span class="n">rtolatol</span><span class="p">,</span><span class="n">rtolmargin</span><span class="p">,</span><span class="mf">0.01</span><span class="p">,</span><span class="n">atolmargin</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mf">01.</span>
<span class="w">                        </span><span class="n">To</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="n">multiple</span><span class="w"> </span><span class="n">verifiers</span><span class="p">,</span><span class="w"> </span><span class="n">add</span><span class="w"> </span><span class="n">additional</span><span class="w"> </span><span class="o">--</span><span class="n">verifier</span><span class="w"> </span><span class="n">CosineSimilarity</span>

<span class="n">optional</span><span class="w"> </span><span class="n">arguments</span><span class="o">:</span>
<span class="w">  </span><span class="o">-</span><span class="n">w</span><span class="w"> </span><span class="n">WORKING_DIR</span><span class="p">,</span><span class="w"> </span><span class="o">--</span><span class="n">working_dir</span><span class="w"> </span><span class="n">WORKING_DIR</span>
<span class="w">                        </span><span class="n">Working</span><span class="w"> </span><span class="n">directory</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">save</span><span class="w"> </span><span class="n">results</span><span class="p">.</span><span class="w"> </span><span class="n">Creates</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">new</span><span class="w"> </span><span class="n">directory</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">the</span>
<span class="w">                        </span><span class="n">specified</span><span class="w"> </span><span class="n">working</span><span class="w"> </span><span class="n">directory</span><span class="w"> </span><span class="n">does</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">exist</span>
<span class="w">  </span><span class="o">--</span><span class="n">data_type</span><span class="w"> </span><span class="p">{</span><span class="n">int8</span><span class="p">,</span><span class="n">uint8</span><span class="p">,</span><span class="n">int16</span><span class="p">,</span><span class="n">uint16</span><span class="p">,</span><span class="n">float32</span><span class="p">}</span>
<span class="w">                        </span><span class="n">DataType</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span>
<span class="w">  </span><span class="o">--</span><span class="n">target_encodings</span><span class="w"> </span><span class="n">TARGET_ENCODINGS</span>
<span class="w">                        </span><span class="n">Path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">target</span><span class="w"> </span><span class="n">encodings</span><span class="w"> </span><span class="n">json</span><span class="w"> </span><span class="n">file</span><span class="p">.</span>
<span class="w">  </span><span class="o">-</span><span class="n">v</span><span class="p">,</span><span class="w"> </span><span class="o">--</span><span class="n">verbose</span><span class="w">         </span><span class="n">Verbose</span><span class="w"> </span><span class="n">printing</span>
</pre></div>
</div>
<p><strong>Sample Commands</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="cp"># Basic run</span>
<span class="n">qnn</span><span class="o">-</span><span class="n">accuracy</span><span class="o">-</span><span class="n">debugger</span><span class="w"> </span><span class="o">--</span><span class="n">tensor_inspection</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">golden_data</span><span class="w"> </span><span class="n">golden_tensors_dir</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">target_data</span><span class="w"> </span><span class="n">target_tensors_dir</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">verifier</span><span class="w"> </span><span class="n">sqnr</span>

<span class="cp"># Pass target encodings file and enable multiple verifiers</span>
<span class="n">qnn</span><span class="o">-</span><span class="n">accuracy</span><span class="o">-</span><span class="n">debugger</span><span class="w"> </span><span class="o">--</span><span class="n">tensor_inspection</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">golden_data</span><span class="w"> </span><span class="n">golden_tensors_dir</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">target_data</span><span class="w"> </span><span class="n">target_tensors_dir</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">verifier</span><span class="w"> </span><span class="n">mse</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">verifier</span><span class="w"> </span><span class="n">sqnr</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">verifier</span><span class="w"> </span><span class="n">rtolatol</span><span class="p">,</span><span class="n">rtolmargin</span><span class="p">,</span><span class="mf">0.01</span><span class="p">,</span><span class="n">atolmargin</span><span class="p">,</span><span class="mf">0.01</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">target_encodings</span><span class="w"> </span><span class="n">qnn_encoding</span><span class="p">.</span><span class="n">json</span>
</pre></div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>A working_directory is generated from wherever this script is called from unless otherwise specified.</p>
</div>
<div class="figure align-default">
<img alt="../_static/resources/tensor_inspection.png" src="../_static/resources/tensor_inspection.png" />
</div>
<p>The figure above shows a sample output from a Tensor inspection run.
The following details what each file contains.</p>
<blockquote>
<div><ul class="simple">
<li><p>Each tensor will have its own directory; the directory name matches the tensor name.</p>
<ul>
<li><p>CDF_plots.png – Golden vs target CDF graph</p></li>
<li><p>Diff_plots.png – Golden and target deviation graph</p></li>
<li><p>Distribution_min-max.png – Density plot for target tensor highlighting target vs calibrated min/max values</p></li>
<li><p>Histograms.png – Golden and target histograms</p></li>
<li><p>golden_data.csv – Golden tensor data</p></li>
<li><p>target_data.csv – Target tensor data</p></li>
</ul>
</li>
<li><p>log.txt – Log statements from the entire run</p></li>
<li><p>summary.csv – Target min/max, calibrated min/max, golden output min/max, target vs calibrated min/max differences, and verifier outputs</p></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="run-qnn-accuracy-debugger-e2e">
<h4>Run QNN Accuracy Debugger E2E<a class="headerlink" href="#run-qnn-accuracy-debugger-e2e" title="Permalink to this heading">¶</a></h4>
<p>This feature is designed to run the framework diagnosis, inference engine, and verification features sequentially with a single command to debug the model. The following debugging algorithms are available.</p>
<ol class="arabic simple">
<li><dl class="simple">
<dt>Oneshot-layerwise(default):</dt><dd><ul class="simple">
<li><dl class="simple">
<dt>This algorithm is designed to debug all layers of model at a time by performing below steps</dt><dd><ul>
<li><p>Execute framework diagnosis to collect reference outputs in fp32</p></li>
<li><p>Execute inference engine to collect backend outputs in provided target precision.</p></li>
<li><p>Execute verification for comparison of intermediate outputs from the above 2 steps</p></li>
<li><p>Execute tensor inspection (when –enable_tensor_inspection is passed) to dump various plots, e.g., scatter, line, CDF, etc., for intermediate outputs</p></li>
</ul>
</dd>
</dl>
</li>
<li><p>It provides quick analysis to identify layers of model causing accuracy deviation.</p></li>
<li><p>User can chose cumulative-layerwise(below) for deeper analysis of accuracy deviation.</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Cumulative-layerwise:</dt><dd><ul class="simple">
<li><dl class="simple">
<dt>This algorithm is designed to debug one layer at a time by performing below steps</dt><dd><ul>
<li><p>Execute framework diagnosis to collect reference outputs from all layers of model in fp32.</p></li>
<li><dl class="simple">
<dt>Execute inference engine and verification in iterative manner to perform below operations</dt><dd><ul>
<li><p>to collect backend outputs in target precision for each layer while removing the effect of its preceeding layers on final output.</p></li>
<li><p>to compare intermediate outputs from framework diagnosis and inference engine</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><p>It provides deeper analysis to identify all layers of model causing accuracy deviation.</p></li>
<li><p>Currently this option supports only onnx models.</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Layerwise:</dt><dd><ul class="simple">
<li><dl class="simple">
<dt>This algorithm is designed to debug a single layer model at a time by performing the following steps</dt><dd><ul>
<li><p>Get golden reference per layer outputs from an external tool or, if a golden reference is not given, run framework diagnosis to collect intermediate layer outputs.</p></li>
<li><dl class="simple">
<dt>Iteratively execute inference engine and verification to:</dt><dd><ul>
<li><p>Collect backend outputs in target precision for each single layer model by removing the preceding and following layers</p></li>
<li><p>Compare intermediate output from golden reference with inference engine single layer model output</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><p>Layerwise snooping provides deeper analysis to identify all model layers causing accuracy deviation on hardware with respect to framework/simulation outputs.</p></li>
<li><p>Layerwise snooping only supports ONNX models.</p></li>
</ul>
</dd>
</dl>
</li>
</ol>
</div>
<div class="section" id="id6">
<h4>Usage<a class="headerlink" href="#id6" title="Permalink to this heading">¶</a></h4>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span>usage: qnn-accuracy-debugger [--framework_diagnosis] [--inference_engine] [--verification] [-h]

Script that runs Framework Diagnosis, Inference Engine or Verification.

Arguments to select which component of the tool to run.  Arguments are mutually exclusive (at
most 1 can be selected).  If none are selected, then all components are run:
--framework_diagnosis Run framework
--inference_engine    Run inference engine
--verification        Run verification

optional arguments:
-h, --help              Show this help message. To show help for any of the components, run
                        script with --help and --&lt;component&gt;. For example, to show the help
                        for Framework Diagnosis, run script with the following: --help
                        --framework_diagnosis

usage: qnn-accuracy-debugger [-h] -f FRAMEWORK [FRAMEWORK ...] -m MODEL_PATH -i INPUT_TENSOR
                            [INPUT_TENSOR ...] -o OUTPUT_TENSOR -r RUNTIME -a
                            {aarch64-android,x86_64-linux-clang,aarch64-android-clang6.0}
                            -l INPUT_LIST --default_verifier DEFAULT_VERIFIER
                            [DEFAULT_VERIFIER ...] [-v] [-w WORKING_DIR]
                            [--output_dirname OUTPUT_DIRNAME]
                            [--deep_analyzer {modelDissectionAnalyzer}]
                            [--debugging_algorithm {layerwise,cumulative-layerwise,oneshot-layerwise}]

Options for running the Accuracy Debugger components

optional arguments:
-h, --help            show this help message and exit

Arguments required by both Framework Diagnosis and Inference Engine:
-f FRAMEWORK [FRAMEWORK ...], --framework FRAMEWORK [FRAMEWORK ...]
                        Framework type and version, version is optional. Currently supported
                        frameworks are [tensorflow, tflite, onnx]. For example, tensorflow
                        2.3.0
-m MODEL_PATH, --model_path MODEL_PATH
                        Path to the model file(s).
-i INPUT_TENSOR [INPUT_TENSOR ...], --input_tensor INPUT_TENSOR [INPUT_TENSOR ...]
                        The name, dimensions, raw data, and optionally data type of the
                        network input tensor(s) specifiedin the format &quot;input_name&quot; comma-
                        separated-dimensions path-to-raw-file, for example: &quot;data&quot;
                        1,224,224,3 data.raw float32. Note that the quotes should always be
                        included in order to handle special characters, spaces, etc. For
                        multiple inputs specify multiple --input_tensor on the command line
                        like: --input_tensor &quot;data1&quot; 1,224,224,3 data1.raw --input_tensor
                        &quot;data2&quot; 1,50,100,3 data2.raw float32.
-o OUTPUT_TENSOR, --output_tensor OUTPUT_TENSOR
                        Name of the graph&#39;s specified output tensor(s).

Arguments required by Inference Engine:
-r RUNTIME, --runtime RUNTIME
                        Runtime to be used for inference.
-a {aarch64-android,x86_64-linux-clang,aarch64-android-clang6.0}, --architecture {aarch64-an
droid,x86_64-linux-clang,aarch64-android-clang6.0}
                        Name of the architecture to use for inference engine.
-l INPUT_LIST, --input_list INPUT_LIST
                        Path to the input list text.
Arguments required by Verification:                                                    [3/467]
--default_verifier DEFAULT_VERIFIER [DEFAULT_VERIFIER ...]
                        Default verifier used for verification. The options &quot;RtolAtol&quot;,
                        &quot;AdjustedRtolAtol&quot;, &quot;TopK&quot;, &quot;L1Error&quot;, &quot;CosineSimilarity&quot;, &quot;MSE&quot;,
                        &quot;MAE&quot;, &quot;SQNR&quot;, &quot;MeanIOU&quot;, &quot;ScaledDiff&quot; are supported. An optional
                        list of hyperparameters can be appended. For example:
                        --default_verifier rtolatol,rtolmargin,0.01,atolmargin,0,01. An
                        optional list of placeholders can be appended. For example:
                        --default_verifier CosineSimilarity param1 1 param2 2. to use
                        multiple verifiers, add additional --default_verifier
                        CosineSimilarity

optional arguments:
-v, --verbose           Verbose printing
-w WORKING_DIR, --working_dir WORKING_DIR
                        Working directory for the wrapper to store temporary files. Creates
                        a new directory if the specified working directory does not exitst.
--output_dirname OUTPUT_DIRNAME
                        output directory name for the wrapper to store temporary files under
                        &lt;working_dir&gt;/wrapper. Creates a new directory if the specified
                        working directory does not exist
--deep_analyzer {modelDissectionAnalyzer}
                        Deep Analyzer to perform deep analysis
--golden_output_reference_directory
                        Optional parameter to indicate the directory of the golden reference outputs.
                        When this option is provided, the framework diagnosis is stage skipped.
                        In inference stage, it&#39;s used for tensor mapping without a framework.
                        In verification stage, it&#39;s used as a reference to compare
                        outputs produced in the inference engine stage.
--enable_tensor_inspection
                        Plots graphs (line, scatter, CDF etc.) for each
                        layer&#39;s output. Additionally, summary sheet will have
                        more details like golden min/max, target min/max etc.,
--debugging_algorithm {layerwise,cumulative-layerwise,oneshot-layerwise}
                        Performs model debugging layerwise, cumulative-layerwise or in oneshot-
                        layerwise based on choice.

(below options are allowed only for Layerwise and Cumulative layerwise run)
--start_layer START_LAYER
                        Extracts the given model from mentioned start layer
                        output name
--end_layer END_LAYER
                        Extracts the given model from mentioned end layer
                        output name
</pre></div>
</div>
<p><strong>Sample Command for oneshot-layerwise</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">Command</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">Oneshot</span><span class="o">-</span><span class="n">layerwise</span><span class="w"> </span><span class="n">using</span><span class="w"> </span><span class="n">DSP</span><span class="w"> </span><span class="n">backend</span><span class="o">:</span>

<span class="n">qnn</span><span class="o">-</span><span class="n">accuracy</span><span class="o">-</span><span class="n">debugger</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">framework</span><span class="w"> </span><span class="n">tensorflow</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">runtime</span><span class="w"> </span><span class="n">dspv73</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">model_path</span><span class="w"> </span><span class="n">$RESOURCESPATH</span><span class="o">/</span><span class="n">samples</span><span class="o">/</span><span class="n">InceptionV3Model</span><span class="o">/</span><span class="n">inception_v3_2016_08_28_frozen</span><span class="p">.</span><span class="n">pb</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">input_tensor</span><span class="w"> </span><span class="s">&quot;input:0&quot;</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="mi">299</span><span class="p">,</span><span class="mi">299</span><span class="p">,</span><span class="mi">3</span><span class="w"> </span><span class="n">$PATHTOGOLDENI</span><span class="o">/</span><span class="n">samples</span><span class="o">/</span><span class="n">InceptionV3Model</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">chairs</span><span class="p">.</span><span class="n">raw</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">output_tensor</span><span class="w"> </span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">Predictions</span><span class="o">/</span><span class="n">Reshape_1</span><span class="o">:</span><span class="mi">0</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">architecture</span><span class="w"> </span><span class="n">x86_64</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">clang</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">debugging_algorithm</span><span class="w"> </span><span class="n">oneshot</span><span class="o">-</span><span class="n">layerwise</span>
<span class="w">    </span><span class="o">--</span><span class="n">input_list</span><span class="w"> </span><span class="n">$RESOURCESPATH</span><span class="o">/</span><span class="n">samples</span><span class="o">/</span><span class="n">InceptionV3Model</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">image_list</span><span class="p">.</span><span class="n">txt</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">default_verifier</span><span class="w"> </span><span class="n">CosineSimilarity</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">framework_results</span><span class="w"> </span><span class="n">$PROJECTREPOPATH</span><span class="o">/</span><span class="n">working_directory</span><span class="o">/</span><span class="n">framework_diagnosis</span><span class="o">/</span><span class="n">latest</span><span class="o">/</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">inference_results</span><span class="w"> </span><span class="n">$PROJECTREPOPATH</span><span class="o">/</span><span class="n">working_directory</span><span class="o">/</span><span class="n">inference_engine</span><span class="o">/</span><span class="n">latest</span><span class="o">/</span><span class="n">output</span><span class="o">/</span><span class="n">Result_0</span><span class="o">/</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">tensor_mapping</span><span class="w"> </span><span class="n">$PROJECTREPOPATH</span><span class="o">/</span><span class="n">working_directory</span><span class="o">/</span><span class="n">inference_engine</span><span class="o">/</span><span class="n">latest</span><span class="o">/</span><span class="n">tensor_mapping</span><span class="p">.</span><span class="n">json</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">graph_struct</span><span class="w"> </span><span class="n">$PROJECTREPOPATH</span><span class="o">/</span><span class="n">working_directory</span><span class="o">/</span><span class="n">inference_engine</span><span class="o">/</span><span class="n">latest</span><span class="o">/</span><span class="n">qnn_model_graph_struct</span><span class="p">.</span><span class="n">json</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">qnn_model_json_path</span><span class="w"> </span><span class="n">$PROJECTREPOPATH</span><span class="o">/</span><span class="n">working_directory</span><span class="o">/</span><span class="n">inference_engine</span><span class="o">/</span><span class="n">latest</span><span class="o">/</span><span class="n">qnn_model_net</span><span class="p">.</span><span class="n">json</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">enable_tensor_inspection</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">verbose</span>
</pre></div>
</div>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">Command</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">Oneshot</span><span class="o">-</span><span class="n">layerwise</span><span class="w"> </span><span class="n">using</span><span class="w"> </span><span class="n">HTP</span><span class="w"> </span><span class="n">emulation</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">x86</span><span class="w"> </span><span class="n">host</span><span class="o">:</span>

<span class="n">qnn</span><span class="o">-</span><span class="n">accuracy</span><span class="o">-</span><span class="n">debugger</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">framework</span><span class="w"> </span><span class="n">onnx</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">runtime</span><span class="w"> </span><span class="n">htp</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">model_path</span><span class="w"> </span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">mnt</span><span class="o">/</span><span class="n">workspace</span><span class="o">/</span><span class="n">models</span><span class="o">/</span><span class="n">vit</span><span class="o">/</span><span class="n">vit_base_16_224</span><span class="p">.</span><span class="n">onnx</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">input_tensor</span><span class="w"> </span><span class="s">&quot;input.1&quot;</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">224</span><span class="p">,</span><span class="mi">224</span><span class="w"> </span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">mnt</span><span class="o">/</span><span class="n">workspace</span><span class="o">/</span><span class="n">models</span><span class="o">/</span><span class="n">vit</span><span class="o">/</span><span class="mo">00000003</span><span class="mi">9769</span><span class="n">_1_3_224_224</span><span class="p">.</span><span class="n">raw</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">output_tensor</span><span class="w"> </span><span class="mi">1597</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">architecture</span><span class="w"> </span><span class="n">x86_64</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">clang</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">input_list</span><span class="w"> </span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">mnt</span><span class="o">/</span><span class="n">workspace</span><span class="o">/</span><span class="n">models</span><span class="o">/</span><span class="n">vit</span><span class="o">/</span><span class="n">list</span><span class="p">.</span><span class="n">txt</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">default_verifier</span><span class="w"> </span><span class="n">CosineSimilarity</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">offline_prepare</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">debugging_algorithm</span><span class="w"> </span><span class="n">oneshot</span><span class="o">-</span><span class="n">layerwise</span>
<span class="w">    </span><span class="o">--</span><span class="n">engine</span><span class="w"> </span><span class="n">QNN</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">enable_tensor_inspection</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">verbose</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The –enable_tensor_inspection argument significantly increases overall execution time when used with large models. To speed up execution, omit this argument.</p>
</div>
<p><strong>Output</strong></p>
<p>The program creates framework_diagnosis, inference_engine, verification, and wrapper output directories as below:</p>
<div class="figure align-default">
<img alt="../_static/resources/oneshot-layerwise.png" src="../_static/resources/oneshot-layerwise.png" />
</div>
<ul class="simple">
<li><p>framework_diagnosis – Contains a timestamped directory that contains the intermediate layer outputs (framework) stored in .raw format as described in the framework diagnosis step.</p></li>
<li><p>inference_engine – Contains a timestamped directory that contains the intermediate layer outputs (inference engine) stored in .raw format as described in the inference engine step.</p></li>
<li><p>verification directory – Contains a timestamped directory that contains the following:</p>
<ul>
<li><p>A directory for each verifier specified while running oneshot; it contains CSV and HTML files with metric details for each layer output</p></li>
<li><p>tensor_inspection – Individual directories for each layer’s output with the following contents:</p>
<ul>
<li><p>CDF_plots.png – Golden vs target CDF graph</p></li>
<li><p>Diff_plots.png – Golden and target deviation graph</p></li>
<li><p>Histograms.png – Golden and target histograms</p></li>
<li><p>golden_data.csv – Golden tensor data</p></li>
<li><p>target_data.csv – Target tensor data</p></li>
</ul>
</li>
<li><p>summary.csv – Report for verification results of each layers output</p></li>
</ul>
</li>
<li><p>Wrapper directory containing log.txt with the entire log for the run.</p></li>
</ul>
<p>Note: Except wrapper directory all other directories will have a folder called latest which is a symlink to the latest run’s corresponding timestamped directory.</p>
<p>Snapshot of summary.csv file:</p>
<div class="figure align-default">
<img alt="../_static/resources/oneshot_summary.png" src="../_static/resources/oneshot_summary.png" />
</div>
<p>Understanding the oneshot-layerwise report:</p>
<table class="colwidths-given docutils align-default">
<colgroup>
<col style="width: 17%" />
<col style="width: 83%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Column</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Name</p></td>
<td><p>Output name of the current layer</p></td>
</tr>
<tr class="row-odd"><td><p>Layer Type</p></td>
<td><p>Type of the current layer</p></td>
</tr>
<tr class="row-even"><td><p>Size</p></td>
<td><p>Size of this layer’s output</p></td>
</tr>
<tr class="row-odd"><td><p>Tensor_dims</p></td>
<td><p>Shape of this layer’s output</p></td>
</tr>
<tr class="row-even"><td><p>&lt;Verifier name&gt;</p></td>
<td><p>Verifier value of the current layer output compared to reference output</p></td>
</tr>
<tr class="row-odd"><td><p>golden_min</p></td>
<td><p>minimum value in the reference output for current layer</p></td>
</tr>
<tr class="row-even"><td><p>golden_max</p></td>
<td><p>maximum value in the reference output for current layer</p></td>
</tr>
<tr class="row-odd"><td><p>target_min</p></td>
<td><p>minimum value in the target output for current layer</p></td>
</tr>
<tr class="row-even"><td><p>target_max</p></td>
<td><p>maximum value in the target output for current layer</p></td>
</tr>
</tbody>
</table>
<p><strong>Sample Command for cumulative-layerwise</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">Command</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">Cumulative</span><span class="o">-</span><span class="n">layerwise</span><span class="w"> </span><span class="n">using</span><span class="w"> </span><span class="n">DSP</span><span class="w"> </span><span class="n">backend</span><span class="o">:</span>

<span class="n">qnn</span><span class="o">-</span><span class="n">accuracy</span><span class="o">-</span><span class="n">debugger</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">framework</span><span class="w"> </span><span class="n">onnx</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">runtime</span><span class="w"> </span><span class="n">dspv73</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">model_path</span><span class="w"> </span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">mnt</span><span class="o">/</span><span class="n">workspace</span><span class="o">/</span><span class="n">models</span><span class="o">/</span><span class="n">vit</span><span class="o">/</span><span class="n">vit_base_16_224</span><span class="p">.</span><span class="n">onnx</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">input_tensor</span><span class="w"> </span><span class="s">&quot;input.1&quot;</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">224</span><span class="p">,</span><span class="mi">224</span><span class="w"> </span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">mnt</span><span class="o">/</span><span class="n">workspace</span><span class="o">/</span><span class="n">models</span><span class="o">/</span><span class="n">vit</span><span class="o">/</span><span class="mo">00000003</span><span class="mi">9769</span><span class="n">_1_3_224_224</span><span class="p">.</span><span class="n">raw</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">output_tensor</span><span class="w"> </span><span class="mi">1597</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">architecture</span><span class="w"> </span><span class="n">x86_64</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">clang</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">input_list</span><span class="w"> </span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">mnt</span><span class="o">/</span><span class="n">workspace</span><span class="o">/</span><span class="n">models</span><span class="o">/</span><span class="n">vit</span><span class="o">/</span><span class="n">list</span><span class="p">.</span><span class="n">txt</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">default_verifier</span><span class="w"> </span><span class="n">CosineSimilarity</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">offline_prepare</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">debugging_algorithm</span><span class="w"> </span><span class="n">cumulative</span><span class="o">-</span><span class="n">layerwise</span>
<span class="w">    </span><span class="o">--</span><span class="n">engine</span><span class="w"> </span><span class="n">QNN</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">verbose</span>
</pre></div>
</div>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">Command</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">Cumulative</span><span class="o">-</span><span class="n">layerwise</span><span class="w"> </span><span class="n">using</span><span class="w"> </span><span class="n">HTP</span><span class="w"> </span><span class="n">emulation</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">x86</span><span class="w"> </span><span class="n">host</span><span class="o">:</span>

<span class="n">qnn</span><span class="o">-</span><span class="n">accuracy</span><span class="o">-</span><span class="n">debugger</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">framework</span><span class="w"> </span><span class="n">onnx</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">runtime</span><span class="w"> </span><span class="n">htp</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">model_path</span><span class="w"> </span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">mnt</span><span class="o">/</span><span class="n">workspace</span><span class="o">/</span><span class="n">models</span><span class="o">/</span><span class="n">vit</span><span class="o">/</span><span class="n">vit_base_16_224</span><span class="p">.</span><span class="n">onnx</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">input_tensor</span><span class="w"> </span><span class="s">&quot;input.1&quot;</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">224</span><span class="p">,</span><span class="mi">224</span><span class="w"> </span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">mnt</span><span class="o">/</span><span class="n">workspace</span><span class="o">/</span><span class="n">models</span><span class="o">/</span><span class="n">vit</span><span class="o">/</span><span class="mo">00000003</span><span class="mi">9769</span><span class="n">_1_3_224_224</span><span class="p">.</span><span class="n">raw</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">output_tensor</span><span class="w"> </span><span class="mi">1597</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">architecture</span><span class="w"> </span><span class="n">x86_64</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">clang</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">input_list</span><span class="w"> </span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">mnt</span><span class="o">/</span><span class="n">workspace</span><span class="o">/</span><span class="n">models</span><span class="o">/</span><span class="n">vit</span><span class="o">/</span><span class="n">list</span><span class="p">.</span><span class="n">txt</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">default_verifier</span><span class="w"> </span><span class="n">CosineSimilarity</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">offline_prepare</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">debugging_algorithm</span><span class="w"> </span><span class="n">cumulative</span><span class="o">-</span><span class="n">layerwise</span>
<span class="w">    </span><span class="o">--</span><span class="n">engine</span><span class="w"> </span><span class="n">QNN</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">verbose</span>
</pre></div>
</div>
<p><strong>Output</strong></p>
<p>The program creates output directories framework_diagnosis, cumulative_layerwise_snooping and wrapper directories as below</p>
<div class="figure align-default">
<img alt="../_static/resources/cumulative_layerwise_work_dir.png" src="../_static/resources/cumulative_layerwise_work_dir.png" />
</div>
<ul class="simple">
<li><p>framework_diagnosis directory contains timestamped directory that contains the intermediate layers outputs stored
in .raw format just as mentioned in Framework Diagnosis step.</p></li>
<li><p>cumulative_layerwise_snooping directory contains intemediate outputs obtained from inference engine step stored
in separate directories with respective layer names. Also it contains final report named cumulative_layerwise.csv
which contains verifier scores for each layer. User can identify layers with most deviating scores as problematic nodes.</p></li>
<li><p>Wrapper directory consists a log.txt where user can refer entire logs for the whole run.</p></li>
</ul>
<div class="figure align-default">
<img alt="../_static/resources/cumulative_layerwise_report.png" src="../_static/resources/cumulative_layerwise_report.png" />
</div>
<p>Understanding the cumulative-layerwise report</p>
<p>At the end of cumulative-layerwise run, the tool generates .csv with below information for each layer</p>
<table class="colwidths-given docutils align-default">
<colgroup>
<col style="width: 17%" />
<col style="width: 83%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Column</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>O/P Name</p></td>
<td><p>Output name of the current layer.</p></td>
</tr>
<tr class="row-odd"><td><p>Status</p></td>
<td><dl class="simple">
<dt>If empty, indicates normal execution.Other possible values:</dt><dd><ul class="simple">
<li><p>skip - This layer was not debugged as requested by the user.</p></li>
<li><p>part - Due to the mismatch at this layer, the model was partitioned after this layer</p></li>
<li><p>err_part - error occured while partitioning model at that layer.</p></li>
<li><p>err_con -  coverter error occurred at this layer.</p></li>
<li><p>err_lib - lib-generator error occurred at this layer.</p></li>
<li><p>err_cntx - context-bin-generator error occurred at this layer.</p></li>
<li><p>err-exec - Failed to execute the compiled model at this layer.</p></li>
<li><p>err-compare - Failed to compare the backend output of this layer with reference.</p></li>
</ul>
</dd>
</dl>
</td>
</tr>
<tr class="row-even"><td><p>Layer Type</p></td>
<td><p>Type of the current layer.</p></td>
</tr>
<tr class="row-odd"><td><p>Shape</p></td>
<td><p>Shape of this layer’s output.</p></td>
</tr>
<tr class="row-even"><td><p>Activations</p></td>
<td><p>The Min, Max and Median of the outputs at this layer taken from reference execution.</p></td>
</tr>
<tr class="row-odd"><td><p>&lt;Verifier name&gt;</p></td>
<td><p>Absolute verifier value of the current layer compared to reference platform.</p></td>
</tr>
<tr class="row-even"><td><p>Orig outputs</p></td>
<td><div class="line-block">
<div class="line">Displays the original outputs verifier score observed when the model was run with the current</div>
<div class="line">layer output enabled starting from the last partitioned layer.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>Info</p></td>
<td><p>Displays information for the output verifiers, if the values are abnormal.</p></td>
</tr>
</tbody>
</table>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">Command</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">Layerwise</span><span class="o">:</span>

<span class="n">qnn</span><span class="o">-</span><span class="n">accuracy</span><span class="o">-</span><span class="n">debugger</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">framework</span><span class="w"> </span><span class="n">onnx</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">runtime</span><span class="w"> </span><span class="n">dspv73</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">model_path</span><span class="w"> </span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">mnt</span><span class="o">/</span><span class="n">workspace</span><span class="o">/</span><span class="n">models</span><span class="o">/</span><span class="n">vit</span><span class="o">/</span><span class="n">vit_base_16_224</span><span class="p">.</span><span class="n">onnx</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">input_tensor</span><span class="w"> </span><span class="s">&quot;input.1&quot;</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">224</span><span class="p">,</span><span class="mi">224</span><span class="w"> </span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">mnt</span><span class="o">/</span><span class="n">workspace</span><span class="o">/</span><span class="n">models</span><span class="o">/</span><span class="n">vit</span><span class="o">/</span><span class="mo">00000003</span><span class="mi">9769</span><span class="n">_1_3_224_224</span><span class="p">.</span><span class="n">raw</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">output_tensor</span><span class="w"> </span><span class="mi">1597</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">architecture</span><span class="w"> </span><span class="n">x86_64</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">clang</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">input_list</span><span class="w"> </span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">mnt</span><span class="o">/</span><span class="n">workspace</span><span class="o">/</span><span class="n">models</span><span class="o">/</span><span class="n">vit</span><span class="o">/</span><span class="n">list</span><span class="p">.</span><span class="n">txt</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">default_verifier</span><span class="w"> </span><span class="n">CosineSimilarity</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">offline_prepare</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">debugging_algorithm</span><span class="w"> </span><span class="n">layerwise</span>
<span class="w">    </span><span class="o">--</span><span class="n">golden_directory</span><span class="w"> </span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">mnt</span><span class="o">/</span><span class="n">workspace</span><span class="o">/</span><span class="n">aimet_layer_output_dump</span><span class="o">/</span><span class="n">outputs</span><span class="o">/</span><span class="n">layer_outputs_0</span><span class="o">/</span>
<span class="w">    </span><span class="o">--</span><span class="n">quantization_overrides</span><span class="w"> </span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">mnt</span><span class="o">/</span><span class="n">workspace</span><span class="o">/</span><span class="n">layer_output_dump</span><span class="o">/</span><span class="n">vit_base_16_224</span><span class="p">.</span><span class="n">encodings</span>
<span class="w">    </span><span class="o">--</span><span class="n">engine</span><span class="w"> </span><span class="n">QNN</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">verbose</span>
</pre></div>
</div>
<p><strong>Output</strong></p>
<p>The program creates layerwise_snooping and wrapper output directories as well as framework_diagnosis if a golden reference is not provided (like described for cumulative-layerwise).</p>
<ul class="simple">
<li><p>layerwise_snooping directory – Contains each single layer model outputs obtained from the inference engine stage stored
in separate directories and the final report named layerwise.csv which contains verifier scores for each layer model.
Users can identify layers with the most deviating scores as problematic nodes.</p></li>
<li><p>wrapper directory – Contains log.txt which stores the full logs for the run.</p></li>
<li><p>The output .csv is similar to the cumulative-layerwise output, but the original outputs column will not be present in layerwise snooping,
since we are not dealing with final outputs of the model.</p></li>
</ul>
<p><strong>Debugging Accuracy issue with Quantized model using Cumulative Layerwise Snooping</strong></p>
<ul>
<li><p>With quantized models, it is expected to have some mismatch at most data intensive layers - arising due to quantization error.</p></li>
<li><p>The debugger can be used to identify operators which are most sensitive with high verifier score and run those at higher precision to improve overall accuracy.</p></li>
<li><p>The sensitivity is determined by the verifier score seen at that layer regarding the reference platform (like ONNXRT).</p></li>
<li><p>Note that Cumulative-layerwise debugging takes considerable time as the partitioned model shall be quantized and compiled at every layer that does not have a 100% match with reference.</p></li>
<li><p>Below is one strategy to debug larger models:</p>
<blockquote>
<div><ul>
<li><p>Run Oneshot-layerwise on the model which helps to identify the starting point of sensitivity in the model.</p></li>
<li><p>Run Cumulative-layerwise at different parts of the model using start-layer and end-layer options (if the model has 100 nodes, use start layer at starting node from Oneshot-layerwise run
and end layer at the 25th node for run 1, start layer at 26th and end layer at 50th node for run 2, start layer at 51st node and end layer at 75th node for run 3 .. and so on).The final
reports of all runs help to identify the most sensitive layers in the model. Let’s say node A,B,C have high verifier scores which indicates high sensitivity</p>
<blockquote>
<div><ul class="simple">
<li><p>Run the original model with those specific layers (A/B/C - one at a time or combinations) in FP16 and observe the improvement in accuracy.</p></li>
</ul>
</div></blockquote>
</li>
</ul>
</div></blockquote>
</li>
</ul>
<p><strong>Debugging Accuracy issue for models exhibiting Accuracy discrepancy between golden reference (for ex. - AIMET/framework runtime output) vs target output using Layerwise Snooping</strong></p>
<ul class="simple">
<li><dl class="simple">
<dt>One of the popular usecase for layerwise snooping is debugging accuracy difference between AIMET vs target</dt><dd><ul>
<li><p>Though we are creating an exact simulation of hardware using tools like AIMET, still it is expected to have a very minute mismatch due to environment differences.
This can be because simulation executes on GPU FP32 kernels and is simulating noise rather than actual execution on integer kernels in the case of hardware execution.</p></li>
<li><p>If we have a higher deviation between simulation and hardware, then layerwise snooping could be used to point out to the nodes having higher deviations.
The nodes showing higher deviation as per layerwise.csv can be identified as the erroneous nodes.</p></li>
</ul>
</dd>
</dl>
</li>
<li><p>Other usecases include debugging Framework runtime’s FP32 output vs target INT16 output deviations.</p></li>
</ul>
</div>
</div>
<div class="section" id="qnn-platform-validator">
<h3>qnn-platform-validator<a class="headerlink" href="#qnn-platform-validator" title="Permalink to this heading">¶</a></h3>
<p>qnn-platform-validator checks the QNN compatibility/capability of a device. The output is saved in a CSV file in the
“output” directory, in a csv format. Basic logs are also displayed on the console.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="nl">DESCRIPTION</span><span class="p">:</span>
<span class="o">------------</span>
<span class="n">Helper</span><span class="w"> </span><span class="n">script</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">set</span><span class="w"> </span><span class="n">up</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">environment</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">launch</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">qnn</span><span class="o">-</span><span class="n">platform</span><span class="o">-</span>
<span class="n">validator</span><span class="w"> </span><span class="n">executable</span><span class="p">.</span>

<span class="n">REQUIRED</span><span class="w"> </span><span class="n">ARGUMENTS</span><span class="o">:</span>
<span class="o">-------------------</span>
<span class="o">--</span><span class="n">backend</span><span class="w">            </span><span class="o">&lt;</span><span class="n">BACKEND</span><span class="o">&gt;</span><span class="w">          </span><span class="n">Specify</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">backend</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">validate</span><span class="o">:</span><span class="w"> </span><span class="o">&lt;</span><span class="n">gpu</span><span class="o">&gt;</span><span class="p">,</span><span class="w"> </span><span class="o">&lt;</span><span class="n">dsp</span><span class="o">&gt;</span>
<span class="w">                                        </span><span class="o">&lt;</span><span class="n">all</span><span class="o">&gt;</span><span class="p">.</span>

<span class="o">--</span><span class="n">directory</span><span class="w">          </span><span class="o">&lt;</span><span class="kt">DIR</span><span class="o">&gt;</span><span class="w">              </span><span class="n">Path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">root</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">unpacked</span><span class="w"> </span><span class="n">SDK</span><span class="w"> </span><span class="n">directory</span><span class="w"> </span><span class="n">containing</span>
<span class="w">                                        </span><span class="n">the</span><span class="w"> </span><span class="n">executable</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">library</span><span class="w"> </span><span class="n">files</span>

<span class="o">--</span><span class="n">dsp_type</span><span class="w">           </span><span class="o">&lt;</span><span class="n">DSP_VERSION</span><span class="o">&gt;</span><span class="w">      </span><span class="n">Specify</span><span class="w"> </span><span class="n">DSP</span><span class="w"> </span><span class="n">variant</span><span class="o">:</span><span class="w"> </span><span class="n">v66</span><span class="w"> </span><span class="n">or</span><span class="w"> </span><span class="n">v68</span>

<span class="n">OPTIONALS</span><span class="w"> </span><span class="n">ARGUMENTS</span><span class="o">:</span>
<span class="o">--------------------</span>
<span class="o">--</span><span class="n">buildVariant</span><span class="w">       </span><span class="o">&lt;</span><span class="n">TOOLCHAIN</span><span class="o">&gt;</span><span class="w">        </span><span class="n">Specify</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">build</span><span class="w"> </span><span class="n">variant</span>
<span class="w">                                        </span><span class="n">aarch64</span><span class="o">-</span><span class="n">android</span><span class="w"> </span><span class="n">or</span><span class="w"> </span><span class="n">aarch64</span><span class="o">-</span><span class="n">windows</span><span class="o">-</span><span class="n">msvc</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">validated</span><span class="p">.</span>
<span class="w">                                        </span><span class="nl">Default</span><span class="p">:</span><span class="w"> </span><span class="n">aarch64</span><span class="o">-</span><span class="n">android</span>

<span class="o">--</span><span class="n">testBackend</span><span class="w">                           </span><span class="n">Runs</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">small</span><span class="w"> </span><span class="n">program</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">runtime</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">Checks</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">QNN</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">supported</span><span class="w"> </span><span class="k">for</span>
<span class="w">                                        </span><span class="n">backend</span><span class="p">.</span>

<span class="o">--</span><span class="n">deviceId</span><span class="w">           </span><span class="o">&lt;</span><span class="n">DEVICE_ID</span><span class="o">&gt;</span><span class="w">        </span><span class="n">Uses</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">device</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">running</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">adb</span><span class="w"> </span><span class="n">command</span><span class="p">.</span>
<span class="w">                                        </span><span class="n">Defaults</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">first</span><span class="w"> </span><span class="n">device</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">adb</span><span class="w"> </span><span class="n">devices</span><span class="w"> </span><span class="n">list</span><span class="p">..</span>

<span class="o">--</span><span class="n">coreVersion</span><span class="w">                           </span><span class="n">Outputs</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">version</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">runtime</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">present</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">target</span><span class="p">.</span>

<span class="o">--</span><span class="n">libVersion</span><span class="w">                            </span><span class="n">Outputs</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">library</span><span class="w"> </span><span class="n">version</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">runtime</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">present</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">target</span><span class="p">.</span>

<span class="o">--</span><span class="n">targetPath</span><span class="w">          </span><span class="o">&lt;</span><span class="kt">DIR</span><span class="o">&gt;</span><span class="w">             </span><span class="n">The</span><span class="w"> </span><span class="n">path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">used</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">device</span><span class="p">.</span>
<span class="w">                                        </span><span class="n">Defaults</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">platformValidator</span>

<span class="o">--</span><span class="n">remoteHost</span><span class="w">         </span><span class="o">&lt;</span><span class="n">REMOTEHOST</span><span class="o">&gt;</span><span class="w">       </span><span class="n">Run</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">remote</span><span class="w"> </span><span class="n">host</span><span class="w"> </span><span class="n">through</span><span class="w"> </span><span class="n">remote</span><span class="w"> </span><span class="n">adb</span><span class="w"> </span><span class="n">server</span><span class="p">.</span>
<span class="w">                                        </span><span class="n">Defaults</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">localhost</span><span class="p">.</span>

<span class="o">--</span><span class="n">debug</span><span class="w">                                 </span><span class="n">Set</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">turn</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">Debug</span><span class="w"> </span><span class="n">log</span>
</pre></div>
</div>
<div class="line-block">
<div class="line">Additional details:</div>
</div>
<ul>
<li><div class="line-block">
<div class="line">The following files need to be pushed to the device for the DSP to pass validator test.</div>
<div class="line">Note that the stub and skel libraries are specific to the DSP architecture version(e.g., v73):</div>
</div>
<blockquote>
<div><div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>// Android
bin/aarch64-android/qnn-platform-validator
lib/aarch64-android/libQnnHtpV73CalculatorStub.so
lib/hexagon-${DSP_ARCH}/unsigned/libCalculator_skel.so

// Windows
bin/aarch64-windows-msvc/qnn-platform-validator.exe
lib/aarch64-windows-msvc/QnnHtpV73CalculatorStub.dll
lib/hexagon-${DSP_ARCH}/unsigned/libCalculator_skel.so
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>The following example pushes the aarch64-android variant to /data/local/tmp/platformValidator</p>
<blockquote>
<div><div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>adb push $SNPE_ROOT/bin/aarch64-android/snpe-platform-validator /data/local/tmp/platformValidator/bin/qnn-platform-validator
adb push $SNPE_ROOT/lib/aarch64-android/ /data/local/tmp/platformValidator/lib
adb push $SNPE_ROOT/lib/dsp /data/local/tmp/platformValidator/dsp
</pre></div>
</div>
</div></blockquote>
</li>
</ul>
<div class="line-block">
<div class="line"><br /></div>
</div>
</div>
<div class="section" id="qnn-profile-viewer">
<h3>qnn-profile-viewer<a class="headerlink" href="#qnn-profile-viewer" title="Permalink to this heading">¶</a></h3>
<p>The <strong>qnn-profile-viewer</strong> tool is used to parse profiling data that is generated using
<strong>qnn-net-run</strong>. Additionally, the same data can be saved to a csv file.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="nl">usage</span><span class="p">:</span><span class="w"> </span><span class="n">qnn</span><span class="o">-</span><span class="n">profile</span><span class="o">-</span><span class="n">viewer</span><span class="w"> </span><span class="o">--</span><span class="n">input_log</span><span class="w"> </span><span class="n">PROFILING_LOG</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">help</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">output</span><span class="o">=</span><span class="n">CSV_FILE</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">extract_opaque_objects</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">reader</span><span class="o">=</span><span class="n">CUSTOM_READER_SHARED_LIB</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">schematic</span><span class="o">=</span><span class="n">SCHEMATIC_BINARY</span><span class="p">]</span>

<span class="n">Reads</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">profiling</span><span class="w"> </span><span class="n">log</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">outputs</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">contents</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">stdout</span>

<span class="nl">Note</span><span class="p">:</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">IPS</span><span class="w"> </span><span class="n">calculation</span><span class="w"> </span><span class="n">takes</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">following</span><span class="w"> </span><span class="n">into</span><span class="w"> </span><span class="n">account</span><span class="o">:</span><span class="w"> </span><span class="n">graph</span><span class="w"> </span><span class="n">execute</span><span class="w"> </span><span class="n">time</span><span class="p">,</span><span class="w"> </span><span class="n">tensor</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">IO</span><span class="w"> </span><span class="n">time</span><span class="p">,</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">misc</span><span class="p">.</span><span class="w"> </span><span class="n">time</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">quantization</span><span class="p">,</span><span class="w"> </span><span class="n">callbacks</span><span class="p">,</span><span class="w"> </span><span class="n">etc</span><span class="p">.</span>

<span class="n">required</span><span class="w"> </span><span class="n">arguments</span><span class="o">:</span>
<span class="w">  </span><span class="o">--</span><span class="n">input_log</span><span class="w">                     </span><span class="n">PROFILING_LOG</span>
<span class="w">                                  </span><span class="n">Profiling</span><span class="w"> </span><span class="n">log</span><span class="w"> </span><span class="n">file</span><span class="p">.</span>

<span class="n">optional</span><span class="w"> </span><span class="n">arguments</span><span class="o">:</span>
<span class="w">  </span><span class="o">--</span><span class="n">output</span><span class="w">                        </span><span class="n">PATH</span>
<span class="w">                                  </span><span class="n">Output</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">processed</span><span class="w"> </span><span class="n">profiling</span><span class="w"> </span><span class="n">data</span><span class="p">.</span><span class="w"> </span><span class="n">File</span><span class="w"> </span><span class="n">formats</span><span class="w"> </span><span class="n">vary</span><span class="w"> </span><span class="n">depending</span><span class="w"> </span><span class="n">upon</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">reader</span><span class="w"> </span><span class="n">used</span>
<span class="w">                                  </span><span class="p">(</span><span class="n">see</span><span class="w"> </span><span class="o">--</span><span class="n">reader</span><span class="p">).</span><span class="w"> </span><span class="n">If</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">provided</span><span class="p">,</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">created</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">help</span><span class="w">                          </span><span class="n">Displays</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">help</span><span class="w"> </span><span class="n">message</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">reader</span><span class="w">                        </span><span class="n">CUSTOM_READER_SHARED_LIB</span>
<span class="w">                                  </span><span class="n">Path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">reader</span><span class="w"> </span><span class="n">library</span><span class="p">.</span><span class="w"> </span><span class="n">If</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">specified</span><span class="p">,</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">default</span><span class="w"> </span><span class="n">reader</span><span class="w"> </span><span class="n">outputs</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">CSV</span><span class="w"> </span><span class="n">file</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">schematic</span><span class="w">                     </span><span class="n">SCHEMATIC_BINARY</span>
<span class="w">                                  </span><span class="n">Path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">schematic</span><span class="w"> </span><span class="n">binary</span><span class="w"> </span><span class="n">file</span><span class="p">.</span>
<span class="w">                                  </span><span class="n">Please</span><span class="w"> </span><span class="n">note</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">option</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">specific</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">QnnHtpOptraceProfilingReader</span><span class="w"> </span><span class="n">library</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">version</span><span class="w">                       </span><span class="n">Displays</span><span class="w"> </span><span class="n">version</span><span class="w"> </span><span class="n">information</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">extract_opaque_objects</span><span class="w">        </span><span class="n">Specifies</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">opaque</span><span class="w"> </span><span class="n">objects</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">dumped</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="n">files</span>
</pre></div>
</div>
</div>
<div class="section" id="qnn-netron-beta">
<h3>qnn-netron (<a class="reference internal" href="#qnn-ai-tools-beta-note"><span class="std std-ref">Beta</span></a>)<a class="headerlink" href="#qnn-netron-beta" title="Permalink to this heading">¶</a></h3>
<div class="section" id="overview">
<h4>Overview<a class="headerlink" href="#overview" title="Permalink to this heading">¶</a></h4>
<p>QNN Netron tool is making model debugging and visualization less daunting. qnn-netron is an extension of the
<a class="reference external" href="https://github.com/lutzroeder/netron">netron</a> graph tool. It provides for easier graph debugging and convenient runtime information.
There are currently two key functionalities of the tool:</p>
<ol class="arabic simple">
<li><p>The Visualize section allows customers to view their desired models after using the QNN Converter by importing the
JSON representation of the model</p></li>
<li><p>The Diff section allows customers to run networks of their choosing on different
runtimes in order to compare network accuracy and performance</p></li>
</ol>
</div>
<div class="section" id="launching-tool">
<h4>Launching Tool<a class="headerlink" href="#launching-tool" title="Permalink to this heading">¶</a></h4>
<p><strong>Dependencies</strong></p>
<p>The QNN netron tool leverages electron JS framework for building GUI frontend and depends on npm/node_js to be available
in system. Additionally, python libraries for accuracy analysis are required by backend of tool. A convenient script is
available in the QNN SDK to download necessary dependencies for building and running the tool.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="cp"># Note: following command should be run as administrator/root to be able to install system libraries</span>
<span class="n">$</span><span class="w"> </span><span class="n">sudo</span><span class="w"> </span><span class="n">bash</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">bin</span><span class="o">/</span><span class="n">check</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">dependency</span><span class="p">.</span><span class="n">sh</span>
<span class="n">$</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">bin</span><span class="o">/</span><span class="n">check</span><span class="o">-</span><span class="n">python</span><span class="o">-</span><span class="n">dependency</span>
</pre></div>
</div>
<p><strong>Launching Application</strong></p>
<p><cite>qnn-netron</cite> script is used to be able to launch the QNN Netron application. This script:</p>
<ol class="arabic simple">
<li><p>Clones vanilla netron git project</p></li>
<li><p>Applies custom patches for enabling Netron for QNN</p></li>
<li><p>Build the npm project</p></li>
<li><p>Launches application</p></li>
</ol>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="n">qnn</span><span class="o">-</span><span class="n">netron</span><span class="w"> </span><span class="o">-</span><span class="n">h</span>
<span class="nl">usage</span><span class="p">:</span><span class="w"> </span><span class="n">qnn</span><span class="o">-</span><span class="n">netron</span><span class="w"> </span><span class="p">[</span><span class="o">-</span><span class="n">h</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">-</span><span class="n">w</span><span class="w"> </span><span class="o">&lt;</span><span class="n">working_dir</span><span class="o">&gt;</span><span class="p">]</span>
<span class="n">Script</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">build</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">launch</span><span class="w"> </span><span class="n">QNN</span><span class="w"> </span><span class="n">Netron</span><span class="w"> </span><span class="n">tool</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">visualizing</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">running</span><span class="w"> </span><span class="n">analysis</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">Qnn</span><span class="w"> </span><span class="n">Models</span><span class="p">.</span>

<span class="n">Optional</span><span class="w"> </span><span class="n">argument</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="o">:</span>
<span class="w"> </span><span class="o">-</span><span class="n">w</span><span class="w"> </span><span class="o">&lt;</span><span class="n">working_dir</span><span class="o">&gt;</span><span class="w">                      </span><span class="n">Location</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">building</span><span class="w"> </span><span class="n">QNN</span><span class="w"> </span><span class="n">Netron</span><span class="w"> </span><span class="n">tool</span><span class="p">.</span><span class="w"> </span><span class="n">Default</span><span class="o">:</span><span class="w"> </span><span class="n">current_dir</span>


<span class="cp"># To build and run application use</span>
<span class="n">$</span><span class="w"> </span><span class="n">qnn</span><span class="o">-</span><span class="n">netron</span><span class="w"> </span><span class="o">-</span><span class="n">w</span><span class="w"> </span><span class="o">&lt;</span><span class="n">my_working_dir</span><span class="o">&gt;</span>
</pre></div>
</div>
</div>
<div class="section" id="qnn-netron-visualize-deep-dive">
<h4>QNN Netron Visualize Deep Dive<a class="headerlink" href="#qnn-netron-visualize-deep-dive" title="Permalink to this heading">¶</a></h4>
<p>First, the user is prompted to open a JSON file that represents their converted model. This JSON comes from the
converter tool. Please refer to this <a class="reference internal" href="converters.html#overview"><span class="std std-ref">Overview</span></a> for more details.</p>
<div class="figure align-default">
<img alt="../_static/resources/landing_page_netron.jpg" src="../_static/resources/landing_page_netron.jpg" />
</div>
<p>Once the file is loaded into the tool, the graph should be displayed in the UI as shown below:</p>
<p>After loading in the model, the user can click on any of the nodes and a side pop-up section will display node
information such as the type and name as well as vital parameter information such as inputs and outputs
(datatypes, encodings, and shapes)</p>
<div class="figure align-default">
<img alt="../_static/resources/netron_detailed_nodes_visualization.jpg" src="../_static/resources/netron_detailed_nodes_visualization.jpg" />
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
</div>
<div class="section" id="netron-diff-customization-deep-dive">
<h4>Netron Diff Customization Deep Dive<a class="headerlink" href="#netron-diff-customization-deep-dive" title="Permalink to this heading">¶</a></h4>
<p><strong>Limitations</strong></p>
<ol class="arabic simple">
<li><p>Diff Tool comparison between source framework goldens only works for framework goldens that are spatial first axis order. (NHWC)</p></li>
<li><p>For usecases where source framework golden is used for comparison, Diff Tool is only tested to work for tensorflow and tensorflow variant frameworks.</p></li>
</ol>
<p>In order for the user to open the Diff Customization tool, they can either click file and then “Open Diff…” or on
tool startup by clicking “Diff…” as shown below:</p>
<div class="figure align-default">
<img alt="../_static/resources/netron_diff_ui_opening.jpg" src="../_static/resources/netron_diff_ui_opening.jpg" />
</div>
<div class="figure align-default">
<img alt="../_static/resources/open_diff_tool_netron.png" src="../_static/resources/open_diff_tool_netron.png" />
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>Upon launch of the Diff Customization tool, at the top, the user is prompted to select a use case for the tool.
There are 3 options to choose from:</p>
<div class="figure align-default">
<img alt="../_static/resources/use_case_netron.png" src="../_static/resources/use_case_netron.png" />
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>For the purposes of this documentation, only inference vs inference will be detailed. The setup procedure for the other
use cases is similar. The other two use cases are explained below:</p>
<ol class="arabic simple">
<li><p>Golden vs Inference: Used to test inference run using goldens from a particular ML framework and comparing against
the output of a QNN backend</p></li>
<li><p>Output vs Output: Used to test existing inference results against ML framework goldens OR used to test differences
between two existing inference results</p></li>
<li><p>Inference Vs Inference: Used to test inference between two converted QNN models or the same QNN model on different
QNN backends</p></li>
</ol>
</div>
<div class="section" id="inference-vs-inference">
<h4>Inference vs Inference<a class="headerlink" href="#inference-vs-inference" title="Permalink to this heading">¶</a></h4>
<p>If this use case is selected, the user is presented with various form fields for the purposes of running two
jobs asynchronously with the option of choosing different runtimes for each QNN network being run.</p>
<div class="figure align-center">
<img alt="qnn-netron" src="../_static/resources/inferencevinference.png" />
</div>
<p>A more detailed view of what the user is prompted is displayed below:</p>
<div class="figure align-center">
<img alt="qnn-netron" src="../_static/resources/inferencedetailed.png" />
</div>
<p>In order to execute the networks, the user has two options:</p>
<p><strong>Running on Host machine</strong></p>
<p>When the Target Device is selected as “host”, the user can only use the CPU as a runtime. In addition, the user can
only select “x86_64-linux-clang” as the architecture in this use case.</p>
<div class="figure align-center">
<img alt="qnn-netron" src="../_static/resources/netron_host_use_case.png" />
</div>
<p><strong>Running On-Device</strong></p>
<p>When the Target Device is selected as “on-device”, a Device ID is required to connect to the device via adb.
Thereafter, the user can select any of the three QNN backend runtimes available (CPU, GPU, or DSPv[68, 69, 73]) and the user
can select architecture “aarch64-android”</p>
<div class="figure align-center">
<img alt="qnn-netron" src="../_static/resources/netron_device_use_case.png" />
</div>
<p>After choosing the desired target device and runtime configurations, the rest of the fields are explained in detail
below:</p>
<div class="line-block">
<div class="line"><br /></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Users are able to click again and change the location to any of the path fields</p>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<table class="docutils align-default">
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Setup Parameters</p></th>
<th class="head"><p>Configurations to Select</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>The options for what verifier to run on the outputs of the model are (See Note below table for custom verifier
(accuracy + performance) thresholds and see table below for providing custom accuracy verifier hyperparameters):</p></td>
<td><p>RtolAtol, AdjustedRtolAtol, TopK, MeanIOU, L1Error, CosineSimilarity, MSE, SQNR</p></td>
</tr>
<tr class="row-odd"><td><p>Model JSON</p></td>
<td><p>upload &lt;model&gt;_net.json file that was outputted from the QNN converters.</p></td>
</tr>
<tr class="row-even"><td><p>Model Cpp</p></td>
<td><p>upload &lt;model&gt;.cpp that was outputted from the QNN converters.</p></td>
</tr>
<tr class="row-odd"><td><p>Model Bin</p></td>
<td><p>upload &lt;model&gt;.bin that was outputted from the QNN converters.</p></td>
</tr>
<tr class="row-even"><td><p>NDK Path</p></td>
<td><p>upload the path to your Android NDK</p></td>
</tr>
<tr class="row-odd"><td><p>Devices Engine Path</p></td>
<td><p>upload the path to the top-level of the unzipped qnn-sdk</p></td>
</tr>
<tr class="row-even"><td><p>Input List</p></td>
<td><p>provide a path to the input file for the model</p></td>
</tr>
<tr class="row-odd"><td><p>Save Run Configurations</p></td>
<td><p>provide a location where the inference and runtime results from the Diff customization tool will be stored</p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Users have the option of providing a custom accuracy and performance verifier threshold when running diff.
A custom accuracy verifier threshold can be provided for any of the accuracy verifiers. By default the
verifier thresholds are 0.01. The custom thresholds can be provided in the text boxes labelled “Accuracy
Threshold” and “Perf Threshold”.</p>
</div>
<p>Users now have the option to enter accuracy verifier specific hyperparameters inside textboxes. The Default Values are displayed
inside the text-boxes and can be customized as per user needs. The table below highlights the hyperparameters for each
verifier that can be customized.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Verifier</p></th>
<th class="head"><p>Hyperparameters</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>AdjustedRtolAtol</p></td>
<td><p>Number of Levels</p></td>
</tr>
<tr class="row-odd"><td><p>RtolAtol</p></td>
<td><p>Rtol Margin, Atol Margin</p></td>
</tr>
<tr class="row-even"><td><p>Topk</p></td>
<td><p>K, Ordered</p></td>
</tr>
<tr class="row-odd"><td><p>MeanIOU</p></td>
<td><p>Background Classification</p></td>
</tr>
<tr class="row-even"><td><p>L1Error</p></td>
<td><p>Multiplier, Scale</p></td>
</tr>
<tr class="row-odd"><td><p>CosineSimilarity</p></td>
<td><p>Multiplier, Scale</p></td>
</tr>
<tr class="row-even"><td><p>MSE (Mean Square Error)</p></td>
<td><p>N/A</p></td>
</tr>
<tr class="row-odd"><td><p>SQNR (Signal-To-Noise Ratio)</p></td>
<td><p>N/A</p></td>
</tr>
</tbody>
</table>
<p>Below is an example of what the fields should look like once filled to completion:</p>
<div class="figure align-center">
<img alt="qnn-netron" src="../_static/resources/inferencefinalview.png" />
</div>
<p>After running the Diff Customization tool, the output directories/files should be present in the working directory
file path provided in the last field</p>
<div class="figure align-center">
<img alt="qnn-netron" src="../_static/resources/output_directories_netron_diff.png" />
</div>
</div>
<div class="section" id="results-and-outputs">
<h4>Results and Outputs:<a class="headerlink" href="#results-and-outputs" title="Permalink to this heading">¶</a></h4>
<p>After pressing the Run button as mentioned above, the visualization of the network should pop-up. Nodes will be
highlighted if there are any accuracy and/or performance variations. Clicking on each node will show more information
about the accuracy and performance diff information as shown below.</p>
<div class="figure align-center">
<img alt="qnn-netron" src="../_static/resources/netron_diff_overall_view.png" />
</div>
</div>
<div class="section" id="performance-and-accuracy-diff-visualizations">
<h4>Performance and Accuracy Diff Visualizations:<a class="headerlink" href="#performance-and-accuracy-diff-visualizations" title="Permalink to this heading">¶</a></h4>
<div class="figure align-center">
<img alt="qnn-netron" src="../_static/resources/netron_accuracy_performance_diff.png" />
</div>
<p>As seen above, the performance and accuracy diff information is shown under the Diff section of any given node.
The color of the node boundary in the viewer represents whether a performance or accuracy error (above the default
verifier threshold of 0.01) was reported. For example, in the Conv2d node shown below, there are two boundaries of
orange and red indicating that this node has both an accuracy and performance difference across the runs. The
FullyConnected node shown only has a yellow boundary indicating that only a performance difference was found.</p>
<div class="figure align-center">
<img alt="qnn-netron" src="../_static/resources/netron_node_overlay_both.png" />
</div>
<div class="figure align-center">
<img alt="qnn-netron" src="../_static/resources/netron_node_overlay_perf.png" />
</div>
</div>
<div class="section" id="qnn-netron-diff-navigation">
<h4>QNN Netron Diff Navigation<a class="headerlink" href="#qnn-netron-diff-navigation" title="Permalink to this heading">¶</a></h4>
<p>QNN Netron has the ability to locate the first node in the graph with any performance or accuracy diffs. When the
user clicks on the next and previous arrows, the visualization of the graph will zoom into the desired node with the
first performance or accuracy difference. This makes model debugging much easier for larger models as the user doesn’t
have to look for the nodes themselves to find where the network performance and accuracy errors starts to diverge.</p>
<div class="figure align-center">
<img alt="qnn-netron" src="../_static/resources/netron_error_first_node_selected.png" />
</div>
</div>
</div>
<div class="section" id="qnn-context-binary-utility">
<h3>qnn-context-binary-utility<a class="headerlink" href="#qnn-context-binary-utility" title="Permalink to this heading">¶</a></h3>
<p>The <strong>qnn-context-binary-utility</strong> tool validates and serializes the metadata of context binary into a json file. This json file
can then be used for inspecting the context binary aiding in debugging. A QNN context can be serialized to binary
using QNN APIs or qnn-context-binary-generator tool.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="nl">usage</span><span class="p">:</span><span class="w"> </span><span class="n">qnn</span><span class="o">-</span><span class="n">context</span><span class="o">-</span><span class="n">binary</span><span class="o">-</span><span class="n">utility</span><span class="w"> </span><span class="o">--</span><span class="n">context_binary</span><span class="w"> </span><span class="n">CONTEXT_BINARY_FILE</span><span class="w"> </span><span class="o">--</span><span class="n">json_file</span><span class="w"> </span><span class="n">JSON_FILE_NAME</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">help</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="o">--</span><span class="n">version</span><span class="p">]</span>

<span class="n">Reads</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">serialized</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="n">binary</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">validates</span><span class="w"> </span><span class="n">its</span><span class="w"> </span><span class="n">metadata</span><span class="p">.</span>
<span class="n">If</span><span class="w"> </span><span class="o">--</span><span class="n">json_file</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">provided</span><span class="p">,</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="n">outputs</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">metadata</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">json</span><span class="w"> </span><span class="n">file</span>

<span class="n">required</span><span class="w"> </span><span class="n">arguments</span><span class="o">:</span>
<span class="w">  </span><span class="o">--</span><span class="n">context_binary</span><span class="w">  </span><span class="n">CONTEXT_BINARY_FILE</span>
<span class="w">                    </span><span class="n">Path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">cached</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="n">binary</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">which</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">binary</span><span class="w"> </span><span class="n">info</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">extracted</span>
<span class="w">                    </span><span class="n">and</span><span class="w"> </span><span class="n">written</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">json</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">json_file</span><span class="w">       </span><span class="n">JSON_FILE_NAME</span>
<span class="w">                    </span><span class="n">Provide</span><span class="w"> </span><span class="n">path</span><span class="w"> </span><span class="n">along</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="o">&lt;</span><span class="kt">DIR</span><span class="o">&gt;/&lt;</span><span class="n">FILE_NAME</span><span class="o">&gt;</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">serialize</span>
<span class="w">                    </span><span class="n">context</span><span class="w"> </span><span class="n">binary</span><span class="w"> </span><span class="n">info</span><span class="w"> </span><span class="n">into</span><span class="w"> </span><span class="n">json</span><span class="p">.</span>
<span class="w">                    </span><span class="n">The</span><span class="w"> </span><span class="n">directory</span><span class="w"> </span><span class="n">path</span><span class="w"> </span><span class="n">must</span><span class="w"> </span><span class="n">exist</span><span class="p">.</span><span class="w"> </span><span class="n">File</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">FILE_NAME</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">created</span><span class="w"> </span><span class="n">at</span><span class="w"> </span><span class="kt">DIR</span><span class="p">.</span>

<span class="n">optional</span><span class="w"> </span><span class="n">arguments</span><span class="o">:</span>
<span class="w">  </span><span class="o">--</span><span class="n">help</span><span class="w">          </span><span class="n">Displays</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">help</span><span class="w"> </span><span class="n">message</span><span class="p">.</span>

<span class="w">  </span><span class="o">--</span><span class="n">version</span><span class="w">       </span><span class="n">Displays</span><span class="w"> </span><span class="n">version</span><span class="w"> </span><span class="n">information</span><span class="p">.</span>
</pre></div>
</div>
</div>
<div class="section" id="accuracy-evaluator-plugins">
<h3>Accuracy Evaluator plugins<a class="headerlink" href="#accuracy-evaluator-plugins" title="Permalink to this heading">¶</a></h3>
<div class="section" id="file-based-plugins">
<h4>File-based plugins<a class="headerlink" href="#file-based-plugins" title="Permalink to this heading">¶</a></h4>
<p>This section lists the built-in file-based plugins.</p>
<div class="section" id="dataset-plugins">
<h5>Dataset plugins<a class="headerlink" href="#dataset-plugins" title="Permalink to this heading">¶</a></h5>
<p><strong>create_squad_examples</strong> - Extracts examples from given squad dataset file and save them to a file.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>squad_version</p></td>
<td><p>Squad version 1 or 2</p></td>
<td><p>Integer</p></td>
<td><p>1</p></td>
</tr>
</tbody>
</table>
<p><strong>filter_dataset</strong> - Filters the dataset including the input list, calibration and annotation files.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>max_inputs</p></td>
<td><p>Maximum number of inputs in inputlist to be considered for execution</p></td>
<td><p>Integer</p></td>
<td><p>Mandatory</p></td>
</tr>
<tr class="row-odd"><td><p>max_calib</p></td>
<td><p>Maximum number of inputs in calibration to be considered for execution</p></td>
<td><p>Integer</p></td>
<td><p>Mandatory</p></td>
</tr>
<tr class="row-even"><td><p>random</p></td>
<td><p>Shuffles the inputlist and calibration files</p></td>
<td><p>Boolean</p></td>
<td><p>False</p></td>
</tr>
</tbody>
</table>
<p><strong>gpt2_tokenizer</strong> - Tokenizes data from files using GPT2TokenizerFast.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>vocab_file</p></td>
<td><p>Path to the vocabulary file</p></td>
<td><p>String</p></td>
<td><p>Mandatory</p></td>
</tr>
<tr class="row-odd"><td><p>merges_file</p></td>
<td><p>Path to the merges file</p></td>
<td><p>String</p></td>
<td><p>Mandatory</p></td>
</tr>
<tr class="row-even"><td><p>seq_length</p></td>
<td><p>Sequence length for the generated model inputs</p></td>
<td><p>Integer</p></td>
<td><p>Mandatory</p></td>
</tr>
<tr class="row-odd"><td><p>past_seq_length</p></td>
<td><p>Sequence length for the “past” inputs</p></td>
<td><p>Integer</p></td>
<td><p>Mandatory</p></td>
</tr>
<tr class="row-even"><td><p>past_shape</p></td>
<td><p>Shape of the ‘past’ inputs</p></td>
<td><p>List</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>num_past</p></td>
<td><p>Number of ‘past’ inputs</p></td>
<td><p>Integer</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
<p><strong>split_txt_data</strong> - Saves individual text files for each line present in the given input text file.</p>
</div>
<div class="section" id="preprocessing-plugins">
<h5>Preprocessing plugins<a class="headerlink" href="#preprocessing-plugins" title="Permalink to this heading">¶</a></h5>
<p><strong>centernet_preproc</strong> - Performs preprocessing on CenterNet dataset examples.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>dims</p></td>
<td><p>Height and width; comma delimited, e.g., 416,416</p></td>
<td><p>String</p></td>
<td><p>Mandatory</p></td>
</tr>
<tr class="row-odd"><td><p>scale</p></td>
<td><p>Scale factor for image</p></td>
<td><p>Float</p></td>
<td><p>1.0</p></td>
</tr>
<tr class="row-even"><td><p>fix_res</p></td>
<td><p>Resolution of the image</p></td>
<td><p>Boolean</p></td>
<td><p>True</p></td>
</tr>
<tr class="row-odd"><td><p>pad</p></td>
<td><p>Image padding</p></td>
<td><p>Integer</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
<p><strong>convert_nchw</strong> - Transposes WHC to CHW or CHW to WHC and adds an extra N dimension.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>expand-dims</p></td>
<td><p>Add the Nth dimension</p></td>
<td><p>Boolean</p></td>
<td><p>True</p></td>
</tr>
</tbody>
</table>
<p><strong>create_batch</strong> - Concatenates raw input files into a single file using numpy.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>delete_prior</p></td>
<td><p>To delete prior unbatched data to save space</p></td>
<td><p>Boolean</p></td>
<td><p>True</p></td>
</tr>
<tr class="row-odd"><td><p>truncate</p></td>
<td><p>If num inputs are not a multiple of batch size, then truncate left over inputs in the last batch or not</p></td>
<td><p>Boolean</p></td>
<td><p>False</p></td>
</tr>
</tbody>
</table>
<p><strong>crop</strong> - Center crops an image to the given dimensions using numpy or torchvision based on the library parameter.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>dims</p></td>
<td><p>Height and width; comma delimited, e.g., 640,640</p></td>
<td><p>String</p></td>
<td><p>Mandatory</p></td>
</tr>
<tr class="row-odd"><td><p>library</p></td>
<td><p>Python library used to crop the given input; valid values are: numpy | torchvision</p></td>
<td><p>String</p></td>
<td><p>numpy</p></td>
</tr>
<tr class="row-even"><td><p>typecasting_required</p></td>
<td><p>To convert final output to numpy or not. Note: This option is specific to torchvision library</p></td>
<td><p>Boolean</p></td>
<td><p>True</p></td>
</tr>
</tbody>
</table>
<p><strong>expand_dims</strong> - Adds the N dimension for images, e.g., HWC to NHWC.</p>
<p><strong>image_transformers_input</strong> - Creates input files with image and/or text for image transformer models like ViT and CLIP.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>dims</p></td>
<td><p>Expected processed output dimension in CHW format</p></td>
<td><p>String</p></td>
<td><p>Mandatory</p></td>
</tr>
<tr class="row-odd"><td><p>num_base_class</p></td>
<td><p>Number of base classes in classification; used in the scenario where text input is also provided</p></td>
<td><p>Integer</p></td>
<td><p>Total classes available</p></td>
</tr>
<tr class="row-even"><td><p>num_prompt</p></td>
<td><p>Number of prompts for text classes; used in the scenario where text input is also provided</p></td>
<td><p>Integer</p></td>
<td><p>Total classes available</p></td>
</tr>
<tr class="row-odd"><td><p>image_only</p></td>
<td><p>Data type of raw data</p></td>
<td><p>Boolean</p></td>
<td><p>False</p></td>
</tr>
</tbody>
</table>
<p><strong>normalize</strong> - Normalizes input per the given scheme; data must be of NHWC format.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>library</p></td>
<td><p>Python library used to crop the given input; valid values are: numpy | torchvision</p></td>
<td><p>String</p></td>
<td><p>numpy</p></td>
</tr>
<tr class="row-odd"><td><p>norm</p></td>
<td><p>Normalization factor, all values divided by norm</p></td>
<td><p>float32</p></td>
<td><p>255</p></td>
</tr>
<tr class="row-even"><td><p>means</p></td>
<td><p>Dictionary of means to be subtracted, e.g., {“R”:0.485, “G”:0.456, “B”:0.406}</p></td>
<td><p>RGB dictionary</p></td>
<td><p>{“R”:0, “G”:0, “B”:0}</p></td>
</tr>
<tr class="row-odd"><td><p>std</p></td>
<td><p>Dictionary of std-dev for rescaling the values, e.g., {“R”:0.229, “G”:0.224, “B”:0.225}</p></td>
<td><p>RGB dictionary</p></td>
<td><p>{“R”:1, “G”:1, “B”:1}</p></td>
</tr>
<tr class="row-even"><td><p>channel_order</p></td>
<td><p>Channel order to specify means and std values per channel - RGB | BGR</p></td>
<td><p>String</p></td>
<td><p>RGB</p></td>
</tr>
<tr class="row-odd"><td><p>normalize_first</p></td>
<td><div class="line-block">
<div class="line">To perform normalization before or after mean subtraction and standard deviation.</div>
<div class="line">normalize_first=True means perform normalization before.</div>
<div class="line">Note: torchvision library does not use this option</div>
</div>
</td>
<td><p>Boolean</p></td>
<td><p>True</p></td>
</tr>
<tr class="row-even"><td><p>typecasting_required</p></td>
<td><p>To convert final output to numpy or not. Note: This option is specific to the Torchvision library</p></td>
<td><p>Boolean</p></td>
<td><p>True</p></td>
</tr>
<tr class="row-odd"><td><p>pil_to_tensor_input</p></td>
<td><p>To convert input to tensor before normalization. Note: This option is specific to the Torchvision library</p></td>
<td><p>Boolean</p></td>
<td><p>True</p></td>
</tr>
</tbody>
</table>
<p><strong>onmt_preprocess</strong> - Performs preprocessing on WMT dataset for FasterTransformer OpenNMT model</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>vocab_path</p></td>
<td><p>Path to OpenNMT model vocabulary file (pickle file)</p></td>
<td><p>String</p></td>
<td><p>Mandatory</p></td>
</tr>
<tr class="row-odd"><td><p>src_seq_len</p></td>
<td><p>The maximum total input sequence length</p></td>
<td><p>Integer</p></td>
<td><p>128</p></td>
</tr>
<tr class="row-even"><td><p>skip_sentencepiece</p></td>
<td><p>Skip sentencepiece encoding</p></td>
<td><p>Boolean</p></td>
<td><p>True</p></td>
</tr>
<tr class="row-odd"><td><p>sentencepiece_model_path</p></td>
<td><p>Path to sentencepiece model for WMT dataset (mandatory  when “skip_sentencepiece” is False)</p></td>
<td><p>String</p></td>
<td><p>None</p></td>
</tr>
</tbody>
</table>
<p><strong>pad</strong> - Image padding with constant pad size or based on target dimensions</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>type</p></td>
<td><dl class="simple">
<dt>Type of padding. Valid options:</dt><dd><ul class="simple">
<li><p>constant: Add padding of constant sides on 4 sides (pad_size must be provided)</p></li>
<li><p>target_dims: Add padding based on difference in image size and target size (dims param must be provided)</p></li>
</ul>
</dd>
</dl>
</td>
<td><p>String</p></td>
<td><p>Mandatory</p></td>
</tr>
<tr class="row-odd"><td><p>dims</p></td>
<td><p>Height and width comma delimited, e.g., 416,416 for ‘target-dims’ type of padding</p></td>
<td><p>String</p></td>
<td><p>Mandatory</p></td>
</tr>
<tr class="row-even"><td><p>pad_size</p></td>
<td><p>Size of padding for ‘constant’ type of padding</p></td>
<td><p>Integer</p></td>
<td><p>None</p></td>
</tr>
<tr class="row-odd"><td><p>img_position</p></td>
<td><p>Parameter to specify position of image, either ‘center’ or ‘corner’ (top-left). Padding is added accordingly. Currently used for ‘target_dims’ type padding</p></td>
<td><p>String</p></td>
<td><p>center</p></td>
</tr>
<tr class="row-even"><td><p>color</p></td>
<td><p>Padding value for all planes</p></td>
<td><p>Integer</p></td>
<td><p>114</p></td>
</tr>
</tbody>
</table>
<p><strong>resize</strong> - Resizes an image using the specified library parameter: cv2(Default), pillow or torchvision</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>dims</p></td>
<td><p>Height and width; comma delimited, e.g., 640,640</p></td>
<td><p>String</p></td>
<td><p>Mandatory</p></td>
</tr>
<tr class="row-odd"><td><p>library</p></td>
<td><p>Python library to be used for resizing a given input; valid values are: opencv | pillow | torchvision</p></td>
<td><p>String</p></td>
<td><p>opencv</p></td>
</tr>
<tr class="row-even"><td><p>channel_order</p></td>
<td><p>Convert image to specified channel order. At present this parameter only takes the ‘RGB’ value</p></td>
<td><p>String</p></td>
<td><p>RGB</p></td>
</tr>
<tr class="row-odd"><td><p>interp</p></td>
<td><dl class="simple">
<dt>Interpolation Type. Options:</dt><dd><ul class="simple">
<li><p>bilinear (supported by opencv, Torchvision, pillow)</p></li>
<li><p>area (supported by opencv only)</p></li>
<li><p>nearest (supported by opencv, Torchvision, pillow)</p></li>
<li><p>bicubic (supported by Torchvision, pillow)</p></li>
<li><p>box (supported by pillow only)</p></li>
<li><p>hamming (supported by pillow only)</p></li>
<li><p>lanczos (supported by pillow only)</p></li>
</ul>
</dd>
</dl>
</td>
<td><p>String</p></td>
<td><div class="line-block">
<div class="line">For opencv and torchvision: bilinear</div>
<div class="line">For pillow: bicubic</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>type</p></td>
<td><dl class="simple">
<dt>Type of resize to be done. Note: Torchvision does not use this option. Options:</dt><dd><ul class="simple">
<li><p>letterbox : Used for YOLO models.</p></li>
<li><p>imagenet : Scale followed by resize.</p></li>
<li><p>aspect_ratio : Resize while keeping aspect ratio.</p></li>
<li><p>None : The default behavior is to auto-resize the image to the target dims.</p></li>
</ul>
</dd>
</dl>
</td>
<td><p>String</p></td>
<td><p>auto-resize</p></td>
</tr>
<tr class="row-odd"><td><p>resize_before_typecast</p></td>
<td><p>To resize before or after conversion to target datatype e.g., fp32</p></td>
<td><p>Boolean</p></td>
<td><p>True</p></td>
</tr>
<tr class="row-even"><td><p>typecasting_required</p></td>
<td><p>To convert final output to numpy or not. Note: This option is specific to the Torchvision library</p></td>
<td><p>Boolean</p></td>
<td><p>True</p></td>
</tr>
<tr class="row-odd"><td><p>mean</p></td>
<td><p>Dictionary of means to be subtracted, e.g., {“R”:0.485, “G”:0.456, “B”:0.406}. Note: This option is specific to the Tensorflow library</p></td>
<td><p>RGB dictionary</p></td>
<td><p>{“R”:0, “G”:0, “B”:0}</p></td>
</tr>
<tr class="row-even"><td><p>std</p></td>
<td><p>Dictionary of std-dev for rescaling the values, e.g., {“R”:0.229, “G”:0.224, “B”:0.225}. Note: This option is specific to the Tensorflow library</p></td>
<td><p>RGB dictionary</p></td>
<td><p>{“R”:0, “G”:0, “B”:0}</p></td>
</tr>
<tr class="row-odd"><td><p>normalize_before_resize</p></td>
<td><p>To perform normalization before or after mean subtraction and standard deviation. Note: This option is specific to the Tensorflow library</p></td>
<td><p>Boolean</p></td>
<td><p>False</p></td>
</tr>
<tr class="row-even"><td><p>crop_before_resize</p></td>
<td><p>To perform cropping before resize. Note: This option is specific to the Tensorflow library</p></td>
<td><p>Boolean</p></td>
<td><p>False</p></td>
</tr>
</tbody>
</table>
<p><strong>squad_read</strong> - Reads the SQuAD dataset JSON file. Preprocesses the question-context pairs into features for language models like BERT-Large</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>vocab_path</p></td>
<td><p>Path for local directory containing vocabulary files</p></td>
<td><p>String</p></td>
<td><p>Mandatory</p></td>
</tr>
<tr class="row-odd"><td><p>max_seq_length</p></td>
<td><p>The maximum total input sequence length after WordPiece tokenization. Sequences longer than this will be truncated, and sequences shorter than this will be padded</p></td>
<td><p>Integer</p></td>
<td><p>384</p></td>
</tr>
<tr class="row-even"><td><p>max_query_length</p></td>
<td><p>The maximum number of tokens for the question. Questions longer than this will be truncated to this length</p></td>
<td><p>Integer</p></td>
<td><p>64</p></td>
</tr>
<tr class="row-odd"><td><p>doc_stride</p></td>
<td><p>When splitting up a long document into chunks, how much stride to take between chunks</p></td>
<td><p>Integer</p></td>
<td><p>128</p></td>
</tr>
<tr class="row-even"><td><p>packing_strategy</p></td>
<td><p>Set this flag when using packing strategy for bert based models</p></td>
<td><p>Boolean</p></td>
<td><p>False</p></td>
</tr>
<tr class="row-odd"><td><p>max_sequence_per_pack</p></td>
<td><p>The maximum number of sequences which can be packed together</p></td>
<td><p>Integer</p></td>
<td><p>3</p></td>
</tr>
<tr class="row-even"><td><p>mask_type</p></td>
<td><p>This can take either of three values - ‘None’, ‘Boolean’ or ‘Compressed’ depending on the masking to be done on input_mask</p></td>
<td><p>String</p></td>
<td><p>None</p></td>
</tr>
<tr class="row-odd"><td><p>compressed_mask_length</p></td>
<td><p>Set this value if mask_type is set to compressed</p></td>
<td><p>Integer</p></td>
<td><p>None</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="postprocessing-plugins">
<h5>Postprocessing plugins<a class="headerlink" href="#postprocessing-plugins" title="Permalink to this heading">¶</a></h5>
<p><strong>bert_predict</strong> - Predicts answers for a SQuAD dataset given start and end logits.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>vocab_path</p></td>
<td><p>Path for a local directory containing vocabulary files</p></td>
<td><p>String</p></td>
<td><p>Mandatory</p></td>
</tr>
<tr class="row-odd"><td><p>max_seq_length</p></td>
<td><p>The maximum total input sequence length after WordPiece tokenization. Sequences longer than this will be truncated, and sequences shorter than this will be padded (optional if preprocessing is run)</p></td>
<td><p>Integer</p></td>
<td><p>384</p></td>
</tr>
<tr class="row-even"><td><p>doc_stride</p></td>
<td><p>When splitting up a long document into chunks, how much stride to take between chunks (optional if preprocessing is run)</p></td>
<td><p>Integer</p></td>
<td><p>128</p></td>
</tr>
<tr class="row-odd"><td><p>max_query_length</p></td>
<td><p>The maximum number of tokens for the question. Questions longer than this will be truncated to this length (optional if preprocessing is run)</p></td>
<td><p>Integer</p></td>
<td><p>64</p></td>
</tr>
<tr class="row-even"><td><p>n_best_size</p></td>
<td><p>The total number of n-best predictions to generate in the post.json output file</p></td>
<td><p>Integer</p></td>
<td><p>20</p></td>
</tr>
<tr class="row-odd"><td><p>max_answer_length</p></td>
<td><p>The maximum length of an answer that can be generated. This is needed because the start and end predictions are not conditioned on one another</p></td>
<td><p>Integer</p></td>
<td><p>30</p></td>
</tr>
<tr class="row-even"><td><p>packing_strategy</p></td>
<td><p>This flag is set to True if using packing strategy</p></td>
<td><p>Boolean</p></td>
<td><p>False</p></td>
</tr>
</tbody>
</table>
<p><strong>centerface_postproc</strong> - Processes the inference outputs to parse detections and generates a detections file for the metric evaluator. Used for processing CenterFace face detector.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>dims</p></td>
<td><p>Height and width; comma delimited, e.g., 640,640</p></td>
<td><p>String</p></td>
<td><p>Mandatory</p></td>
</tr>
<tr class="row-odd"><td><p>dtypes</p></td>
<td><p>List of datatypes to be used for bounding boxes, scores, and labels (in order), e.g., [float32, float32, int64]. Defaults to the datatypes fetched from the ‘outputs_info’ for the model’s config.yaml</p></td>
<td><p>List</p></td>
<td><p>Datatypes from the outputs_info section of the model config.yaml</p></td>
</tr>
<tr class="row-even"><td><p>heatmap_threshold</p></td>
<td><p>User input for heatmap threshold</p></td>
<td><p>Float</p></td>
<td><p>0.05</p></td>
</tr>
<tr class="row-odd"><td><p>nms_threshold</p></td>
<td><p>User input for nms threshold</p></td>
<td><p>Float</p></td>
<td><p>0.3</p></td>
</tr>
</tbody>
</table>
<p><strong>centernet_postprocess</strong> - Processes the inference outputs to parse detections and generate a detections file for the metric evaluator. Used for processing CenterNet detector.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>dtypes</p></td>
<td><p>List of datatypes (at least 3) to be used to infer outputs</p></td>
<td><p>String</p></td>
<td><p>Mandatory</p></td>
</tr>
<tr class="row-odd"><td><p>output_dims</p></td>
<td><p>Height and width; comma delimited, e.g., 640,640</p></td>
<td><p>String</p></td>
<td><p>Mandatory</p></td>
</tr>
<tr class="row-even"><td><p>top_k</p></td>
<td><p>Top K proposals are given from the postprocess plugin</p></td>
<td><p>Integer</p></td>
<td><p>100</p></td>
</tr>
<tr class="row-odd"><td><p>num_classes</p></td>
<td><p>Number of classes</p></td>
<td><p>Integer</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>score</p></td>
<td><p>Threshold to purify the detections</p></td>
<td><p>Integer</p></td>
<td><p>1</p></td>
</tr>
</tbody>
</table>
<p><strong>lprnet_predict</strong> - Used for LPRNET license plate prediction.</p>
<p><strong>object_detection</strong> - Processes the inference outputs to parse detections and generate a detections file for metric evaluator</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>dims</p></td>
<td><p>Height and width; comma delimited, e.g., 640,640</p></td>
<td><p>String</p></td>
<td><p>Mandatory</p></td>
</tr>
<tr class="row-odd"><td><p>type</p></td>
<td><p>Type of post-processing (e.g., letterbox, stretch)</p></td>
<td><p>String</p></td>
<td><p>None</p></td>
</tr>
<tr class="row-even"><td><p>label_offset</p></td>
<td><p>Offset for the labels information</p></td>
<td><p>Integer</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>score_threshold</p></td>
<td><p>Threshold limit for the detection scores</p></td>
<td><p>Float</p></td>
<td><p>0.001</p></td>
</tr>
<tr class="row-even"><td><p>xywh_to_xyxy</p></td>
<td><p>Convert bounding box format from box center (xywh) to box corner (xyxy) format</p></td>
<td><p>Boolean</p></td>
<td><p>False</p></td>
</tr>
<tr class="row-odd"><td><p>xy_swap</p></td>
<td><p>Swap the X and Y coordinates of bbox</p></td>
<td><p>Boolean</p></td>
<td><p>False</p></td>
</tr>
<tr class="row-even"><td><p>dtypes</p></td>
<td><p>List of datatypes used for bounding boxes, scores, and labels in order, e.g., [float32, float32, int64]. Defaults to the datatypes fetched from the ‘outputs_info’ for the model’s config.yaml.</p></td>
<td><p>List</p></td>
<td><p>Datatypes from the outputs_info section of the model config.yaml</p></td>
</tr>
<tr class="row-odd"><td><p>mask</p></td>
<td><p>Do postprocessing on mask</p></td>
<td><p>Boolean</p></td>
<td><p>False</p></td>
</tr>
<tr class="row-even"><td><p>mask_dims</p></td>
<td><p>Output dims of model. Provide this only if mask = True. E.g., 100,80,28,28</p></td>
<td><p>String</p></td>
<td><p>None</p></td>
</tr>
<tr class="row-odd"><td><p>padded_outputs</p></td>
<td><p>Pad the outputs</p></td>
<td><p>Boolean</p></td>
<td><p>False</p></td>
</tr>
<tr class="row-even"><td><p>scale</p></td>
<td><p>Comma separated scale values</p></td>
<td><p>String</p></td>
<td><p>‘1’</p></td>
</tr>
<tr class="row-odd"><td><p>skip_padding</p></td>
<td><p>Skip padding while rescaling to original image shape</p></td>
<td><p>Boolean</p></td>
<td><p>False</p></td>
</tr>
</tbody>
</table>
<p><strong>onmt_postprocess</strong> - Performs preprocessing for OpenNMT model outputs</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>sentencepiece_model_path</p></td>
<td><p>Path to sentencepiece model for WMT dataset</p></td>
<td><p>String</p></td>
<td><p>Mandatory</p></td>
</tr>
<tr class="row-odd"><td><p>unrolled_count</p></td>
<td><p>Upper limit on the unrolls required for the output (no. of output tokens to be considered for metric)</p></td>
<td><p>Integer</p></td>
<td><p>26</p></td>
</tr>
<tr class="row-even"><td><p>vocab_path</p></td>
<td><p>Path to OpenNMT model vocabulary file (pickle file), optional if preprocessing is run</p></td>
<td><p>String</p></td>
<td><p>None</p></td>
</tr>
<tr class="row-odd"><td><p>skip_sentencepiece</p></td>
<td><p>Skip sentencepiece encoding, optional if preprocessing is run</p></td>
<td><p>Boolean</p></td>
<td><p>None</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="metric-plugins">
<h5>Metric plugins<a class="headerlink" href="#metric-plugins" title="Permalink to this heading">¶</a></h5>
<p><strong>bleu</strong> - Evaluates bleu score using sacrebleu library</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>round</p></td>
<td><p>Number of decimal places to round the result to</p></td>
<td><p>Integer</p></td>
<td><p>1</p></td>
</tr>
</tbody>
</table>
<p><strong>map_coco</strong> - Evaluates the mAP score 50 and 50:05:95 for COCO dataset</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>map_80_to_90</p></td>
<td><p>Mapping of classes in range 0-80 to 0-90</p></td>
<td><p>Boolean</p></td>
<td><p>False</p></td>
</tr>
<tr class="row-odd"><td><p>segm</p></td>
<td><p>Flag to calculate mAP for mask</p></td>
<td><p>Boolean</p></td>
<td><p>False</p></td>
</tr>
<tr class="row-even"><td><p>keypoint_map</p></td>
<td><p>Flag to calculate mAP for keypoint</p></td>
<td><p>Boolean</p></td>
<td><p>False</p></td>
</tr>
</tbody>
</table>
<p><strong>perplexity</strong> - Calculates the perplexity metric. Model outputs are expected to be the logits of proper shape. Ground truth data is expected to be in tokenized format and in the form of token IDs. The ground truth will be automatically generated, if using the “gpt2_tokenizer” dataset plugin.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>logits_index</p></td>
<td><p>Index of the logits output if the model has multiple outputs</p></td>
<td><p>Integer</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
<p><strong>precision</strong> - Calculates the precision metric, i.e., (correct predictions / total predictions). Ground truth data is expected in the format “filename &lt;space&gt; correct_text”. The postprocessed model outputs are expected to be text files with just the “predicted_text”.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>round</p></td>
<td><p>Number of decimal places to round the result to</p></td>
<td><p>Integer</p></td>
<td><p>7</p></td>
</tr>
<tr class="row-odd"><td><p>input_image_index</p></td>
<td><p>For multi input models, the index of image file in input file list csv</p></td>
<td><p>Integer</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
<p><strong>squad_em</strong> - Calculates the exact match for SQuAD v1.1 dataset predictions and ground truth.</p>
<p><strong>squad_f1</strong> - Calculates F1 score for SQuAD v1.1 dataset predictions and ground truth.</p>
<p><strong>topk</strong> - Evaluates topk value by comparing results and annotations.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>kval</p></td>
<td><p>Top k values, e.g., 1,5 evaluates top1 and top5</p></td>
<td><p>String</p></td>
<td><p>5</p></td>
</tr>
<tr class="row-odd"><td><p>softmax_index</p></td>
<td><p>Index of the softmax output in the results file list</p></td>
<td><p>Integer</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>label_offset</p></td>
<td><p>Offset required in the labels’ scores, e,g., if shape is 1x1001, then labels_offset=1</p></td>
<td><p>Integer</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>round</p></td>
<td><p>Number of decimal places to round the result to</p></td>
<td><p>Integer</p></td>
<td><p>3</p></td>
</tr>
<tr class="row-even"><td><p>input_image_index</p></td>
<td><p>For multi input models, the index of image file in input file list csv</p></td>
<td><p>Integer</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
<p><strong>widerface_AP</strong> - Computes average precision for easy, medium, and hard cases.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>IoU_threshold</p></td>
<td><p>User input for IoU threshold</p></td>
<td><p>Float</p></td>
<td><p>0.4</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="memory-based-plugins">
<h4>Memory-based plugins<a class="headerlink" href="#memory-based-plugins" title="Permalink to this heading">¶</a></h4>
<p>This section lists the built-in memory-based plugins.</p>
<div class="section" id="id7">
<h5>Dataset plugins<a class="headerlink" href="#id7" title="Permalink to this heading">¶</a></h5>
<p><strong>create_squad_examples</strong> - Extracts examples from a given squad dataset file and saves them to a file.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>squad_version</p></td>
<td><p>Squad version 1 or 2</p></td>
<td><p>Integer</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>max_inputs</p></td>
<td><p>Maximum number of inputs in inputlist to be considered for execution</p></td>
<td><p>Integer</p></td>
<td><p>-1 (Complete Dataset)</p></td>
</tr>
<tr class="row-even"><td><p>max_calib</p></td>
<td><p>Maximum number of inputs in calibration to be considered for execution</p></td>
<td><p>Integer</p></td>
<td><p>-1 (Complete Dataset)</p></td>
</tr>
</tbody>
</table>
<p><strong>filter_dataset</strong> - Filters the dataset including the input list, calibration, and annotation files.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>max_inputs</p></td>
<td><p>Maximum number of inputs in inputlist to be considered for execution</p></td>
<td><p>Integer</p></td>
<td><p>Mandatory</p></td>
</tr>
<tr class="row-odd"><td><p>max_calib</p></td>
<td><p>Maximum number of inputs in calibration to be considered for execution</p></td>
<td><p>Integer</p></td>
<td><p>Mandatory</p></td>
</tr>
<tr class="row-even"><td><p>random</p></td>
<td><p>Shuffles the inputlist and calibration files</p></td>
<td><p>Boolean</p></td>
<td><p>False</p></td>
</tr>
</tbody>
</table>
<p><strong>tokenize_wikitext_2</strong> - Tokenizes wikitext-2 dataset into model inputs.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>seq_length</p></td>
<td><p>Sequence length for the generated model inputs</p></td>
<td><p>Integer</p></td>
<td><p>Mandatory</p></td>
</tr>
<tr class="row-odd"><td><p>tokenizer_name</p></td>
<td><p>Name of the tokenizer to be used for generating model inputs</p></td>
<td><p>String</p></td>
<td><p>Mandatory</p></td>
</tr>
<tr class="row-even"><td><p>past_shape</p></td>
<td><p>Shape of the ‘past’ inputs</p></td>
<td><p>List</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>num_past</p></td>
<td><p>Number of ‘past’ inputs</p></td>
<td><p>Integer</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>pos_id</p></td>
<td><p>Flag to configure whether position ids are required</p></td>
<td><p>Bool</p></td>
<td><p>True</p></td>
</tr>
<tr class="row-odd"><td><p>mask_dtype</p></td>
<td><p>Data type of the mask used.</p></td>
<td><p>String</p></td>
<td><p>‘float32’</p></td>
</tr>
<tr class="row-even"><td><p>cached_path</p></td>
<td><p>Path to cached tokenizer file (if available)</p></td>
<td><p>String</p></td>
<td></td>
</tr>
</tbody>
</table>
<p><strong>split_txt_data</strong> - Saves individual text files for each line present in the given input text file.</p>
</div>
<div class="section" id="preprocessing-memory-plugins">
<h5>Preprocessing memory plugins<a class="headerlink" href="#preprocessing-memory-plugins" title="Permalink to this heading">¶</a></h5>
<p><strong>centernet_preproc</strong> - Performs preprocessing on CenterNet dataset examples.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>dims</p></td>
<td><p>Height and width; comma delimited, e.g., 416,416</p></td>
<td><p>String</p></td>
<td><p>Mandatory</p></td>
</tr>
<tr class="row-odd"><td><p>scale</p></td>
<td><p>Scale factor for image</p></td>
<td><p>Float</p></td>
<td><p>1.0</p></td>
</tr>
<tr class="row-even"><td><p>fix_res</p></td>
<td><p>Resolution of the image</p></td>
<td><p>Boolean</p></td>
<td><p>True</p></td>
</tr>
<tr class="row-odd"><td><p>pad</p></td>
<td><p>Image padding</p></td>
<td><p>Integer</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
<p><strong>convert_nchw</strong> - Transposes WHC to CHW or CHW to WHC and adds an extra N dimension.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>expand-dims</p></td>
<td><p>Add the Nth dimension</p></td>
<td><p>Boolean</p></td>
<td><p>True</p></td>
</tr>
</tbody>
</table>
<p><strong>create_batch</strong> - Concatenates raw input files into a single file using numpy.</p>
<p><strong>crop</strong> - Center crops an image to the given dimensions using numpy or torchvision based on the library parameter.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>dims</p></td>
<td><p>Height and width; comma delimited, e.g., 640,640</p></td>
<td><p>String</p></td>
<td><p>Mandatory</p></td>
</tr>
<tr class="row-odd"><td><p>library</p></td>
<td><p>Python library used to crop the given input; valid values are: numpy | torchvision</p></td>
<td><p>String</p></td>
<td><p>numpy</p></td>
</tr>
<tr class="row-even"><td><p>typecasting_required</p></td>
<td><p>To convert final output to numpy or not. Note: This option is specific to torchvision library</p></td>
<td><p>Boolean</p></td>
<td><p>True</p></td>
</tr>
</tbody>
</table>
<p><strong>expand_dims</strong> - Adds the N dimension for images, e.g., HWC to NHWC.</p>
<p><strong>image_transformers_input</strong> - Creates input files with image and/or text for image transformer models like ViT and CLIP. (Note: This plugin requires Pillow  package version:10.0.0)</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>dims</p></td>
<td><p>Expected processed output dimension in CHW format</p></td>
<td><p>String</p></td>
<td><p>Mandatory</p></td>
</tr>
<tr class="row-odd"><td><p>image_only</p></td>
<td><p>Data type of raw data</p></td>
<td><p>Boolean</p></td>
<td><p>True</p></td>
</tr>
</tbody>
</table>
<p><strong>normalize</strong> - Normalizes input per the given scheme; data must be of NHWC format.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>library</p></td>
<td><p>Python library used to crop the given input; valid values are: numpy | torchvision</p></td>
<td><p>String</p></td>
<td><p>numpy</p></td>
</tr>
<tr class="row-odd"><td><p>norm</p></td>
<td><p>Normalization factor, all values divided by norm</p></td>
<td><p>float32</p></td>
<td><p>255.0</p></td>
</tr>
<tr class="row-even"><td><p>means</p></td>
<td><p>Dictionary of means to be subtracted, e.g., {“R”:0.485, “G”:0.456, “B”:0.406}</p></td>
<td><p>RGB dictionary</p></td>
<td><p>{“R”:0, “G”:0, “B”:0}</p></td>
</tr>
<tr class="row-odd"><td><p>std</p></td>
<td><p>Dictionary of std-dev for rescaling the values, e.g., {“R”:0.229, “G”:0.224, “B”:0.225}</p></td>
<td><p>RGB dictionary</p></td>
<td><p>{“R”:1, “G”:1, “B”:1}</p></td>
</tr>
<tr class="row-even"><td><p>channel_order</p></td>
<td><p>Channel order to specify means and std values per channel - RGB | BGR</p></td>
<td><p>String</p></td>
<td><p>‘RGB’</p></td>
</tr>
<tr class="row-odd"><td><p>normalize_first</p></td>
<td><div class="line-block">
<div class="line">To perform normalization before or after mean subtraction and standard deviation.</div>
<div class="line">normalize_first=True means perform normalization before.</div>
<div class="line">Note: torchvision library does not use this option</div>
</div>
</td>
<td><p>Boolean</p></td>
<td><p>True</p></td>
</tr>
<tr class="row-even"><td><p>typecasting_required</p></td>
<td><p>To convert final output to numpy or not. Note: This option is specific to the Torchvision library</p></td>
<td><p>Boolean</p></td>
<td><p>True</p></td>
</tr>
<tr class="row-odd"><td><p>pil_to_tensor_input</p></td>
<td><p>To convert input to tensor before normalization. Note: This option is specific to the Torchvision library</p></td>
<td><p>Boolean</p></td>
<td><p>True</p></td>
</tr>
</tbody>
</table>
<p><strong>pad</strong> - Image padding with constant pad size or based on target dimensions</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>dims</p></td>
<td><p>Height and width comma delimited, e.g., 416,416 for ‘target-dims’ type of padding</p></td>
<td><p>String</p></td>
<td><p>Mandatory</p></td>
</tr>
<tr class="row-odd"><td><p>type</p></td>
<td><dl class="simple">
<dt>Type of padding. Valid options:</dt><dd><ul class="simple">
<li><p>constant: Add padding of constant sides on 4 sides (pad_size must be provided)</p></li>
<li><p>target_dims: Add padding based on difference in image size and target size (dims param must be provided)</p></li>
</ul>
</dd>
</dl>
</td>
<td><p>String</p></td>
<td><p>Mandatory</p></td>
</tr>
<tr class="row-even"><td><p>pad_size</p></td>
<td><p>Size of padding for ‘constant’ type of padding</p></td>
<td><p>Integer</p></td>
<td><p>None</p></td>
</tr>
<tr class="row-odd"><td><p>img_position</p></td>
<td><p>Parameter to specify position of image, either ‘center’ or ‘corner’ (top-left). Padding is added accordingly. Currently used for ‘target_dims’ type padding</p></td>
<td><p>String</p></td>
<td><p>‘center’</p></td>
</tr>
<tr class="row-even"><td><p>color</p></td>
<td><p>Padding value for all planes</p></td>
<td><p>Integer</p></td>
<td><p>114</p></td>
</tr>
</tbody>
</table>
<p><strong>resize</strong> - Resizes an image using the specified library parameter: cv2(Default), pillow or torchvision</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>dims</p></td>
<td><p>Height and width; comma delimited, e.g., 640,640</p></td>
<td><p>String</p></td>
<td><p>Mandatory</p></td>
</tr>
<tr class="row-odd"><td><p>library</p></td>
<td><p>Python library to be used for resizing a given input; valid values are: opencv | pillow | torchvision</p></td>
<td><p>String</p></td>
<td><p>opencv</p></td>
</tr>
<tr class="row-even"><td><p>channel_order</p></td>
<td><p>Convert image to specified channel order. At present this parameter only takes the ‘RGB’ value</p></td>
<td><p>String</p></td>
<td><p>RGB</p></td>
</tr>
<tr class="row-odd"><td><p>interp</p></td>
<td><dl class="simple">
<dt>Interpolation Type. Options:</dt><dd><ul class="simple">
<li><p>bilinear (supported by opencv, Torchvision, pillow)</p></li>
<li><p>area (supported by opencv only)</p></li>
<li><p>nearest (supported by opencv, Torchvision, pillow)</p></li>
<li><p>bicubic (supported by Torchvision, pillow)</p></li>
<li><p>box (supported by pillow only)</p></li>
<li><p>hamming (supported by pillow only)</p></li>
<li><p>lanczos (supported by pillow only)</p></li>
</ul>
</dd>
</dl>
</td>
<td><p>String</p></td>
<td><div class="line-block">
<div class="line">For opencv and torchvision: bilinear</div>
<div class="line">For pillow: bicubic</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>type</p></td>
<td><dl class="simple">
<dt>Type of resize to be done. Note: Torchvision does not use this option. Options:</dt><dd><ul class="simple">
<li><p>letterbox : Used for YOLO models.</p></li>
<li><p>imagenet : Scale followed by resize.</p></li>
<li><p>aspect_ratio : Resize while keeping aspect ratio.</p></li>
<li><p>None : The default behavior is to auto-resize the image to the target dims.</p></li>
</ul>
</dd>
</dl>
</td>
<td><p>String</p></td>
<td><p>auto-resize</p></td>
</tr>
<tr class="row-odd"><td><p>resize_before_typecast</p></td>
<td><p>To resize before or after conversion to target datatype e.g., fp32</p></td>
<td><p>Boolean</p></td>
<td><p>True</p></td>
</tr>
<tr class="row-even"><td><p>typecasting_required</p></td>
<td><p>To convert final output to numpy or not. Note: This option is specific to the Torchvision library</p></td>
<td><p>Boolean</p></td>
<td><p>True</p></td>
</tr>
<tr class="row-odd"><td><p>mean</p></td>
<td><p>Dictionary of means to be subtracted, e.g., {“R”:0.485, “G”:0.456, “B”:0.406}. Note: This option is specific to the Tensorflow library</p></td>
<td><p>RGB dictionary</p></td>
<td><p>{“R”:0, “G”:0, “B”:0}</p></td>
</tr>
<tr class="row-even"><td><p>std</p></td>
<td><p>Dictionary of std-dev for rescaling the values, e.g., {“R”:0.229, “G”:0.224, “B”:0.225}. Note: This option is specific to the Tensorflow library</p></td>
<td><p>RGB dictionary</p></td>
<td><p>{“R”:0, “G”:0, “B”:0}</p></td>
</tr>
<tr class="row-odd"><td><p>normalize_before_resize</p></td>
<td><p>To perform normalization before or after mean subtraction and standard deviation. Note: This option is specific to the Tensorflow library</p></td>
<td><p>Boolean</p></td>
<td><p>False</p></td>
</tr>
<tr class="row-even"><td><p>crop_before_resize</p></td>
<td><p>To perform cropping before resize. Note: This option is specific to the Tensorflow library</p></td>
<td><p>Boolean</p></td>
<td><p>False</p></td>
</tr>
<tr class="row-odd"><td><p>norm</p></td>
<td><p>Normalization factor, all values divided by norm</p></td>
<td><p>float32</p></td>
<td><p>255.0</p></td>
</tr>
<tr class="row-even"><td><p>normalize_first</p></td>
<td><div class="line-block">
<div class="line">To perform normalization before or after mean subtraction and standard deviation.</div>
<div class="line">normalize_first=True means perform normalization before.</div>
<div class="line">Note: torchvision library does not use this option</div>
</div>
</td>
<td><p>Boolean</p></td>
<td><p>True</p></td>
</tr>
</tbody>
</table>
<p><strong>squad_preprocess</strong> - Reads the processed files created by the <cite>create_squad_examples</cite> plugin.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>mask_type</p></td>
<td><p>The type of masking to apply.  If ‘bool’, boolean masking is applied. If None, no masking is applied.</p></td>
<td><p>String</p></td>
<td><p>None</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="postprocessing-memory-plugins">
<h5>Postprocessing memory plugins<a class="headerlink" href="#postprocessing-memory-plugins" title="Permalink to this heading">¶</a></h5>
<p><strong>squad_postprocess</strong> - Predicts answers for a SQuAD dataset for the given start and end scores.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>packing_strategy</p></td>
<td><p>This flag is set to True if using packing strategy</p></td>
<td><p>Boolean</p></td>
<td><p>False</p></td>
</tr>
</tbody>
</table>
<p><strong>centerface_postproc</strong> - Processes the inference outputs to parse detections and generates a detections file for the metric evaluator. Used for processing CenterFace face detector.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>dims</p></td>
<td><p>Height and width; comma delimited, e.g., 640,640</p></td>
<td><p>String</p></td>
<td><p>Mandatory</p></td>
</tr>
<tr class="row-odd"><td><p>heatmap_threshold</p></td>
<td><p>User input for heatmap threshold</p></td>
<td><p>Float</p></td>
<td><p>0.05</p></td>
</tr>
<tr class="row-even"><td><p>nms_threshold</p></td>
<td><p>User input for nms threshold</p></td>
<td><p>Float</p></td>
<td><p>0.3</p></td>
</tr>
</tbody>
</table>
<p><strong>centernet_postprocess</strong> - Processes the inference outputs to parse detections and generate a detections file for the metric evaluator. Used for processing CenterNet detector.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>output_dims</p></td>
<td><p>Height and width; comma delimited, e.g., 640,640</p></td>
<td><p>String</p></td>
<td><p>Mandatory</p></td>
</tr>
<tr class="row-odd"><td><p>top_k</p></td>
<td><p>Top K proposals are given from the postprocess plugin</p></td>
<td><p>Integer</p></td>
<td><p>100</p></td>
</tr>
<tr class="row-even"><td><p>num_classes</p></td>
<td><p>Number of classes</p></td>
<td><p>Integer</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>score</p></td>
<td><p>Threshold to purify the detections</p></td>
<td><p>Integer</p></td>
<td><p>1</p></td>
</tr>
</tbody>
</table>
<p><strong>lprnet_predict</strong> - Used for LPRNET license plate prediction.</p>
<p><strong>object_detection</strong> - Processes the inference outputs to parse detections and generate a detections file for metric evaluator</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>dims</p></td>
<td><p>Height and width; comma delimited, e.g., 640,640</p></td>
<td><p>String</p></td>
<td><p>Mandatory</p></td>
</tr>
<tr class="row-odd"><td><p>type</p></td>
<td><p>Type of post-processing (e.g., letterbox, stretch)</p></td>
<td><p>String</p></td>
<td><p>None</p></td>
</tr>
<tr class="row-even"><td><p>label_offset</p></td>
<td><p>Offset for the labels information</p></td>
<td><p>Integer</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>score_threshold</p></td>
<td><p>Threshold limit for the detection scores</p></td>
<td><p>Float</p></td>
<td><p>0.001</p></td>
</tr>
<tr class="row-even"><td><p>xywh_to_xyxy</p></td>
<td><p>Convert bounding box format from box center (xywh) to box corner (xyxy) format</p></td>
<td><p>Boolean</p></td>
<td><p>False</p></td>
</tr>
<tr class="row-odd"><td><p>xy_swap</p></td>
<td><p>Swap the X and Y coordinates of bbox</p></td>
<td><p>Boolean</p></td>
<td><p>False</p></td>
</tr>
<tr class="row-even"><td><p>dtypes</p></td>
<td><p>List of datatypes used for bounding boxes, scores, and labels in order, e.g., [float32, float32, int64]. Defaults to the datatypes fetched from the ‘outputs_info’ for the model’s config.yaml.</p></td>
<td><p>List</p></td>
<td><p>Datatypes from the outputs_info section of the model config.yaml</p></td>
</tr>
<tr class="row-odd"><td><p>mask</p></td>
<td><p>Do postprocessing on mask</p></td>
<td><p>Boolean</p></td>
<td><p>False</p></td>
</tr>
<tr class="row-even"><td><p>mask_dims</p></td>
<td><p>Output dims of model. Provide this only if mask = True. E.g., 100,80,28,28</p></td>
<td><p>String</p></td>
<td><p>None</p></td>
</tr>
<tr class="row-odd"><td><p>padded_outputs</p></td>
<td><p>Pad the outputs</p></td>
<td><p>Boolean</p></td>
<td><p>False</p></td>
</tr>
<tr class="row-even"><td><p>scale</p></td>
<td><p>Comma separated scale values</p></td>
<td><p>String</p></td>
<td><p>‘1’</p></td>
</tr>
<tr class="row-odd"><td><p>skip_padding</p></td>
<td><p>Skip padding while rescaling to original image shape</p></td>
<td><p>Boolean</p></td>
<td><p>False</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="metric-memory-plugins">
<h5>Metric memory plugins<a class="headerlink" href="#metric-memory-plugins" title="Permalink to this heading">¶</a></h5>
<p><strong>map_coco</strong> - Evaluates the mAP score 50 and 50:05:95 for COCO dataset</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>map_80_to_90</p></td>
<td><p>Mapping of classes in range 0-80 to 0-90</p></td>
<td><p>Boolean</p></td>
<td><p>False</p></td>
</tr>
<tr class="row-odd"><td><p>segm</p></td>
<td><p>Flag to calculate mAP for mask</p></td>
<td><p>Boolean</p></td>
<td><p>False</p></td>
</tr>
<tr class="row-even"><td><p>keypoint_map</p></td>
<td><p>Flag to calculate mAP for keypoint</p></td>
<td><p>Boolean</p></td>
<td><p>False</p></td>
</tr>
<tr class="row-odd"><td><p>data</p></td>
<td><p>Dataset used for evaluation. data must be one of ‘openimages’ or ‘coco’</p></td>
<td><p>String</p></td>
<td><p>‘coco’</p></td>
</tr>
</tbody>
</table>
<p><strong>perplexity</strong> - Calculates the perplexity metric. Model outputs are expected to be the logits of proper shape. Ground truth data is expected to be in tokenized format and in the form of token IDs. The ground truth will be automatically generated, if using the “gpt2_tokenizer” dataset plugin.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>logits_index</p></td>
<td><p>Index of the logits output if the model has multiple outputs</p></td>
<td><p>Integer</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
<p><strong>precision</strong> - Calculates the precision metric, i.e., (correct predictions / total predictions). Ground truth data is expected in the format “filename &lt;space&gt; correct_text”. The postprocessed model outputs are expected to be text files with just the “predicted_text”.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>round</p></td>
<td><p>Number of decimal places to round the result to</p></td>
<td><p>Integer</p></td>
<td><p>7</p></td>
</tr>
<tr class="row-odd"><td><p>input_image_index</p></td>
<td><p>For multi input models, the index of image file in input file list csv</p></td>
<td><p>Integer</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
<p><strong>squad_eval</strong> - Calculates F1 score and exact match scores for SQuAD dataset based on predictions and ground truth.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>vocabulary</p></td>
<td><dl class="simple">
<dt>vocabulary used for creating the tokenizer which would be used for evaluation.</dt><dd><ul class="simple">
<li><p>vocabulary from huggingface.co and cache (e.g. “bert-base-uncased”)</p></li>
<li><p>vocabulary from huggingface.co (user-uploaded) and cache (e.g. “deepset/roberta-base-squad2”)</p></li>
<li><p>path for local directory containing vocabulary files(tokenizer was saved using _save_pretrained(‘./test/saved_model/’)</p></li>
</ul>
</dd>
</dl>
</td>
<td><p>String</p></td>
<td><p>Mandatory</p></td>
</tr>
<tr class="row-odd"><td><p>max_answer_length</p></td>
<td><p>The maximum length of an answer, after tokenization. In SQuAD v2 this was set to 30 tokens; in SQuAD v1 it was not specified so a default value of 30 was used.</p></td>
<td><p>Integer</p></td>
<td><p>30</p></td>
</tr>
<tr class="row-even"><td><p>n_best_size</p></td>
<td><p>Specifies how many of the possible answers to return for a given question along with corresponding confidence scores.</p></td>
<td><p>Integer</p></td>
<td><p>20</p></td>
</tr>
<tr class="row-odd"><td><p>do_lower_case</p></td>
<td><p>Whether or not to lowercase all text before processing.</p></td>
<td><p>Bool</p></td>
<td><p>False</p></td>
</tr>
<tr class="row-even"><td><p>squad_version</p></td>
<td><p>Indicates which version of SQuAD style questions and answers we’re dealing with (“v1” or “v2”).</p></td>
<td><p>Integer</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>round</p></td>
<td><p>Number of decimal places to round the result to</p></td>
<td><p>Integer</p></td>
<td><p>6</p></td>
</tr>
<tr class="row-even"><td><p>cached_vocab_path</p></td>
<td><p>Path to cached vocab_file to be used for creating tokenizer</p></td>
<td><p>String</p></td>
<td><p>None</p></td>
</tr>
</tbody>
</table>
<p><strong>topk</strong> - Evaluates topk value by comparing results and annotations.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>kval</p></td>
<td><p>Top k values, e.g., 1,5 evaluates top1 and top5</p></td>
<td><p>String</p></td>
<td><p>‘1,5’</p></td>
</tr>
<tr class="row-odd"><td><p>softmax_index</p></td>
<td><p>Index of the softmax output in the results file list</p></td>
<td><p>Integer</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>label_offset</p></td>
<td><p>Offset required in the labels’ scores, e,g., if shape is 1x1001, then labels_offset=1</p></td>
<td><p>Integer</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>round</p></td>
<td><p>Number of decimal places to round the result to</p></td>
<td><p>Integer</p></td>
<td><p>3</p></td>
</tr>
<tr class="row-even"><td><p>input_image_index</p></td>
<td><p>For multi input models, the index of image file in input file list csv</p></td>
<td><p>Integer</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
<p><strong>widerface_AP</strong> - Computes average precision for easy, medium, and hard cases.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>IoU_threshold</p></td>
<td><p>User input for IoU threshold</p></td>
<td><p>Float</p></td>
<td><p>0.4</p></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="converters.html" class="btn btn-neutral float-right" title="Converters" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="converter_op_package_gen_example.html" class="btn btn-neutral float-left" title="Create a QNN converter Op Package shared library" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020-2024, Qualcomm Technologies, Inc..

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>